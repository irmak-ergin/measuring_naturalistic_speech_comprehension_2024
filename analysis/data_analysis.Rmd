---
title: "Measuring Naturalistic Speech Comprehension in Real Time"
output: html_document

---
# DATA WRANGLING

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
# Load libraries

library(stringr)
library(fs)
library(readxl)
library(readr)
library(simr)
library(lmerTest)
library(patchwork)
library(RColorBrewer)
library(cowplot)
library(scales)
library(gridExtra)
library(PerformanceAnalytics)
library(corrplot)
library(Hmisc)
library(GGally)
library(visreg)
library(broom.mixed)
library(sjPlot)
library(gridExtra)
library(grid) 
library(rempsyc)
library(flextable)
library(officer)
library(ggsignif)
library(MuMIn)
library(rstatix)
library(caret)
library(modelr)
library(tidyr)
library(purrr)
library(broom)
library(dplyr)
library(ggplot2)
library(rsample)


# set the default ggplot theme 
theme_set(theme_classic())
```

```{r}
# Set path and participant ids

# Change the folder path to data folder relative to the wd
folder_path <- '/Users/irmakergin/Desktop/measuring_naturalistic_speech_comprehension_2024/data/raw_data'

# Define the list of participant IDs
participant_ids <- c("p_1","p_2","p_3","p_4","p_5", "p_6", "p_7", "p_8", "p_9","p_10", 
                     "p_11", "p_12", "p_13", "p_14","p_15","p_16","p_17","p_18","p_19","p_20",
                      "p_21", "p_22", "p_23", "p_24","p_25","p_26","p_27","p_28","p_29","p_30")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Organize the data csv files

# Get a list of all files in the folder
files <- dir(folder_path, full.names = TRUE)

# Set a path to save the organized files
organized_data_path<-"/Users/irmakergin/Desktop/measuring_naturalistic_speech_comprehension_2024/data/organized_data"

# Find files by participant ID 
for(part_id in participant_ids) {

  # Match the slider files that involve the participant ID
  slider_pattern <- paste0("slider_", part_id, ".*\\.csv$")
  
  # Match data (csv) files that start with the participant ID
  additional_csv_pattern <- paste0("^", part_id, "_.*\\.csv$")
  
  # Find slider files that match the pattern
  slider_matched_files <- files[str_detect(basename(files), slider_pattern)]
  
  # Find data CSV files that match the pattern
  additional_csv_matched_files <- files[str_detect(basename(files), 
                                                   additional_csv_pattern)]
   
  # Create a directory for the current participant on the organized data path
  participant_folder <- file.path(organized_data_path, part_id)
  if(!dir.exists(participant_folder)) {
    dir.create(participant_folder, recursive = TRUE, showWarnings = TRUE)
  }
  
  # Create a subdirectory for slider files within the participant folder
  slider_folder <- file.path(participant_folder, paste0("slider_", part_id))
  if(!dir.exists(slider_folder)) {
    dir.create(slider_folder, recursive = TRUE, showWarnings = TRUE)
  }
  
  # Move slider matched files to the slider subfolder
  if(length(slider_matched_files) > 0) {
    for(file in slider_matched_files) {
      file_copy(file, file.path(slider_folder, basename(file)), overwrite = TRUE)
    }
  }
  
  # Move additional CSV matched files (experiment data) to the main participant folder 
  # and rename the participant folders
  if(length(additional_csv_matched_files) > 0) {
  # Loop to rename the file during the copy process
  for(file in additional_csv_matched_files) {
    # New filename with '_raw_data'
    new_name <- paste0(part_id, "_raw_data.csv")
    # Copy and rename the file to the new name in the participant folder
    file_copy(file, file.path(participant_folder, new_name), overwrite = TRUE)
  }
}
  # check
  if(length(slider_matched_files) > 0 || length(additional_csv_matched_files) > 0) {
    cat("Files for participant ID '", part_id, 
        "' have been organized successfully. Slider files in: ", slider_folder, 
        ", other CSVs in: ", participant_folder, "\n")
  } else {
    cat("No files found for participant ID '", part_id, "'.\n")
  }
}

# Commented out because they are already there
# # Put excel files (of digit in noise and digit span) in the organized data folder 
# 
#source_folder<-"/Users/irmakergin/Desktop/measuring_naturalistic_speech_comprehension_2024/data"
# destination_folder<-"/Users/irmakergin/Desktop/measuring_naturalistic_speech_comprehension_2024/data/organized_data"
# 
# List all Excel files in the source folder
#excel_files <- dir_ls(path = source_folder, glob = "*.xlsx")

# Copy each Excel file to the destination folder- commented out to re-run
#for (file_path in excel_files) {
#dest_file_path <- file.path(destination_folder)
#file_copy(file_path, dest_file_path)
# # Check
# cat("Copied:", file_path, "to", dest_file_path, "\n")
#}

# Loop through each participant ID
for (part_id in participant_ids) {
  # Define the path to the participant's raw data CSV
  raw_data_path <- file.path(organized_data_path, 
                             part_id, 
                             paste0(part_id, "_raw_data.csv"))

# Read the CSV for each participant 
if (file.exists(raw_data_path)) {
  df.raw <- read_csv(raw_data_path) %>%
    filter(!wav_file %in% c("Someday_trial_condition.wav", 
                            "Someday_trial_condition-5x.wav")) %>%
    mutate(index = row_number()) %>%
    select(index, participant, wav_file, old_name, segment_text, question, 
           C1, C2, C3, C4, C_correct, duration,
           A.numClicks, B.numClicks, C.numClicks, D.numClicks, slider.response, 
           summary, `textbox.text`) %>%
    group_by(wav_file) %>%
    summarise(
      index = first(index), 
      participant = first(participant), 
      old_name = first(old_name),
      segment_text = first(segment_text),
      question = first(question),
      C1 = first(C1),
      C2 = first(C2),
      C3 = first(C3),
      C4 = first(C4),
      C_correct = first(C_correct),
      duration = first(duration),
      summary = first(summary),
      textbox.text = first(textbox.text),
      A.numClicks = max(A.numClicks, na.rm = TRUE),
      B.numClicks = max(B.numClicks, na.rm = TRUE),
      C.numClicks = max(C.numClicks, na.rm = TRUE),
      D.numClicks = max(D.numClicks, na.rm = TRUE),
      likert_response = max(slider.response, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    arrange(index) %>%
    mutate(index = row_number()) 
  
  # Fill NA values in participant_id column with part_id for participant p_3
    if (part_id == "p_3") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
    if (part_id == "p_1") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
    if (part_id == "p_2") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
    if (part_id == "p_14") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
    if (part_id == "p_13") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
    if (part_id == "p_26") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
   # Remove the extra last row from df.raw
    df.raw <- head(df.raw, n = -1)
   
  #Assign processed data of each participant to a new df
  assign(paste0("df.raw.merged_", part_id), df.raw, envir = .GlobalEnv)
  }
}
```

```{r}
# Merge all participant dfs into one df

# Create an empty data frame to store merged data
df.raw <- data.frame()

# Loop through each participant ID and merge their df
for (part_id in participant_ids) {
  # Construct the name of the data frame variable
  df_name <- paste0("df.raw.merged_", part_id)
  
  # Check if the data frame exists in the global environment
  if (exists(df_name, envir = .GlobalEnv)) {
    # Get the data frame from its name
    df_participant <- get(df_name, envir = .GlobalEnv)
    
    # Merge the data frame with the main merged data frame
    df.raw <- rbind(df.raw, df_participant)
  }
}
```

```{r}
# Add accuracy columns for multiple choice questions

# Calculate the multiple choice accuracy

df.raw<- df.raw %>%
  mutate(
    multiple_choice_accuracy = apply(., 1, function(row) {
      # Extract the first character from C_correct - it is the correct letter to click on
      correct_answer <- substr(row[["C_correct"]], 1, 1)
      
      # Determine which of the click columns matches the correct answer
      clicked_column <- ifelse(row[["A.numClicks"]] == 1, "A",
                               ifelse(row[["B.numClicks"]] == 1, "B",
                                      ifelse(row[["C.numClicks"]] == 1, "C",
                                             ifelse(row[["D.numClicks"]] == 1, "D", NA))))
      
      # Return 1 if the clicked column matches the correct answer, 0 otherwise
      if (!is.na(clicked_column) && clicked_column == correct_answer) {
        return(1)
      } else {
        return(0)
      }
    })
  )

# Change the column the order
df.raw<- df.raw[c("index", "participant", "wav_file", "old_name", 
                                 "duration","segment_text", 
                                 "question", "C1", "C2", "C3", "C4", "C_correct", 
                                 "A.numClicks", "B.numClicks", "C.numClicks",
                                 "D.numClicks", "multiple_choice_accuracy", 
                                 "likert_response", "summary","textbox.text")]
```

```{r}
# Extract the speech rate information from the trial name and create a speech_rate column
df.raw<- df.raw %>%
  mutate(speech_rate = case_when(
    str_detect(wav_file, "2x") ~ 2,
    str_detect(wav_file, "3x") ~ 3,
    str_detect(wav_file, "4x") ~ 4,
    str_detect(wav_file, "5x") ~ 5,
    TRUE ~ 1  # not sped up if there are no numbers + x in the name 
  ))
```

```{r}
# Add the Digit Span (working memory) scores of each participant

# Create an empty digit_span_score column
df.raw$digit_span_score <- NA

for (part_id in unique(df.raw$participant)) {
  # Construct the filename based on the participant ID
  filename <- paste0(organized_data_path, "/digit_span_", part_id, ".xlsx")
  
  # Check if the file exists
  if (file.exists(filename)) {
    # Correctly read the file using read_excel
    digit_span_data <- read_excel(filename)
    
    # The score is located in the first row of the 'score' column
    score <- digit_span_data$score[1]
      
    df.raw<- df.raw%>%
      mutate(digit_span_score = ifelse(participant == part_id, score, digit_span_score))
  } else {
    cat("File not found for participant", part_id, "\n")
  }
}
```

```{r}
# Add the hearWHO (digit-in-noise task for hearing in noise) scores of each participants

hearwho_pilot_data <- read_excel(paste0(organized_data_path,
                                        "/hearwho_experiment.xlsx"))

# Excel file has columns named 'participant' and 'score'
# Merge the scores into df.raw based on participant ID
df.raw <- merge(df.raw, hearwho_pilot_data[, c("participant", "score")], 
                       by.x = "participant", by.y = "participant", all.x = TRUE)

# Rename the 'score' column to 'digit_in_noise_score'
names(df.raw)[names(df.raw) == "score"] <- "digit_in_noise_score"
```

```{r}
# Add slider values and time points

# Check if the 'slider_values' column exists, if not, create it
if (!"slider_values" %in% names(df.raw)) {
  df.raw$slider_values <- NA
}

# Check if the 'slider_time' column exists, if not, create it
if (!"slider_time" %in% names(df.raw)) {
  df.raw$slider_time <- NA
}

# Define a function to extract and return slider values and time as strings
get_slider_values_and_time <- function(wav_file_name, part_id) {
  # Extract the trial number from wav_file name
  matches <- regmatches(wav_file_name, regexec("Someday_([0-9]+)", wav_file_name))
  if (length(matches[[1]]) < 2) { # If no match or match does not have a capture group
    return(list(values = NA, time = NA))
  }
  trial_number <- matches[[1]][2]

  # Construct the path to the slider file
  slider_file_path <- sprintf("%s/%s/slider_%s/slider_%s_001_Someday_%s_condition*.csv", 
                              organized_data_path, 
                              part_id, part_id, part_id, 
                              trial_number)

  # Find slider files matching the pattern
  slider_files <- Sys.glob(slider_file_path)
  
  if (length(slider_files) == 0) {
    return(list(values = NA, time = NA)) # if no matching file found
  }
  
  slider_data <- read.csv(slider_files[1], skip = 1)

  # Prepare output for values
  slider_values_str <- if ("value" %in% colnames(slider_data)) {
    paste(slider_data$value, collapse = ",")
  } else {
    NA
  }

  # Prepare output for time
  slider_time_str <- if ("time" %in% colnames(slider_data)) {
    rounded_times <- round(slider_data$time, 2)  
    paste(rounded_times, collapse = ",")
  } else {
    NA
  }

  return(list(values = slider_values_str, time = slider_time_str))
}

# Apply the function to each row of df.raw and extract results into separate columns
slider_info <- mapply(get_slider_values_and_time, 
                      df.raw$wav_file, 
                      df.raw$participant, 
                      SIMPLIFY = FALSE)  # Ensure output is a list 

# Assign values and time from the list output to respective columns
df.raw$slider_values <- sapply(slider_info, '[[', "values")
df.raw$slider_time <- sapply(slider_info, '[[', "time")
```

```{r}
# Re-scale slider values to be between 0-1

# Convert slider_values from string to numeric lists 
df.raw$slider_values <- strsplit(as.character(df.raw$slider_values), 
                                        ",\\s*")
df.raw$slider_values <- lapply(df.raw$slider_values, 
                                      function(x) as.numeric(x))

rescale_values <- function(values) {
  sapply(values, function(x) x / 255)
}

# Apply the rescaling function to each row's slider_values
df.raw$slider_values_rescaled <- lapply(df.raw$slider_values, 
                                               rescale_values)
```

```{r}
# Create a column showing number of slider values
df.raw <- df.raw %>%
  mutate(slider_values_number = lengths(slider_values_rescaled))
```

```{r}
# Rescale the 10-point-scale values to be between 0-1
df.raw <- df.raw %>%
  mutate(likert_rescaled = likert_response / 10)
```

## Calculations for the data exclusion criteria
For the post-hoc comprehension measures, to ensure that we are validating the novel measure against actual comprehension, following exclusion criteria will be applied: 

### 1. Absolute threshold for comprehension 
75% correctness on the multiple choice questions for the slowest (easiest to comprehend) speech rate.

```{r}
# Find trials that do not have a number right before .wav in the wav_file column 
# (slowest condition)
attention_check_trials <- df.raw %>%
  filter(!grepl("x\\.wav$", wav_file))

# Calculate the accuracy
accuracy_results <- attention_check_trials %>%
  group_by(participant) %>%
  summarise(
    accuracy = mean(multiple_choice_accuracy == 1, na.rm = TRUE),
    n = n()
  ) %>%
  mutate(passed = if_else(accuracy >= 0.75, TRUE, FALSE))

# Print out participants who passed or failed
passed_participants <- accuracy_results %>%
  filter(passed) %>%
  pull(participant)

failed_participants <- accuracy_results %>%
  filter(!passed) %>%
  select(participant, accuracy)

if(length(passed_participants) > 0) {
  cat(paste(passed_participants, collapse = ", "), 
      "passed the question accuracy criterion\n")
}

if(nrow(failed_participants) > 0) {
  cat(paste(failed_participants$participant, "with accuracy:", failed_participants$accuracy, collapse = ", "), 
      "failed the question accuracy criterion\n")
}
```
#### Figure: failed vs accepted participants

```{r figure-size, fig.width=6, fig.height=4}
# Histogram with all participants accuracies for speech rate 1
p_mult <- ggplot(accuracy_results, aes(x = accuracy)) +
  geom_histogram(binwidth = 0.04, fill = "violetred", alpha = 0.7, color = "black") + 
  labs(title = "All Participants' Multiple Choice Question Accuracy for Speech Speed x1",
       x = "Accuracy",
       y = "Number of Participants")

# Failed vs passed
p_mult + 
  geom_histogram(data = failed_participants, aes(x = accuracy), 
                 binwidth = 0.04, fill = "pink", alpha = 0.7, color = "black") +
  labs(title = "",
       x = "Accuracy",
       y = "Number of Participants") +
  scale_x_continuous(breaks = c(0.6, 0.8, 1), labels= c('0.6', '0.8', '1')) +  
  theme(axis.title.x = element_text(size = 14),   
        axis.title.y = element_text(size = 14),   
        plot.title = element_text(hjust = 0.5, size = 15, margin = margin(b = 40)),  =
        axis.text = element_text(size = 12))  

```
### 2. Ratings on the 10-point scale 
They should be significantly different for the slowest and fastest speech rates.

```{r}
# t-test 
participants <- unique(df.raw$participant)

# Create a vector to store results
rating_criteria_results <- character(length(participants))

# Loop through each participant
for (i in seq_along(participants)) {
  part_id <- participants[i]
  
  # Subset data for the current participant for slowest and fastest speech rates
  subset_data <- df.raw %>%
    filter(participant == part_id & (speech_rate == 1 | speech_rate == 4)) %>%
    select(likert_response, speech_rate)
  
  # Perform a t-test comparing likert_response for speech_rate 1 vs 4
  test_result <- t.test(likert_response ~ speech_rate, data = subset_data)
  
  # Check if the difference is significant (p-value < 0.05)
  if (test_result$p.value < 0.05) {
    rating_criteria_results[i] <- paste(part_id, "passed the rating criterion")
  } else {
    rating_criteria_results[i] <- paste(part_id, "failed the rating criterion")
  }
}

# Print the results
cat(rating_criteria_results, sep = "\n")
```
### 3. Summaries
Participants shouldn't leave all summaries empty

```{r}
# If all rows under textbox.tex are empty

# Check if all 'textbox.text' entries are empty or NA for each participant
summary_criterion <- df.raw %>%
  group_by(participant) %>%
  summarise(all_empty = all(textbox.text == "" | is.na(textbox.text))) %>%
  ungroup()

# Print the results for participants where all textbox.text are empty
summary_criterion %>%
  filter(all_empty) %>%
  pull(participant) %>%
  walk(~ cat(.x, "failed summary criterion\n"))

# Print the results for participants where not all textbox.text are empty
summary_criterion %>%
  filter(!all_empty) %>%
  pull(participant) %>%
  walk(~ cat(.x, "passed summary criterion\n"))
```

### 4. Slider movements
For the physical slider to ensure that participants actually used it actively: Distribution of the amount of slider movements across trials and participants- If a participant's slider movements exceeds ±3.5 standard deviations from the mean, we will remove those participants' data.

```{r}
# Calculate movement score with magnitude for each trial
df.raw <- df.raw %>%
  rowwise() %>%
  mutate(trial_movement_score_magnitude = 
           ifelse(slider_values[[1]][1] == 0, sum(abs(diff(unlist(slider_values)))), NA))

# Aggregate these scores for each participant
participant_movement_scores <- df.raw %>%
  group_by(participant) %>%
  summarise(movement_score_all = sum(trial_movement_score_magnitude, na.rm = TRUE))

# Add the movement scores back to the df
df.raw <- left_join(df.raw, participant_movement_scores, by = "participant")

# Calculate mean and standard deviation for movement scores, identify outliers
mean_movement_score <- mean(participant_movement_scores$movement_score_all, na.rm = TRUE)
sd_movement_score <- sd(participant_movement_scores$movement_score_all, na.rm = TRUE)

cutoff_upper <- mean_movement_score + 3.5 * sd_movement_score
cutoff_lower <- mean_movement_score - 3.5 * sd_movement_score

outliers <- participant_movement_scores %>%
  filter(movement_score_all < cutoff_lower | movement_score_all > cutoff_upper) %>%
  pull(participant)

# Print participant IDs for those outside the ±3.5 SD range
cat("Participants outside the ±3.5 SD range:", paste(outliers, collapse = ", "), "\n")
```

### Remove the participants that failed the criteria. 
p_12, p_16, p_2, p_20, p_23, p_26, p_27, p_6 failed the question accuracy criterion.
```{r}
# Remove failed participants
df.raw_success <- df.raw %>%
  filter(participant != "p_12")
df.raw_success <- df.raw_success %>%
  filter(participant != "p_16")
df.raw_success <- df.raw_success %>%
  filter(participant != "p_2")
df.raw_success <- df.raw_success %>%
  filter(participant != "p_20")
df.raw_success <- df.raw_success %>%
  filter(participant != "p_23")
df.raw_success <- df.raw_success %>%
  filter(participant != "p_26")
df.raw_success <- df.raw_success %>%
  filter(participant != "p_27")
df.raw_success <- df.raw_success %>%
  filter(participant != "p_6")
# check
count(df.raw_success, participant)
```

## Save the df 
```{r}
# Convert list to string for slider values
df.raw_success$slider_values <- sapply(df.raw_success$slider_values, 
                                      function(x) paste(x, collapse = ","))

output_file_path <- file.path(organized_data_path, "organized_data.csv")
write.csv(df.raw_success, output_file_path, row.names = FALSE)

```

# ANALYSIS

Typos in the summaries participants wrote are corrected first using this code and then manually: *analysis/correct_typos.ipynb*

Semantic similarity between the summaries participants wrote and segments they heard in each trial are calculated using the code in: *analysis/semantic_similarity.ipynb*
data file with semantic similarity scores are in: *data/organized_data/df_semantic.csv*

```{r}
# upload data after semantic similarity calculations
df.data <- read.csv(file.path(organized_data_path, "df_semantic.csv"))
```

```{r}
# Calculate median trial movements for each trial

df.data <- df.data %>%
  mutate(
    slider_values_numeric = str_split(slider_values_rescaled, ",") %>%
      map(~ as.numeric(.x))
  )

# Mean and median slider values for each trial
df.data <- df.data %>%
  rowwise() %>%
  mutate(trial_mean_rescaled = mean(unlist(slider_values_numeric), na.rm = TRUE),
         trial_median_rescaled = median(unlist(slider_values_numeric), na.rm = TRUE)) %>%
  ungroup()
```

## I. Semantic Similarity
Which semantic similarity analysis explains comprehension better?

### Visualization
First, let's visualize all 3 to see if they show the same pattern:

```{r figure-size, fig.width=9, fig.height=5}

# GloVe - Written Summary
average_similarity_summary <- df.data %>%
  group_by(speech_rate) %>%
  summarise(
    average_similarity = mean(glove_sum_max_similarity_rescaled, na.rm = TRUE),
    sd = sd(glove_sum_max_similarity_rescaled, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n)  # standard error
  )


# GloVe - Heard Segment
average_similarity_text <- df.data %>%
  group_by(speech_rate) %>%
  summarise(
    average_similarity = mean(glove_text_average_max_similarity, na.rm = TRUE),
    sd = sd(glove_text_average_max_similarity, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n)  # standard error
  )

# BERT 
average_similarity_bert <- df.data %>%
  group_by(speech_rate) %>%
  summarise(
    average_similarity = mean(bert_cosine_similarity, na.rm = TRUE),
    sd = sd(bert_cosine_similarity, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n)  # standard error
  )


# Create the plots without x and y labels
plot_glove_summary <- ggplot(average_similarity_summary, aes(x = factor(speech_rate), 
                               y = average_similarity, 
                               fill = as.factor(speech_rate))) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7, fill = "green3") +
  scale_x_discrete(labels = function(x) paste0("x ", x)) +
  labs(x = NULL, y= NULL, 
       title = "GloVe- Written Summary") +
  geom_text(aes(label = round(average_similarity, 2)), 
            vjust = -1, position = position_dodge(width = 0.7), size = 3) +
  geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se), 
                width = 0, position = position_dodge(width = 0.7)) + 
  theme(axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14),
        plot.title = element_text(size = 13, hjust = 0.5))

plot_glove_text <- ggplot(average_similarity_text, aes(x = factor(speech_rate), 
                               y = average_similarity, 
                               fill = as.factor(speech_rate))) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7, fill = "darkseagreen2") +
  scale_x_discrete(labels = function(x) paste0("x ", x)) +
  labs(x = NULL, y = NULL, 
       title = "GloVe - Heard Segment") +
  geom_text(aes(label = round(average_similarity, 2)), 
            vjust = -1, position = position_dodge(width = 0.7), size = 3) +
  geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se), 
                width = 0,position = position_dodge(width = 0.7)) +
  theme(plot.title = element_text(size = 13, hjust = 0.5))

# Adjust the plot for BERT without individual axis labels
plot_bert <- ggplot(average_similarity_bert, aes(x = factor(speech_rate), 
                               y = average_similarity, 
                               fill = as.factor(speech_rate))) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7, fill = "darkseagreen3") +
  scale_x_discrete(labels = function(x) paste0("x ", x)) +
  labs(x = NULL, y = NULL, 
       title = "BERT") +
  geom_text(aes(label = round(average_similarity, 2)), 
            vjust = -1.2, position = position_dodge(width = 0.7), size = 3) +
  geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se), 
                width = 0, position = position_dodge(width = 0.7)) +
  theme(plot.title = element_text(size = 13, hjust = 0.5))

#put them in a grid 
grid.arrange(
  arrangeGrob(
  plot_glove_summary,plot_glove_text, plot_bert, 
    ncol = 3,
    left = textGrob("Average Normalized Cosine Similarity Value", 
                    rot = 90, vjust = 0.5),
   bottom = textGrob("Speech Speed", 
                    rot = 0, vjust = 0.5)
  ) 
)
```
### Correlation

Do 3 different semantic similarity measures correlate with each other?
```{r}
# Subset the relevant columns
semantic_columns <- df.data[, c("bert_cosine_similarity", 
                                "glove_text_average_max_similarity", 
                                "glove_sum_max_similarity_rescaled")]

# Calculate the correlation matrix
correlation_matrix <- cor(semantic_columns, use = "complete.obs")

# Print 
print(correlation_matrix)

```
Test if any of these correlations are statistically significant:
```{r}
# Correlation test between BERT and GloVe- Heard Segment
test1 <- cor.test(df.data$bert_cosine_similarity, 
                  df.data$glove_text_average_max_similarity)

# Correlation test between BERT and GloVe- Written Summary
test2 <- cor.test(df.data$bert_cosine_similarity, 
                  df.data$glove_sum_max_similarity_rescaled)

# Correlation test between GloVe- Heard Segment & GloVe- Written Summary
test3 <- cor.test(df.data$glove_text_average_max_similarity,
                  df.data$glove_sum_max_similarity_rescaled)

print(test1)
print(test2)
print(test3)

```

### Comparison
Which semantic similarity analysis method explains the most varience in comprehension?

To see which if  semantic similarity scores explains variance in comprehension better than the others,we conducted a linear mixed model was conducted to predict the 10-Point Scale responses based on BERT, GloVe- Written Summary, and GloVe- Heard Segment. Random effects for digit span score and digit in noise score were included in the model.


```{r}
model_all<- lmer(likert_rescaled ~ 
                 bert_cosine_similarity +   glove_text_average_max_similarity +   
                   glove_sum_max_similarity_rescaled + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)
#model_compare <- anova(model_g1, model_all)
summary(model_all) 
```
All fixed effects (different sem similarity scores) are significant, they are finding independent variance, interestingly. Glove- Written Summary has the highest t value and estimate. Therefore, I will use/ report the GLoVe- written summary Segment for the rest of the analyses.   

### Speech Speed 
Are semantic similarity scores btw speech rates sig different from each other?

#### Glove- Written Summary
```{r}
semantic.anova <- aov(glove_sum_max_similarity_rescaled ~ speech_rate, data = df.data)
summary(semantic.anova)

pairwise_results <- df.data %>% pairwise_t_test(
    glove_sum_max_similarity_rescaled ~ speech_rate, pool.sd = FALSE,
    p.adjust.method = "bonferroni"
    )
pairwise_results
```
All significant except x1-x2

#### Glove- Heard Segment
```{r}
semantic.anova2 <- aov(glove_text_average_max_similarity ~ speech_rate, data = df.data)
summary(semantic.anova2)

pairwise_results2 <- df.data %>% pairwise_t_test(
    glove_text_average_max_similarity ~ speech_rate, pool.sd = FALSE,
    p.adjust.method = "bonferroni"
    )
pairwise_results2
```
All significant except x1-x2 & x2-x3

#### BERT
```{r}
semantic.anova3 <- aov(bert_cosine_similarity ~ speech_rate, data = df.data)
summary(semantic.anova3)

pairwise_results3 <- df.data %>% pairwise_t_test(
    bert_cosine_similarity ~ speech_rate, pool.sd = FALSE,
    p.adjust.method = "bonferroni"
    )
pairwise_results3
```
All significant except except x2-x3

### Recency Effect
Is there a recency effect in summaries that participants wrote?

#### Glove- Written Summary
```{r}
# First, make sure that the right ones are NA, right now NAs are also 0
df.data <- df.data %>%
  mutate(
    rescaled_glove_bin1 = ifelse(textbox.text_corrected_manual == "", 
                                 NA, rescaled_glove_bin1),
    rescaled_glove_bin2 = ifelse(textbox.text_corrected_manual == "", 
                                 NA, rescaled_glove_bin2),
    rescaled_glove_bin3 = ifelse(textbox.text_corrected_manual == "", 
                                 NA, rescaled_glove_bin3),
    rescaled_glove_bin4 = ifelse(textbox.text_corrected_manual == "", 
                                 NA, rescaled_glove_bin4),
    rescaled_glove_bin5 = ifelse(textbox.text_corrected_manual == "", 
                                 NA, rescaled_glove_bin5)
  )

# Create a long dataset for bins and scores
df.bin <- df.data %>%
  select(participant, index, speech_rate, segment_text, textbox.text_corrected_manual, 
         glove_bin1, glove_bin2, glove_bin3, glove_bin4, glove_bin5) %>%
  pivot_longer(cols = starts_with("glove_bin"), 
               names_to = "bin", 
               names_prefix = "glove_bin", 
               values_to = "semantic_score") %>%
  mutate(bin = as.integer(bin)) %>%
  arrange(participant, index, bin)

# If a recency effect ->  latter bins should be recalled better than earlier ones
binned.anova <- aov(semantic_score ~ speech_rate * bin, data = df.bin)
summary(binned.anova)

df.bin_factor = df.bin %>%
  mutate(speech_bin = factor(paste(speech_rate, bin, sep="_")))

# pairwise comparison
df.bin_factor %>% pairwise_t_test(
   semantic_score ~ speech_bin, pool.sd = FALSE,
    p.adjust.method = "bonferroni"
    )
```

#### Glove- Heard Segment
```{r}
# First, make sure that the right ones are NA, right now NAs are also 0

df.data <- df.data %>%
  mutate(
    glove2_bin1 = ifelse(textbox.text_corrected_manual == "", NA, glove2_bin1),
    glove2_bin2 = ifelse(textbox.text_corrected_manual == "", NA, glove2_bin2),
    glove2_bin3 = ifelse(textbox.text_corrected_manual == "", NA, glove2_bin3),
    glove2_bin4 = ifelse(textbox.text_corrected_manual == "", NA, glove2_bin4),
    glove2_bin5 = ifelse(textbox.text_corrected_manual == "", NA, glove2_bin5)
  )

# Create a long dataset for bins and scores 
df.bin_glove2 <- df.data %>%
  select(participant, index, speech_rate, segment_text, textbox.text_corrected_manual, 
         glove2_bin1, glove2_bin2, glove2_bin3, glove2_bin4, glove2_bin5) %>%
  pivot_longer(cols = starts_with("glove2_bin"), 
               names_to = "bin", 
               names_prefix = "glove2_bin", 
               values_to = "glove2_score") %>%
  mutate(bin = as.integer(bin)) %>%
  arrange(participant, index, bin)

# If a recency effect ->  latter bins should be recalled better than earlier ones

binned.anova2 <- aov(glove2_score ~ speech_rate * bin, data = df.bin_glove2)
summary(binned.anova2)

df.bin_factor = df.bin_glove2 %>%
  mutate(speech_bin = factor(paste(speech_rate, bin, sep="_")))

# pairwise comparison
df.bin_factor %>% pairwise_t_test(
   glove2_score ~ speech_bin, pool.sd = FALSE,
    p.adjust.method = "bonferroni"
    )
```

#### BERT
```{r}
# Create a long dataset for bins and scores for BERT
df.bin_bert <- df.data %>%
  select(participant, index, speech_rate, segment_text, textbox.text_corrected_manual, 
         cosine_similarity_bin1, 
         cosine_similarity_bin2, 
         cosine_similarity_bin3, 
         cosine_similarity_bin4, 
         cosine_similarity_bin5) %>%
  pivot_longer(cols = starts_with("cosine_similarity_bin"), 
               names_to = "bin", 
               names_prefix = "cosine_similarity_bin", 
               values_to = "bert_score") %>%
  mutate(bin = as.integer(bin)) %>%
  arrange(participant, index, bin)

binned.anova_bert <- aov(bert_score ~ speech_rate * bin, data = df.bin_bert)
summary(binned.anova_bert)

df.bin_bert_factor = df.bin_bert %>%
  mutate(speech_bin = factor(paste(speech_rate, bin, sep="_")))

# pairwise comparison
df.bin_bert_factor %>% pairwise_t_test(
   bert_score ~ speech_bin, pool.sd = FALSE,
    p.adjust.method = "bonferroni"
    )
```

There is a main effect of speech speed for all 3, main effect of binning (recency) only for GloVe- Heard Summary & BERT.

## II. Validation
Can we predict the scores of our novel comprehension measure from the existing post-hoc comprehension measures?

### Each measure as a fixed effect in seperate models

#### 10-Point Scale Ratings

```{r}
model_likert<- lmer(trial_median_rescaled ~ 
                likert_rescaled + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_likert)
```

##### Visualize 10-Point Scale Ratings per Sepeech Speed
```{r figure-size, fig.width=6, fig.height=6 }

# Calculate the average 10-point scale scores per speech_rate category
average_score_likert <- df.data %>%
  group_by(speech_rate) %>%
  summarise(average_likert = mean(likert_rescaled, na.rm = TRUE),
            sd = sd(likert_rescaled, na.rm = TRUE),
            n = n(),
            se = sd / sqrt(n)  # standard error
  )

# Plot
likert_plot <- ggplot(average_score_likert, aes(x = factor(speech_rate), 
                                                y = average_likert)) +
  geom_bar(stat = "identity", width = 0.7, fill = "royalblue") +   
  labs(x = "Speech Speed", y = "10-Point Scale Response") +
  geom_text(aes(label = round(average_likert, 2)), vjust = -0.2, size = 6) +
  scale_x_discrete(labels = function(x) paste0("x ", x)) +
  geom_errorbar(aes(ymin = average_likert - se, ymax = average_likert + se), 
                width = 0, position = position_dodge(width = 0.5)) +  
  scale_y_continuous(breaks = c(0, 0.25, 0.50)) + 
  theme(axis.title.x = element_text(size = 24),
        axis.title.y = element_text(size = 24),
        axis.text.x = element_text(size = 20), 
        axis.text.y = element_text(size = 20))  

print(likert_plot)
```

#### Multiple Choice Quesitons
```{r}
model_multiple_choice<- lmer(trial_median_rescaled ~ 
                multiple_choice_accuracy + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_multiple_choice)
```

##### Visualize Multiple Choice Quesitons per Speech Speed
```{r figure-size, fig.width=6, fig.height=6 }
# Calculate the average multiple_choice_accuracy scores per speech_rate category
average_scores <- df.data %>%
  group_by(speech_rate) %>%
  summarise(average_accuracy = mean(multiple_choice_accuracy, na.rm = TRUE),
            sd = sd(multiple_choice_accuracy, na.rm = TRUE),
            n = n(),
            se = sd / sqrt(n)  # standard error
  )

question_plot <- ggplot(average_scores, aes(x = factor(speech_rate), y = average_accuracy)) +
  geom_bar(stat = "identity", width = 0.7, fill = "violetred") +   
  labs(x = "Speech Speed", y = "Multiple Choice Accuracy") +
  geom_text(aes(label = round(average_accuracy, 2)), vjust = -0.5, size = 6) +
  scale_x_discrete(labels = function(x) paste0("x ", x)) +
  geom_errorbar(aes(ymin = average_accuracy - se, ymax = average_accuracy + se), 
                width = 0, position = position_dodge(width = 0.5)) +
  # Add dashed line at chance level:
  geom_hline(yintercept = 0.25, linetype = "dashed", color = "black", size=1.5) +  
  scale_y_continuous(breaks = c(0, 0.25, 0.5)) +  
  theme(axis.title.x = element_text(size = 24),
        axis.title.y = element_text(size = 24),
        axis.text.x = element_text(size = 20),  
        axis.text.y = element_text(size = 20))  

print(question_plot)
```

#### Summary
```{r}
# Summary- GloVe- Written Summary
model_glove1<- lmer(trial_median_rescaled ~ 
                glove_sum_max_similarity_rescaled + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_glove1)
```

##### Visualize Semantic Simlarity Scores of Summaries per Speech Speed
```{r figure-size, fig.width=6, fig.height=6}
plot_glove_summary <- ggplot(average_similarity_summary, aes(x = factor(speech_rate), 
                               y = average_similarity, 
                               fill = as.factor(speech_rate))) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7, fill = "green3") +
  scale_x_discrete(labels = function(x) paste0("x ", x)) +
  labs(x = "Speech Speed", y= "Semantic Similarity\nScore") +
  geom_text(aes(label = round(average_similarity, 2)), 
            vjust = -0.5, position = position_dodge(width = 0.7), size = 6) +
  geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se), 
                width = 0, position = position_dodge(width = 0.7)) + 
  scale_y_continuous(breaks = c(0, 0.25, 0.5), limits = c(0, 0.5)) +  
  theme(axis.title.x = element_text(size = 24),
        axis.title.y = element_text(size = 24),
        axis.text.x = element_text(size = 20),  
        axis.text.y = element_text(size = 20), 
        plot.title = element_text(size = 20, hjust = 0.5))

plot_glove_summary
```

### Visualize single fixed effect regression models
```{r figure-size, fig.width=6, fig.height=6 }
visreg_summary <- visreg(model_glove1, "glove_sum_max_similarity_rescaled", gg=TRUE,    
                         line=list(col="green3", size=1.7), points=list(alpha=0.5),
                         xlab= "Semantic Similarity Score", 
                         ylab= "Predicted Real-Time\nComprehension Score") +
  theme(axis.title.x = element_text(size = 24),
        axis.title.y = element_text(size = 24),
        axis.text.x = element_text(size = 20),  # Adjust X-axis tick label size
        axis.text.y = element_text(size = 20)) +  # Adjust Y-axis tick label size
  scale_x_continuous(breaks = c(0, 0.5, 1)) +  # Set x-axis ticks at 0, 0.5, 1
  scale_y_continuous(breaks = c(0, 0.5, 1)) 


visreg_likert <- visreg(model_likert, "likert_rescaled", gg=TRUE,
                        line=list(col="royalblue",size=1.7), points=list(alpha=0.5),
                        xlab= "10-point Scale Response", 
                        ylab= "") +
theme(axis.title.x = element_text(size = 24),
        axis.title.y = element_text(size = 24),
        axis.text.x = element_text(size = 20),  # Adjust X-axis tick label size
        axis.text.y = element_text(size = 20)) +  # Adjust Y-axis tick label size
  scale_x_continuous(breaks = c(0, 0.5, 1)) +  # Force x-axis to include 0, 0.5, 1
  scale_y_continuous(breaks = c(0, 0.5, 1))


visreg_multiple <- visreg(model_multiple_choice, "multiple_choice_accuracy", gg=TRUE,
                          line=list(col="violetred", size=1.7), points=list(alpha=0.5),
                          xlab= "Multiple Choice Accuracy", 
                          ylab= "") +
  theme(axis.title.x = element_text(size = 24),
        axis.title.y = element_text(size = 24),
        axis.text.x = element_text(size = 20),  # Adjust X-axis tick label size
        axis.text.y = element_text(size = 20)) +  # Adjust Y-axis tick label size
  scale_x_continuous(breaks = c(0, 0.5, 1)) +  # Set x-axis ticks at 0, 0.5, 1
  scale_y_continuous(breaks = c(0, 0.5, 1))


visreg_summary
visreg_likert
visreg_multiple
```

### Visualize regression models + Bar plots 
```{r figure-size, fig.width=18, fig.height=14}
# Create a multi-panel plot for 

# Create title groups
title1 <- textGrob("Summary", gp=gpar(fontsize=28, color='green3',fontface="bold"))
title2 <- textGrob("10-Point Scale", gp=gpar(fontsize=28, color='royalblue', fontface="bold"))
title3 <- textGrob("Multiple Choice Question", gp=gpar(fontsize=28,color='violetred', fontface="bold"))

combined_plot_effects <- grid.arrange(
  title1, title2, title3,
  visreg_summary, visreg_likert, visreg_multiple,
  plot_glove_summary, likert_plot, question_plot,
  ncol = 3,
  nrow = 4,
  heights = unit.c(unit(4, "null"), unit(8, "null"), unit(2, "null"), unit(8, "null")),  # Adjust heights
  layout_matrix = rbind(c(1, 2, 3), c(4, 5, 6), c(NA, NA, NA), c(7, 8, 9))
)

print(combined_plot_effects)
```

## All post-hoc measures as fixed effects in one regression model
Now predict the novel measure's scores from all post-hoc comprehension measures to see if they can explain independent variance.

```{r}
model_all_glove1 <- lmer(trial_median_rescaled ~  
                glove_sum_max_similarity_rescaled +
                likert_rescaled + 
                multiple_choice_accuracy + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_all_glove1)
```

### Visualize the multiple fixed effects regression
```{r figure-size, fig.width=15, fig.height=5 }

visreg_summary_all <- visreg(model_all_glove1, "glove_sum_max_similarity_rescaled", 
                             gg=TRUE,
                          line=list(col="green3", size = 1.7), points=list(alpha=0.5),
                       xlab= "Semantic Similarity Score", 
                       ylab= "Predicted Real-Time\nComprehension Score") + 
  theme(axis.title.x = element_text(size = 24),
        axis.title.y = element_text(size = 24),
        axis.text.x = element_text(size = 20),  
        axis.text.y = element_text(size = 20)) +  
  scale_x_continuous(breaks = c(0, 0.5, 1)) +  
  scale_y_continuous(breaks = c(0, 0.5, 1))    

# Update for visreg_likert
visreg_likert_all <- visreg(model_all_glove1, "likert_rescaled", gg=TRUE,
                        line=list(col="royalblue",size=1.7), points=list(alpha=0.5),
                       xlab= "10-point Scale Response",
                       ylab= NULL)+ 
  theme(axis.title.x = element_text(size = 24),
        axis.title.y = element_text(size = 24),
        axis.text.x = element_text(size = 20),  
        axis.text.y = element_text(size = 20)) +  
  scale_x_continuous(breaks = c(0, 0.5, 1)) +  
  scale_y_continuous(breaks = c(0, 0.5, 1))    

# Update for visreg_multiple
visreg_multiple_all <- visreg(model_all_glove1, "multiple_choice_accuracy", gg=TRUE,
                           line=list(col="violetred", size=1.7), points=list(alpha=0.5),
                       xlab= "Multiple Choice Accuracy", 
                       ylab= NULL) +
  theme(axis.title.x = element_text(size = 24),
        axis.title.y = element_text(size = 24),
        axis.text.x = element_text(size = 20),  
        axis.text.y = element_text(size = 20)) +  
  scale_x_continuous(breaks = c(0, 0.5, 1)) +  
  scale_y_continuous(breaks = c(0, 0.5, 1))    

# Arrange plots in a grid
grid.arrange(
  arrangeGrob(
    visreg_summary_all, visreg_likert_all, visreg_multiple_all, 
    ncol = 3
  ) 
)


# For random slopes 
visreg_wm <- visreg(model_all_glove1, "digit_span_score", gg=TRUE,
                       xlab= "Working Memory Scores", 
                       ylab= NULL) 
  #scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))

visreg_hearing <- visreg(model_all_glove1, "digit_in_noise_score", gg=TRUE,
                       xlab= "Hearing In Noise", 
                       ylab= NULL)
  #scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))
  

# put them in a grid
grid.arrange(
  arrangeGrob(
   visreg_wm, visreg_hearing,
    ncol = 2,
    left = textGrob(" Predicted real-time comprehension scores 
                    given the post-hoc measures", 
                    rot = 90, vjust = 1.5)
  ) 
)
```

## Figure: Single fixed-effect regressions + bar plots + multiple fixed-effect regression
```{r figure-size, fig.width=18, fig.height=22}
# all 3 plots in the same grid
# Create title groups
title1 <- textGrob("Summary", gp=gpar(fontsize=28, 
                                      color='green3',
                                      fontface="bold"))
title2 <- textGrob("10-Point Scale", gp=gpar(fontsize=28, 
                                             color='royalblue', 
                                             fontface="bold"))
title3 <- textGrob("Multiple Choice Question", gp=gpar(fontsize=28,
                                                       color='violetred', 
                                                       fontface="bold"))

combined_plot_effects <- grid.arrange(
  title1, title2, title3,
  visreg_summary, visreg_likert, visreg_multiple,
  plot_glove_summary, likert_plot, question_plot,
  visreg_summary_all, visreg_likert_all, visreg_multiple_all,
  ncol = 3,
  nrow = 6,
  heights = unit.c(
    unit(4, "null"),  # Title row
    unit(8, "null"),  # First row of plots
    unit(2, "null"),  # Spacer row
    unit(8, "null"),  # Second row of plots
    unit(2, "null"),  # Spacer row (optional)
    unit(8, "null")   # Third row of plots
  ),
  layout_matrix = rbind(
    c(1, 2, 3),
    c(4, 5, 6),
    c(NA, NA, NA),
    c(7, 8, 9),
    c(NA, NA, NA),
    c(10, 11, 12)
  )
)


print(combined_plot_effects)
```

## Predicting real time comprehension from speech rate 
Does comprehension change with speech speed?

```{r}
model_rate <- lmer(speech_rate ~ 
                trial_median_rescaled +
                glove_sum_max_similarity_rescaled +
                likert_rescaled + 
                multiple_choice_accuracy + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_rate)
```

### Figure: Estimates of fixed effects (comprehension measures) on speech speed
```{r}
# Extract the fixed effects from the model (including confidence intervals)
fixed_effects <- tidy(model_rate, effects = "fixed", conf.int = TRUE)

# Filter out the intercept term
fixed_effects <- fixed_effects %>%
  filter(term != "(Intercept)")

# Map the actual terms to custom axis labels
term_mapping <- c(
  "multiple_choice_accuracy" = "Multiple Choice Question Accuracy",
  "glove_sum_max_similarity_rescaled" = "Summary Semantic Similarity",
  "likert_rescaled" = "10-Point Scale",
  "trial_median_rescaled" = "Median Real-Time Comprehension"
)

# Replace the term names with custom labels
fixed_effects <- fixed_effects %>%
  mutate(term = recode(term, !!!term_mapping))

# Reorder the terms based on the estimate values in descending order
fixed_effects <- fixed_effects %>%
  mutate(term = reorder(term, -estimate))

# Add a column for significance stars based on p-values
fixed_effects <- fixed_effects %>%
  mutate(sig_stars = case_when(
    p.value < 0.001 ~ "***",
    p.value < 0.01 ~ "**",
    p.value < 0.05 ~ "*",
    TRUE ~ ""
  ))

# Plot with ggplot2 - swapped x and y axes
ggplot(fixed_effects, aes(y = term, x = estimate)) +  # Swapped axes
  geom_point(aes(color = term), size = 3) +  # Add color to the points
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high, color = term), 
                 height = 0, 
                 width = 0) +  
  geom_text(aes(label = paste0(round(estimate, 2), sig_stars)), vjust = -1, size = 4) +  
  scale_color_manual(values = c(
    "Multiple Choice Question Accuracy" = "violetred",
    "10-Point Scale" = "royalblue",
    "Summary Semantic Similarity" = "green3",
    "Median Real-Time Comprehension" = "purple"
  )) + 
  labs(title = " ",
       x = "Regression Coefficient Estimates",
       y = NULL) +  
  theme(
    plot.title = element_text(hjust = 0.5, size = 18),  
    axis.title.x = element_text(size = 13),              
    axis.text.y = element_text(size = 13, color = "black"),  
    axis.text.x = element_text(size = 12) 
  ) + 
  guides(color = "none") 

```

