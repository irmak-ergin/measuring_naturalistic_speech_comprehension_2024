# Data Wrangling

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
# Load libraries
library(stringr)
library(fs)
library(dplyr)
library(readxl)
library(readr)
library(simr)
library(lmerTest)
library(patchwork)
library(RColorBrewer)
library(cowplot)
library(ggplot2)
library(tidyr) 
library(purrr)
library(scales)
library(gridExtra)
library(PerformanceAnalytics)
library(corrplot)
library(Hmisc)
library(GGally)
library(visreg)
library(broom.mixed)
library(sjPlot)
library(gridExtra)
library(grid) 
library(rempsyc)
library(flextable)
library(officer)
library(ggsignif)
library(MuMIn)
library(rstatix)
library(caret)


# set the default ggplot theme 
theme_set(theme_classic())
```

```{r}
# Set path and participant ids

# Change the folder path to final-project-irmak-ergin/data relative to your wd
folder_path <- '/Users/irmakergin/Desktop/2023_speech_rate/data_all/data_experiment'

# Define the list of participant IDs
participant_ids <- c("p_1","p_2","p_3","p_4","p_5", "p_6", "p_7", "p_8", "p_9","p_10", 
                     "p_11", "p_12", "p_13", "p_14","p_15","p_16","p_17","p_18","p_19","p_20",
                      "p_21", "p_22", "p_23", "p_24","p_25","p_26","p_27","p_28","p_29","p_30")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Organize the data csv files

# Get a list of all files in the folder
files <- dir(folder_path, full.names = TRUE)

# Set a path to save the organized files
organized_data_path<-"/Users/irmakergin/Desktop/2023_speech_rate/data_all/organized_data_experiment"

# Find files by participant ID 
for(part_id in participant_ids) {

  # Match the slider files that involve the participant ID
  slider_pattern <- paste0("slider_", part_id, ".*\\.csv$")
  
  # Match data (csv) files that start with the participant ID
  additional_csv_pattern <- paste0("^", part_id, "_.*\\.csv$")
  
  # Find slider files that match the pattern
  slider_matched_files <- files[str_detect(basename(files), slider_pattern)]
  
  # Find data CSV files that match the pattern
  additional_csv_matched_files <- files[str_detect(basename(files), 
                                                   additional_csv_pattern)]
   
  # Create a directory for the current participant on the organized data path
  participant_folder <- file.path(organized_data_path, part_id)
  if(!dir.exists(participant_folder)) {
    dir.create(participant_folder, recursive = TRUE, showWarnings = TRUE)
  }
  
  # Create a subdirectory for slider files within the participant folder
  slider_folder <- file.path(participant_folder, paste0("slider_", part_id))
  if(!dir.exists(slider_folder)) {
    dir.create(slider_folder, recursive = TRUE, showWarnings = TRUE)
  }
  
  # Move slider matched files to the slider subfolder
  if(length(slider_matched_files) > 0) {
    for(file in slider_matched_files) {
      file_copy(file, file.path(slider_folder, basename(file)), overwrite = TRUE)
    }
  }
  
  # Move additional CSV matched files (experiment data) to the main participant folder 
  #and rename the participant folders
  if(length(additional_csv_matched_files) > 0) {
  # loop to rename the file during the copy process
  for(file in additional_csv_matched_files) {
    # New filename with '_raw_data'
    new_name <- paste0(part_id, "_raw_data.csv")
    # Copy and rename the file to the new name in the participant folder
    file_copy(file, file.path(participant_folder, new_name), overwrite = TRUE)
  }
}
  # check
  if(length(slider_matched_files) > 0 || length(additional_csv_matched_files) > 0) {
    cat("Files for participant ID '", part_id, 
        "' have been organized successfully. Slider files in: ", slider_folder, 
        ", other CSVs in: ", participant_folder, "\n")
  } else {
    cat("No files found for participant ID '", part_id, "'.\n")
  }
}

# # Put excel files (of digit in noise and digit span) in the organized data folder 
# 
#source_folder<-"/Users/irmakergin/Desktop/data_all/data_experiment"
# destination_folder<-"//Users/irmakergin/Desktop/2023_speech_rate/organized_data_experiment"
# 
# List all Excel files in the source folder
#excel_files <- dir_ls(path = source_folder, glob = "*.xlsx")

# Copy each Excel file to the destination folder- commented out to re-run
#for (file_path in excel_files) {
#dest_file_path <- file.path(destination_folder)
#file_copy(file_path, dest_file_path)
# # Check
# cat("Copied:", file_path, "to", dest_file_path, "\n")
#}

# Loop through each participant ID
for (part_id in participant_ids) {
  # Define the path to the participant's raw data CSV
  raw_data_path <- file.path(organized_data_path, 
                             part_id, 
                             paste0(part_id, "_raw_data.csv"))

# Read the CSV for each participant (if it exists)
if (file.exists(raw_data_path)) {
  df.raw <- read_csv(raw_data_path) %>%
    filter(!wav_file %in% c("Someday_trial_condition.wav", 
                            "Someday_trial_condition-5x.wav")) %>%
    mutate(index = row_number()) %>%
    select(index, participant, wav_file, old_name, segment_text, question, 
           C1, C2, C3, C4, C_correct, duration,
           A.numClicks, B.numClicks, C.numClicks, D.numClicks, slider.response, 
           summary, `textbox.text`) %>%
    group_by(wav_file) %>%
    summarise(
      index = first(index), 
      participant = first(participant), 
      old_name = first(old_name),
      segment_text = first(segment_text),
      question = first(question),
      C1 = first(C1),
      C2 = first(C2),
      C3 = first(C3),
      C4 = first(C4),
      C_correct = first(C_correct),
      duration = first(duration),
      summary = first(summary),
      textbox.text = first(textbox.text),
      A.numClicks = max(A.numClicks, na.rm = TRUE),
      B.numClicks = max(B.numClicks, na.rm = TRUE),
      C.numClicks = max(C.numClicks, na.rm = TRUE),
      D.numClicks = max(D.numClicks, na.rm = TRUE),
      likert_response = max(slider.response, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    arrange(index) %>%
    mutate(index = row_number()) 
  
  # Fill NA values in participant_id column with part_id for participant p_3
    if (part_id == "p_3") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
    if (part_id == "p_1") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
    if (part_id == "p_2") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
    if (part_id == "p_14") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
    if (part_id == "p_13") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
    if (part_id == "p_26") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
   # Remove the extra last row from df.raw
    df.raw <- head(df.raw, n = -1)
   
  #Assign processed data of each participant to a new df
  assign(paste0("df.raw.merged_", part_id), df.raw, envir = .GlobalEnv)
  }
}
# Sanity check
# how many rows do we have- should be 125
nrow(df.raw.merged_p_2)
```
```{r}
# p_14 raw 83 is extra ##DOUBLE CHECK
df.raw.merged_p_14 <- df.raw.merged_p_14[-83, ]
df.raw.merged_p_14$index <- 1:nrow(df.raw.merged_p_14)
```

```{r}
# Merge all participant dfs into one df

# Create an empty data frame to store merged data
df.raw <- data.frame()

# Loop through each participant ID and merge their df
for (part_id in participant_ids) {
  # Construct the name of the data frame variable
  df_name <- paste0("df.raw.merged_", part_id)
  
  # Check if the data frame exists in the global environment
  if (exists(df_name, envir = .GlobalEnv)) {
    # Get the data frame from its name
    df_participant <- get(df_name, envir = .GlobalEnv)
    
    # Merge the data frame with the main merged data frame
    df.raw <- rbind(df.raw, df_participant)
  }
}
```

```{r}
# Add accuracy columns for multiple choice questions

# Calculate the multiple choice accuracy

df.raw<- df.raw %>%
  mutate(
    multiple_choice_accuracy = apply(., 1, function(row) {
      # Extract the first character from C_correct - it is the correct letter to click on
      correct_answer <- substr(row[["C_correct"]], 1, 1)
      
      # Determine which of the click columns matches the correct answer
      clicked_column <- ifelse(row[["A.numClicks"]] == 1, "A",
                               ifelse(row[["B.numClicks"]] == 1, "B",
                                      ifelse(row[["C.numClicks"]] == 1, "C",
                                             ifelse(row[["D.numClicks"]] == 1, "D", NA))))
      
      # Return 1 if the clicked column matches the correct answer, 0 otherwise
      if (!is.na(clicked_column) && clicked_column == correct_answer) {
        return(1)
      } else {
        return(0)
      }
    })
  )

# Change the column the order
df.raw<- df.raw[c("index", "participant", "wav_file", "old_name", 
                                 "duration","segment_text", 
                                 "question", "C1", "C2", "C3", "C4", "C_correct", 
                                 "A.numClicks", "B.numClicks", "C.numClicks",
                                 "D.numClicks", "multiple_choice_accuracy", 
                                 "likert_response", "summary","textbox.text")]
```

```{r}
# Extract the speech rate information from the trial name and create a speech_rate column
df.raw<- df.raw %>%
  mutate(speech_rate = case_when(
    str_detect(wav_file, "2x") ~ 2,
    str_detect(wav_file, "3x") ~ 3,
    str_detect(wav_file, "4x") ~ 4,
    str_detect(wav_file, "5x") ~ 5,
    TRUE ~ 1  # not sped up if there are no numbers + x in the name 
  ))
```

```{r}
# Add the Digit Span (working memory) scores of each participant

# Create an empty digit_span_score column
df.raw$digit_span_score <- NA

for (part_id in unique(df.raw$participant)) {
  # Construct the filename based on the participant ID
  filename <- paste0(organized_data_path, "/digit_span_", part_id, ".xlsx")
  
  # Check if the file exists
  if (file.exists(filename)) {
    # Correctly read the file using read_excel
    digit_span_data <- read_excel(filename)
    
    # The score is located in the first row of the 'score' column
    score <- digit_span_data$score[1]
      
    df.raw<- df.raw%>%
      mutate(digit_span_score = ifelse(participant == part_id, score, digit_span_score))
  } else {
    cat("File not found for participant", part_id, "\n")
  }
}
```

```{r}
# Add the hearWHO (digit-in-noise task for hearing in noise) scores of each participants

hearwho_pilot_data <- read_excel(paste0(organized_data_path,
                                        "/hearwho_experiment.xlsx"))

# Excel file has columns named 'participant' and 'score'
# Merge the scores into df.raw based on participant ID
df.raw <- merge(df.raw, hearwho_pilot_data[, c("participant", "score")], 
                       by.x = "participant", by.y = "participant", all.x = TRUE)

# Rename the 'score' column to 'digit_in_noise_score'
names(df.raw)[names(df.raw) == "score"] <- "digit_in_noise_score"
```

```{r}
# check
head(df.raw)
```

```{r}
# Add slider values and time points

# Check if the 'slider_values' column exists, if not, create it
if (!"slider_values" %in% names(df.raw)) {
  df.raw$slider_values <- NA
}

# Check if the 'slider_time' column exists, if not, create it
if (!"slider_time" %in% names(df.raw)) {
  df.raw$slider_time <- NA
}

# Define a function to extract and return slider values and time as strings
get_slider_values_and_time <- function(wav_file_name, part_id) {
  # Extract the trial number from wav_file name
  matches <- regmatches(wav_file_name, regexec("Someday_([0-9]+)", wav_file_name))
  if (length(matches[[1]]) < 2) { # If no match or match does not have a capture group
    return(list(values = NA, time = NA))
  }
  trial_number <- matches[[1]][2]

  # Construct the path to the slider file
  slider_file_path <- sprintf("%s/%s/slider_%s/slider_%s_001_Someday_%s_condition*.csv", 
                              organized_data_path, 
                              part_id, part_id, part_id, 
                              trial_number)

  # Find slider files matching the pattern
  slider_files <- Sys.glob(slider_file_path)
  
  if (length(slider_files) == 0) {
    return(list(values = NA, time = NA)) # No matching file found
  }
  
  slider_data <- read.csv(slider_files[1], skip = 1)

  # Prepare output for values
  slider_values_str <- if ("value" %in% colnames(slider_data)) {
    paste(slider_data$value, collapse = ",")
  } else {
    NA
  }

  # Prepare output for time, rounding to two decimals
  slider_time_str <- if ("time" %in% colnames(slider_data)) {
    rounded_times <- round(slider_data$time, 2)  # Round the time values to two decimal places
    paste(rounded_times, collapse = ",")
  } else {
    NA
  }

  return(list(values = slider_values_str, time = slider_time_str))
}

# Apply the function to each row of df.raw and extract results into separate columns
slider_info <- mapply(get_slider_values_and_time, 
                      df.raw$wav_file, 
                      df.raw$participant, 
                      SIMPLIFY = FALSE)  # Ensure output is a list to handle multiple return values

# Assign values and time from the list output to respective columns
df.raw$slider_values <- sapply(slider_info, '[[', "values")
df.raw$slider_time <- sapply(slider_info, '[[', "time")

```

```{r}
# Re-scale slider values to be between 0-1

# Convert slider_values from string to numeric lists 
df.raw$slider_values <- strsplit(as.character(df.raw$slider_values), 
                                        ",\\s*")
df.raw$slider_values <- lapply(df.raw$slider_values, 
                                      function(x) as.numeric(x))

rescale_values <- function(values) {
  sapply(values, function(x) x / 255)
}

# Apply the rescaling function to each row's slider_values
df.raw$slider_values_rescaled <- lapply(df.raw$slider_values, 
                                               rescale_values)
```

```{r}
# Sanity check: create a column showing number of slider values
df.raw <- df.raw %>%
  mutate(slider_values_number = lengths(slider_values_rescaled))
```

```{r}
# Depict slider movements

df.raw$slider_values_rescaled <- sapply(df.raw$slider_values_rescaled,
                                               function(x) paste(x, collapse = ","))

df.slider <- df.raw%>%
  mutate(slider_values_rescaled = strsplit(slider_values_rescaled, ",")) %>%
  mutate(slider_rescale_float = map(slider_values_rescaled, ~as.numeric(.x)))

df_long <- df.slider %>%
  mutate(id = row_number()) %>%
  unnest(slider_rescale_float) %>%
  rename(slider_value = slider_rescale_float) %>%
  group_by(id, speech_rate) %>%
  mutate(index = row_number()) %>%
  ungroup() %>%
  select(-slider_values_rescaled)

# Set the x-axis limits
p_slider_per_rate <- ggplot(df_long, aes(x = index,
                                         y = slider_value,
                                         group = interaction(id),
                                         color = as.factor(id))) +
  geom_line(alpha = 0.5, size = 0.5) +
  scale_x_continuous(name = "Index of the Slider Value") +
  scale_y_continuous(name = "Slider Value") +
  facet_wrap(~ speech_rate,
             scales = 'free_x',
             ncol = 1) +
  scale_color_manual(values = colorRampPalette(brewer.pal(12, "Paired"))
                     (length(unique(df_long$id)))) +
  theme(legend.position = "none") +
  labs(title = 'Slider Values by Speech Rate',
      caption = "Each line represents the comprehension trajectory at different speech rates.
      As the speech rate increases, comprehension scores seem to get lower."
)

# histograms

p_flipped_histogram_per_rate <- ggplot(df_long, aes(x = slider_value)) +
  geom_histogram(bins = 10, fill = "skyblue", color = "navy") +
  coord_flip() +
  facet_wrap(~speech_rate, scales = 'free_x', ncol = 1) +
  labs(y = "Slider Value Range", x = "Count", title = "Number of Slider Values by Speech Rate") +
  theme(legend.position = "none") +
  scale_y_continuous(labels = comma)

# combine
combined_plot <- p_slider_per_rate + p_flipped_histogram_per_rate +
  plot_layout(ncol = 2, widths = c(2, 1)) +
  theme(strip.background = element_rect(fill = "white", color = "black"))



print(combined_plot)

```

```{r}
# Sanity check for slider values 

rows_not_fitting_criteria_selected_columns <- df.raw %>%
  filter(
    (speech_rate == 1 & (slider_values_number < 6000 | slider_values_number > 8000)) |
    (speech_rate == 2 & (slider_values_number < 3000 | slider_values_number > 4000)) |
    (speech_rate == 3 & (slider_values_number < 2500 | slider_values_number > 3000)) |
    (speech_rate == 4 & (slider_values_number < 1500 | slider_values_number > 2000)) |
    (speech_rate == 5 & (slider_values_number < 1200 | slider_values_number > 1550)) 
  ) %>%
  select(participant, wav_file, index, slider_values_number)

# To print the resulting selected columns for rows that do not fit the criteria
print(rows_not_fitting_criteria_selected_columns)

# manually inspect those to see if they are just sampled slightly longer/ shorter or if the match is wrong
```
check slider files!!!

```{r, eval=FALSE}
# Only trials with the slider starting from point, with time one the x axis

# Modify df.slider to calculate a new column based on the condition
df.slider <- df.raw %>%
  mutate(
    slider_values_rescaled = strsplit(as.character(slider_values_rescaled), ","),
    slider_time_rescaled = strsplit(as.character(slider_time), ",")) %>%
  mutate(
    slider_rescale_float = map(slider_values_rescaled, ~as.numeric(.x)),
    slider_time_float = map(slider_time_rescaled, ~as.numeric(.x) / 1000)  # Convert from ms to seconds
  ) %>%
  rowwise() %>%
  mutate(include = ifelse(slider_values_rescaled[[1]][1] == 0, TRUE, FALSE)) %>%
  ungroup()

# Create a long format data frame with both time and values
df_long <- df.slider %>%
  filter(include) %>%
  mutate(id = row_number()) %>%
  unnest(c(slider_rescale_float, slider_time_float)) %>%
  rename(slider_value = slider_rescale_float, slider_times = slider_time_float) %>%
  group_by(id, speech_rate) %>%
  mutate(index = row_number()) %>%
  ungroup() %>%
  select(-slider_values_rescaled, -slider_time_rescaled, -include)  # Remove unnecessary columns for plotting

# Set the x-axis limits and labels
p_slider_per_rate <- ggplot(df_long, aes(x = slider_times,
                                         y = slider_value,
                                         group = interaction(id),
                                         color = as.factor(id))) +
  geom_line(alpha = 0.5, size = 0.5) +
  scale_x_continuous(name = "Time (seconds)") +
  scale_y_continuous(name = "Slider Value") +
  facet_wrap(~ speech_rate,
             scales = 'free_x',
             ncol = 1) +
  scale_color_manual(values = colorRampPalette(brewer.pal(12, "Paired"))
                     (length(unique(df_long$id)))) +
  theme(legend.position = "none") +
  labs(title = 'Slider Values by Speech Rate',
       caption = "Each line represents the comprehension trajectory at different speech rates.
       As the speech rate increases, comprehension scores seem to get lower.")

# Histograms
p_flipped_histogram_per_rate <- ggplot(df_long, aes(x = slider_value)) +
  geom_histogram(bins = 10, fill = "skyblue", color = "navy") +
  coord_flip() +
  facet_wrap(~speech_rate, scales = 'free_x', ncol = 1) +
  labs(y = "Slider Value Range", x = "Count", title = "Number of Slider Values by Speech Rate") +
  theme(legend.position = "none") +
  scale_y_continuous(labels = scales::comma)

# combine
combined_plot <- p_slider_per_rate + p_flipped_histogram_per_rate +
  plot_layout(ncol = 2, widths = c(2, 1)) +
  theme(strip.background = element_rect(fill = "white", color = "black"))
print(combined_plot)

```

## Rescale all 
```{r}
# slider- done
# accuracy - already 0 or 1
# summary score- most already 0 or one -> should we rescale the sums?
# post-hoc rating:

df.raw <- df.raw %>%
  mutate(likert_rescaled = likert_response / 10)
```

## Calculations for the data exclusion criteria
For the post-hoc comprehension measures, to ensure that we are validating the novel measure against actual comprehension, following exclusion criteria will be applied: 

1. Absolute threshold for comprehension: 75% correctness on the multiple choice questions for the slowest (easiest to comprehend) speech rate.

```{r}
# Find trials that do not have a number right before .wav in the wav_file column 
# (slowest condition)
attention_check_trials <- df.raw %>%
  filter(!grepl("x\\.wav$", wav_file))

# Calculate the accuracy
accuracy_results <- attention_check_trials %>%
  group_by(participant) %>%
  summarise(
    accuracy = mean(multiple_choice_accuracy == 1, na.rm = TRUE),
    n = n()
  ) %>%
  mutate(passed = if_else(accuracy >= 0.75, TRUE, FALSE))

# Print out participants who passed or failed
passed_participants <- accuracy_results %>%
  filter(passed) %>%
  pull(participant)

failed_participants <- accuracy_results %>%
  filter(!passed) %>%
  select(participant, accuracy)

if(length(passed_participants) > 0) {
  cat(paste(passed_participants, collapse = ", "), 
      "passed the question accuracy criterion\n")
}

if(nrow(failed_participants) > 0) {
  cat(paste(failed_participants$participant, "with accuracy:", failed_participants$accuracy, collapse = ", "), 
      "failed the question accuracy criterion\n")
}
```

```{r}
# histogram with all participants accuracies for speech rate 1

ggplot(accuracy_results, aes(x = accuracy)) +
  geom_histogram(binwidth = 0.05, fill = "blue", alpha = 0.7) +
  labs(title = "All Participants' Multiple Choice Question Accuracy for Speech Speed x1",
       x = "Accuracy",
       y = "Number of Participants") 
ggsave(file="multiple_density.svg")
```


```{r}
# scatter plot version of it
ggplot(accuracy_results, aes(x = participant, y = accuracy)) +
  geom_point(color = "blue", alpha = 0.7) +
  labs(title = "Scatter Plot of Participants' Accuracies for speech rate 1",
       x = "Participant ID",
       y = "Accuracy")

```

```{r}
# all participants across all speech rates 

# Calculate accuracy for each participant across all their trials
accuracy_all_trials <- df.raw %>%
  group_by(participant) %>%
  summarise(
    accuracy = mean(multiple_choice_accuracy == 1, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  )

# Histogram of all participants' accuracies
ggplot(accuracy_all_trials, aes(x = accuracy)) +
  geom_histogram(binwidth = 0.05, fill = "blue", alpha = 0.7) +
  labs(title = "Histogram of All Participants' Accuracies Across All Trials",
       x = "Accuracy",
       y = "Count")

# Scatter plot of participant accuracies
ggplot(accuracy_all_trials, aes(x = participant, y = accuracy)) +
  geom_point(color = "blue", alpha = 0.7) +
  labs(title = "Scatter Plot of Participants' Accuracies",
       x = "Participant ID",
       y = "Accuracy") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
# histogram with all participants accuracies across speech rates

# Calculate the accuracy per speech rate for each participant
accuracy_by_rate <- df.raw %>%
  group_by(participant, speech_rate) %>%
  summarise(
    accuracy = mean(multiple_choice_accuracy == 1, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  )

# Histogram of accuracies split by speech rates
ggplot(accuracy_by_rate, aes(x = accuracy, fill = as.factor(speech_rate))) +
  geom_histogram(binwidth = 0.05, alpha = 0.7, position = "identity") +
  facet_wrap(~ speech_rate, scales = "free_y") +
  labs(title = "Histogram of Participants' Accuracies by Speech Rate",
       x = "Accuracy",
       y = "Count",
       fill = "Speech Rate") 
```
```{r}
# look at the likert scale responses for these participants 
# Calculate the average slider responses (likert_rescaled) for the failed participants
failed_participants_list <- failed_participants$participant

# Filter the attention_check_trials for failed participants
failed_participants_data <- attention_check_trials %>%
  filter(participant %in% failed_participants_list)

# Calculate the average slider responses (likert_rescaled) for the relevant trials
average_slider_responses <- failed_participants_data %>%
  group_by(participant) %>%
  summarise(
    average_likert_rescaled_sr1 = mean(likert_rescaled, na.rm = TRUE),
    n = n()
  )

# Print the average slider responses
print(average_slider_responses)
```

2. Ratings on the 10-point (post-hoc likert) scale should be significantly different for the slowest and fastest speech rates.

```{r}
# t-test 
participants <- unique(df.raw$participant)

# Create a vector to store results
rating_criteria_results <- character(length(participants))

# Loop through each participant
for (i in seq_along(participants)) {
  part_id <- participants[i]
  
  # Subset data for the current participant for slowest and fastest speech rates
  subset_data <- df.raw %>%
    filter(participant == part_id & (speech_rate == 1 | speech_rate == 4)) %>%
    select(likert_response, speech_rate)
  
  # Perform a t-test comparing likert_response for speech_rate 1 vs 4
  test_result <- t.test(likert_response ~ speech_rate, data = subset_data)
  
  # Check if the difference is significant (p-value < 0.05)
  if (test_result$p.value < 0.05) {
    rating_criteria_results[i] <- paste(part_id, "passed the rating criterion")
  } else {
    rating_criteria_results[i] <- paste(part_id, "failed the rating criterion")
  }
}

# Print the results
cat(rating_criteria_results, sep = "\n")
```
3. Participants shouldn't skip all the summaries

```{r}
# If all rows under textbox.tex are empty

# Check if all 'textbox.text' entries are empty or NA for each participant
summary_criterion <- df.raw %>%
  group_by(participant) %>%
  summarise(all_empty = all(textbox.text == "" | is.na(textbox.text))) %>%
  ungroup()

# Print the results for participants where all textbox.text are empty
summary_criterion %>%
  filter(all_empty) %>%
  pull(participant) %>%
  walk(~ cat(.x, "failed summary criterion\n"))

# Print the results for participants where not all textbox.text are empty
summary_criterion %>%
  filter(!all_empty) %>%
  pull(participant) %>%
  walk(~ cat(.x, "passed summary criterion\n"))
```
For the physical slider to ensure that participants actually used it actively: Distribution of the amount of slider movements across trials and participants- If a participant's slider movements exceeds ±3.5 standard deviations from the mean, we will remove those participants' data.

```{r}
# Calculate movement score with magnitude for each trial
df.raw <- df.raw %>%
  rowwise() %>%
  mutate(trial_movement_score_magnitude = 
           ifelse(slider_values[[1]][1] == 0, sum(abs(diff(unlist(slider_values)))), NA))

# Aggregate these scores for each participant
participant_movement_scores <- df.raw %>%
  group_by(participant) %>%
  summarise(movement_score_all = sum(trial_movement_score_magnitude, na.rm = TRUE))

# Add the movement scores back to the df
df.raw <- left_join(df.raw, participant_movement_scores, by = "participant")

# Calculate mean and standard deviation for movement scores, identify outliers
mean_movement_score <- mean(participant_movement_scores$movement_score_all, na.rm = TRUE)
sd_movement_score <- sd(participant_movement_scores$movement_score_all, na.rm = TRUE)

cutoff_upper <- mean_movement_score + 3.5 * sd_movement_score
cutoff_lower <- mean_movement_score - 3.5 * sd_movement_score

outliers <- participant_movement_scores %>%
  filter(movement_score_all < cutoff_lower | movement_score_all > cutoff_upper) %>%
  pull(participant)

# Print participant IDs for those outside the ±3.5 SD range
cat("Participants outside the ±3.5 SD range:", paste(outliers, collapse = ", "), "\n")
```
Remove the participants that failed the criteria. 
p_12, p_16, p_2, p_20, p_23, p_26, p_27, p_6 failed the question accuracy criterion
```{r}
# Remove failed participants
df.raw_success <- df.raw %>%
  filter(participant != "p_12")
df.raw_success <- df.raw_success %>%
  filter(participant != "p_16")
df.raw_success <- df.raw_success %>%
  filter(participant != "p_2")
df.raw_success <- df.raw_success %>%
  filter(participant != "p_20")
df.raw_success <- df.raw_success %>%
  filter(participant != "p_23")
df.raw_success <- df.raw_success %>%
  filter(participant != "p_26")
df.raw_success <- df.raw_success %>%
  filter(participant != "p_27")
df.raw_success <- df.raw_success %>%
  filter(participant != "p_6")
# check
count(df.raw_success, participant)
```

```{r}
# Save df
# Convert list to string for slider values
df.raw_success$slider_values <- sapply(df.raw_success$slider_values, 
                                      function(x) paste(x, collapse = ","))

output_file_path <- file.path(organized_data_path, "organized_data.csv")
write.csv(df.raw_success, output_file_path, row.names = FALSE)

```

```{r}
# save another df for failed participants

# Define the participant IDs you want to filter
participant_fail <- c("p_12", "p_16", "p_2", "p_20", "p_23", "p_26", "p_27", "p_6")

# Filter df.raw to include only the specified participant IDs
df.raw_fail <- df.raw %>%
  filter(participant %in% participant_fail)

# check
count(df.raw_fail, participant)

# Save df
# Convert list to string for slider values
df.raw_fail$slider_values <- sapply(df.raw_fail$slider_values, 
                                      function(x) paste(x, collapse = ","))

output_file_path <- file.path(organized_data_path, "organized_data_fail.csv")
write.csv(df.raw_fail, output_file_path, row.names = FALSE)

```

# Analysis

```{r}
# upload data after semantic similarity calculations
df.data <- read.csv(file.path(organized_data_path, "df_semantic.csv"))
```

```{r}
# Calculate median trial movements for each trial

df.data <- df.data %>%
  mutate(
    slider_values_numeric = str_split(slider_values_rescaled, ",") %>%
      map(~ as.numeric(.x))
  )

# Mean and median slider values for each trial
df.data <- df.data %>%
  rowwise() %>%
  mutate(trial_mean_rescaled = mean(unlist(slider_values_numeric), na.rm = TRUE),
         trial_median_rescaled = median(unlist(slider_values_numeric), na.rm = TRUE)) %>%
  ungroup()
```


## Semantic Similarity
Which semantic similarity analysis explains comprehension better?

### Visualize summary scores per speech rate

```{r figure-size, fig.width=9, fig.height=5}
# GloVe - summaries
average_similarity_summary <- df.data %>%
  group_by(speech_rate) %>%
  summarise(
    average_similarity = mean(glove_sum_max_similarity_rescaled, na.rm = TRUE),
    sd = sd(glove_sum_max_similarity_rescaled, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n)  # standard error
  )


# GloVe - over text
average_similarity_text <- df.data %>%
  group_by(speech_rate) %>%
  summarise(
    average_similarity = mean(glove_text_average_max_similarity, na.rm = TRUE),
    sd = sd(glove_text_average_max_similarity, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n)  # standard error
  )

# BERT with whole summary-whole text
average_similarity_bert <- df.data %>%
  group_by(speech_rate) %>%
  summarise(
    average_similarity = mean(bert_cosine_similarity, na.rm = TRUE),
    sd = sd(bert_cosine_similarity, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n)  # standard error
  )


# Create the plots without x and y labels
plot_glove_summary <- ggplot(average_similarity_summary, aes(x = factor(speech_rate), 
                               y = average_similarity, 
                               fill = as.factor(speech_rate))) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7, fill = "darkseagreen1") +
  scale_x_discrete(labels = function(x) paste0("x ", x)) +
  labs(x = "Speech Speed", y = "Average Similarity", 
       title = "GloVe- Heard Segment") +
  geom_text(aes(label = round(average_similarity, 2)), 
            vjust = -1.7, position = position_dodge(width = 0.7), size = 3) +
  geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se), 
                width = 0.25, position = position_dodge(width = 0.7)) + 
  theme(axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14),
        plot.title = element_text(size = 13, hjust = 0.5))

plot_glove_text <- ggplot(average_similarity_text, aes(x = factor(speech_rate), 
                               y = average_similarity, 
                               fill = as.factor(speech_rate))) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7, fill = "darkseagreen2") +
  labs(x = NULL, y = NULL, 
       title = "GloVe- Written Summary") +
  geom_text(aes(label = round(average_similarity, 2)), 
            vjust = -1.2, position = position_dodge(width = 0.7), size = 3) +
  geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se), 
                width = 0.25, position = position_dodge(width = 0.7)) +
  theme(plot.title = element_text(size = 13, hjust = 0.5))

plot_bert <- ggplot(average_similarity_bert, aes(x = factor(speech_rate), 
                               y = average_similarity, 
                               fill = as.factor(speech_rate))) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7, fill = "darkseagreen3") +
  labs(x = NULL, y = NULL, 
       title = "BERT") +
  geom_text(aes(label = round(average_similarity, 2)), 
            vjust = -1.5, position = position_dodge(width = 0.7), size = 3) +
  geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se), 
                width = 0.25, position = position_dodge(width = 0.7)) +
  theme(plot.title = element_text(size = 13, hjust = 0.5))


# put them in a grid
grid.arrange(
  arrangeGrob(
   plot_glove_summary, plot_glove_text, plot_bert,
    ncol = 3,
    left = textGrob(" Average Normalized Cosine Similarity Value", 
                    rot = 90, vjust = 0.5),
   bottom = textGrob(" Speech Rate", 
                    rot = 0, vjust = 0.5)
  ) 
)
```
glove- written summary: Average of Sum of Maximum Cosine Values For Segment Text Words By Speech Rate
glove- heard segment: Average of Maximum Cosine Values For Segment Text Words By Speech Rate


```{r}
# same plot with 1 y axis

# Create the plots
plot_glove_summary <- ggplot(average_similarity_summary, aes(x = factor(speech_rate), 
                               y = average_similarity, 
                               fill = as.factor(speech_rate))) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7, fill = "darkseagreen1") +
  labs(x = NULL, y = NULL, 
       title = "GloVe- Heard Segment") +
  geom_text(aes(label = round(average_similarity, 2)), 
            vjust = -1.5, position = position_dodge(width = 0.7), size = 3) +
  geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se), 
                width = 0.25, position = position_dodge(width = 0.7)) + 
  theme(plot.title = element_text(size = 13, hjust = 0.5)) +
  ylim(0, 0.7)

plot_glove_text <- ggplot(average_similarity_text, aes(x = factor(speech_rate), 
                               y = average_similarity, 
                               fill = as.factor(speech_rate))) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7, fill = "darkseagreen2") +
  labs(x = NULL, y = NULL, 
       title = "GloVe- Written Summary") +
  geom_text(aes(label = round(average_similarity, 2)), 
            vjust = -1.2, position = position_dodge(width = 0.7), size = 3) +
  geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se), 
                width = 0.25, position = position_dodge(width = 0.7)) +
  theme(plot.title = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  ylim(0, 0.7)

plot_bert <- ggplot(average_similarity_bert, aes(x = factor(speech_rate), 
                               y = average_similarity, 
                               fill = as.factor(speech_rate))) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7, fill = "darkseagreen3") +
  labs(x = NULL, y = NULL, 
       title = "BERT") +
  geom_text(aes(label = round(average_similarity, 2)), 
            vjust = -1.5, position = position_dodge(width = 0.7), size = 3) +
  geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se), 
                width = 0.25, position = position_dodge(width = 0.7)) +
  theme(plot.title = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  ylim(0, 0.7)

# Combine the plots
combined_plot <- grid.arrange(
  arrangeGrob(
    plot_glove_summary, plot_glove_text, plot_bert,
    ncol = 3,
    left = textGrob("Average Normalized Cosine Similarity Value", 
                    rot = 90, vjust = 0.5),
    bottom = textGrob("Speech Rate", 
                      rot = 0, vjust = 0.5)
  ) 
)

# Display the combined plot
print(combined_plot)
```

### Correlation
Do 3 different semantic similarity measures correlate with each other?
```{r}
# Subset the relevant columns
semantic_columns <- df.data[, c("bert_cosine_similarity", "glove_text_average_max_similarity", "glove_sum_max_similarity_rescaled")]

# Calculate the correlation matrix
correlation_matrix <- cor(semantic_columns, use = "complete.obs")

# Print 
print(correlation_matrix)

```

```{r}
# test if any of these correlations are statistically significant

# Correlation test between bert_cosine_similarity and glove_text_average_max_similarity
test1 <- cor.test(df.data$bert_cosine_similarity, df.data$glove_text_average_max_similarity)

# Correlation test between bert_cosine_similarity and glove_sum_max_similarity_rescaled
test2 <- cor.test(df.data$bert_cosine_similarity, df.data$glove_sum_max_similarity_rescaled)

# Correlation test between glove_text_average_max_similarity and glove_sum_max_similarity_rescaled
test3 <- cor.test(df.data$glove_text_average_max_similarity, df.data$glove_sum_max_similarity_rescaled)

print(test1)
print(test2)
print(test3)

```
A Pearson correlation analysis was conducted to examine the relationship between three cosine similarity measures measures: BERT, GloVe- Written Summary, and GloVe- Heard Segment. The correlation between BERT and GloVe- Written Summary was positive and moderate (r= 0.54, t(1482)=24.63, p<.001.). The correlation between BERT and GloVe- Heard Segment was positive and moderate (r=0.57,t(1482)=26.71, p<.001). Finally, the strongest correlation observed between the two GloVe-based measures (r=0.78, t(1482)=48.07, p<.001)

### Model comparison
Semantic similarity as a fixed effect to predict likert scale responses
```{r}
# Glove 1- likert response
model_g1<- lmer(likert_rescaled ~ 
                glove_sum_max_similarity_rescaled + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_g1)
r2_g1 <- r.squaredGLMM(model_g1)
r2_g1
```
```{r}
# glove2 -likert response
model_g2<- lmer(likert_rescaled ~ 
                glove_text_average_max_similarity + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_g2)
r2_g2 <- r.squaredGLMM(model_g2)
r2_g2
```

```{r}
# bert- likert
model_b<- lmer(likert_rescaled ~ 
                 bert_cosine_similarity + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_b)

# calculate the R^2
r2_b <- r.squaredGLMM(model_b)
r2_b
```

Let's see if any semantic similarity scores explain  variance in comprehension better than the other
```{r}
# model all vs model simple
model_all<- lmer(likert_rescaled ~ 
                 bert_cosine_similarity +   glove_text_average_max_similarity +   
                   glove_sum_max_similarity_rescaled + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)
#model_compare <- anova(model_g1, model_all)
summary(model_all) 

```
YAY estimates are interpretable now! 

To see which if  semantic similarity scores explains variance in comprehension better than the others,we conducted a linear mixed model was conducted to predict the 10-Point Scale responses based on BERT, GloVe- Written Summary, and GloVe- Heard Segment. Random effects for digit span score and digit in noise score were included in the model.

The results indicated that BERT was a significant predictor of the 10-Point Scale responses, 
b=0.48, SE=0.04, t(1472)=11.34, p<.001. Similarly, GloVe- Heard Segment was also a significant predictor, b=0.28, SE=0.08, t(1465)=3.57, p<.001. Additionally, GloVe- Written Summary significantly predicted the 10-Point Scale responses, b=0.83, SE=0.04, t(1473)=18.55, p<.001. 

All fixed effects (different sem similarity scores) are significant, they are finding independent variance, interestingly. Glove- Written Summary has the heighest t value and estimate. Therefore, I will use/ report the GLoVe- Heard Segment for the rest of the analyses.

### Are semantic similarity scores btw speech rates sig different from each other?

#### Glove1
```{r}
semantic.anova <- aov(glove_sum_max_similarity_rescaled ~ speech_rate, data = df.data)
summary(semantic.anova)

pairwise_results <- df.data %>% pairwise_t_test(
    glove_sum_max_similarity_rescaled ~ speech_rate, pool.sd = FALSE,
    p.adjust.method = "bonferroni"
    )
pairwise_results
# all significant except 1&2
```
A one-way ANOVA showed that there was a significant effect of speech rate on semantic similarity scores. F(1,1482)= 1004, p<.001.Bonferroni corrected pair-wise comparison showed that all speech rate pairs were significantly different from each other, except for speech rate 1 and 2.

#### Glove2

```{r}
# do the not-binned anaylsis for glove2
semantic.anova2 <- aov(glove_text_average_max_similarity ~ speech_rate, data = df.data)
summary(semantic.anova2)

pairwise_results2 <- df.data %>% pairwise_t_test(
    glove_text_average_max_similarity ~ speech_rate, pool.sd = FALSE,
    p.adjust.method = "bonferroni"
    )
pairwise_results2
```

A one-way ANOVA showed that there was a significant effect of speech rate on Glove- Written Summary semantic similarity scores. F(1,1482)= 641.8, p<.001. Bonferroni corrected pair-wise comparison showed that all speech rate pairs were significantly different from each other, except for speech rate 1 and 2, and for 2 and 3.

#### BERT

```{r}
# do the not-binned anaylsis for BERT
semantic.anova3 <- aov(bert_cosine_similarity ~ speech_rate, data = df.data)
summary(semantic.anova3)

pairwise_results3 <- df.data %>% pairwise_t_test(
    bert_cosine_similarity ~ speech_rate, pool.sd = FALSE,
    p.adjust.method = "bonferroni"
    )
pairwise_results3
```
A one-way ANOVA showed that there was a significant effect of speech rate on BERT semantic similarity scores. F(1,1482)= 630.7, p<.001. Bonferroni corrected pair-wise comparison showed that all speech rate pairs were significantly different from each other, except for speech rate 2 and 3.

### Is there a recency effect?

#### Glove1
```{r}
# first, make sure that the right ones are NA, right now NAs are also 0

df.data <- df.data %>%
  mutate(
    rescaled_glove_bin1 = ifelse(textbox.text_corrected_manual == "", NA, rescaled_glove_bin1),
    rescaled_glove_bin2 = ifelse(textbox.text_corrected_manual == "", NA, rescaled_glove_bin2),
    rescaled_glove_bin3 = ifelse(textbox.text_corrected_manual == "", NA, rescaled_glove_bin3),
    rescaled_glove_bin4 = ifelse(textbox.text_corrected_manual == "", NA, rescaled_glove_bin4),
    rescaled_glove_bin5 = ifelse(textbox.text_corrected_manual == "", NA, rescaled_glove_bin5)
  )
```


```{r}
# create a long dataset for bins and scores
df.bin <- df.data %>%
  select(participant, index, speech_rate, segment_text, textbox.text_corrected_manual, 
         glove_bin1, glove_bin2, glove_bin3, glove_bin4, glove_bin5) %>%
  pivot_longer(cols = starts_with("glove_bin"), 
               names_to = "bin", 
               names_prefix = "glove_bin", 
               values_to = "semantic_score") %>%
  mutate(bin = as.integer(bin)) %>%
  arrange(participant, index, bin)

# Print the new long format data frame for verification
print(df.bin)

```

```{r}
# If so, the latter bins should be recalled better than the earlier ones
binned.anova <- aov(semantic_score ~ speech_rate * bin, data = df.bin)
summary(binned.anova)

df.bin_factor = df.bin %>%
  mutate(speech_bin = factor(paste(speech_rate, bin, sep="_")))

# pairwise comparison
df.bin_factor %>% pairwise_t_test(
   semantic_score ~ speech_bin, pool.sd = FALSE,
    p.adjust.method = "bonferroni"
    )

```
An ANOVA to examining the effects of speech rate and bin, and their interaction on GLoVe- Written Summary semantic similarity scores showed a significant main effect of speech rate (F(1,13746)= 2573.8, p<.001), but not of bin or the interaction, indicating that there is no significant recency effect.


#### Glove2
```{r}
# GLOVE2
# first, make sure that the right ones are NA, right now NAs are also 0

df.data <- df.data %>%
  mutate(
    glove2_bin1 = ifelse(textbox.text_corrected_manual == "", NA, glove2_bin1),
    glove2_bin2 = ifelse(textbox.text_corrected_manual == "", NA, glove2_bin2),
    glove2_bin3 = ifelse(textbox.text_corrected_manual == "", NA, glove2_bin3),
    glove2_bin4 = ifelse(textbox.text_corrected_manual == "", NA, glove2_bin4),
    glove2_bin5 = ifelse(textbox.text_corrected_manual == "", NA, glove2_bin5)
  )
```


```{r}
# create a long dataset for bins and scores for BERT
df.bin_glove2 <- df.data %>%
  select(participant, index, speech_rate, segment_text, textbox.text_corrected_manual, 
         glove2_bin1, glove2_bin2, glove2_bin3, glove2_bin4, glove2_bin5) %>%
  pivot_longer(cols = starts_with("glove2_bin"), 
               names_to = "bin", 
               names_prefix = "glove2_bin", 
               values_to = "glove2_score") %>%
  mutate(bin = as.integer(bin)) %>%
  arrange(participant, index, bin)

# Print the new long format data frame for verification
print(df.bin_glove2)

```


```{r}
# If there is ar recency effect, the latter bins should be recalled better than the earlier ones
binned.anova2 <- aov(glove2_score ~ speech_rate * bin, data = df.bin_glove2)
summary(binned.anova2)

df.bin_factor = df.bin_glove2 %>%
  mutate(speech_bin = factor(paste(speech_rate, bin, sep="_")))

# pairwise comparison
df.bin_factor %>% pairwise_t_test(
   glove2_score ~ speech_bin, pool.sd = FALSE,
    p.adjust.method = "bonferroni"
    )
```
An ANOVA was conducted to examine the effects of speech rate and bin, and their interaction on GloVe- Heard segment semantic similarity scores. The results indicated a significant main effect of speech rate, F(1,7416)=2220.34, p<.001, suggesting that speech rate significantly influences the dependent variable. The main effect of bin was also significant, F(1,7416)=6.81, p<.01, indicating that there was a recency effect. Participants' summaries reflected the last parts of what they have heard more that the earlier parts of it.
The interaction effect between speech rate and bin was not significant. 

ps. There is a main effect of bin but non of the within sr bin pairs are significant.

#### BERT

```{r}
# create a long dataset for bins and scores for BERT
df.bin_bert <- df.data %>%
  select(participant, index, speech_rate, segment_text, textbox.text_corrected_manual, 
         cosine_similarity_bin1, cosine_similarity_bin2, cosine_similarity_bin3, cosine_similarity_bin4, cosine_similarity_bin5) %>%
  pivot_longer(cols = starts_with("cosine_similarity_bin"), 
               names_to = "bin", 
               names_prefix = "cosine_similarity_bin", 
               values_to = "bert_score") %>%
  mutate(bin = as.integer(bin)) %>%
  arrange(participant, index, bin)

# Print the new long format data frame for verification
print(df.bin_bert)
```
```{r}
binned.anova_bert <- aov(bert_score ~ speech_rate * bin, data = df.bin_bert)
summary(binned.anova_bert)

df.bin_bert_factor = df.bin_bert %>%
  mutate(speech_bin = factor(paste(speech_rate, bin, sep="_")))

# pairwise comparison
df.bin_bert_factor %>% pairwise_t_test(
   bert_score ~ speech_bin, pool.sd = FALSE,
    p.adjust.method = "bonferroni"
    )
```
An ANOVA was conducted to examine the effects of speech rate and bin, and their interaction on BERT Summary semantic similarity scores. The results indicated a significant main effect of speech rate, F(1,7416)=595.15, p<.001, suggesting that speech rate significantly influences the dependent variable. The main effect of bin was also significant, F(1,7416)=61.82, p<.001, indicating that there was a recency effect. Participants' summaries reflected the last parts of what they have heard more that the earlier parts of it.
The interaction effect between speech rate and bin was not significant (p=.053).

Ps. significant pair-wise comparisons within speech rates: 
- speech rate 1: bin 1 & 4
- speech rate 2: bin 1 & 5, 2 & 5, 3 & 5
- speech rate 3: bin 1 & 5, 2 & 5 
- speech rate 4: bin 1 & 4, 1 & 5

-----
### Predict similarity score as a function of other comprehension measures 
To see if the wm effect on semantic similarity scores will be significant or if it will be too small to be a predictor despite the high correlation btw comprehension measures and wm
-> if it does the next question would be would the recency effect is stronger for the people with worse wm 

#### Glove1
```{r}
#with glove 1
model_all_semantic <- lmer(glove_sum_max_similarity_rescaled ~
                trial_median_rescaled +
                likert_rescaled + 
                multiple_choice_accuracy + 
                  digit_span_score + 
                (1 | participant) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_all_semantic)
```

To see if the working memory effect on semantic similarity scores will be significant or if it will be too small to be a predictor (despite the high correlation btw comprehension measures and wm- whcih we are not reporting?), we conducted a linear mixed model predicting GloVe- Heard Segment semantic similarity scores from all comprehension measures and working memory (digit-span scores). We included random effects for participants and digit in noise score. 

The results indicated that real time comprehension scores were a significant predictor of GloVe- Heard Segment, b=0.04, SE=0.02, t(1469)=2.03, p<.05. The 10-Point Scale responses were also a significant predictor, b=0.43, SE=0.02, t(1471)=17.72, p<.001. However, multiple choice accuracy did not significantly predict GloVe- Heard Segment, nor did digit span score (working memory).

-> no sig effect of wm 

The random effects for participants had a variance of 0.009 (SD = 0.093), and the random effects for digit in noise score had a variance of 0.007 (SD = 0.084). The residual variance was 0.017 (SD = 0.129).

#### Glove2
```{r}
# with glove2
model_all_semantic_g2 <- lmer(glove_text_average_max_similarity ~
                trial_median_rescaled +
                likert_rescaled + 
                multiple_choice_accuracy + 
                  digit_span_score + 
                (1 | participant) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_all_semantic_g2)
```

The results indicated that 10-point scale responses were a significant predictor of GloVe- Written Summary semantic similiarity scores, b=0.21, SE=0.02, t(1477)=12.92, p<.001. However, real-time comprehension scores and multiple choice accuracy did not significantly predict semantic similarity scores, nor did digit span score (working memory).

#### BERT
```{r}
# with bert
model_all_semantic_b <- lmer(bert_cosine_similarity ~
                trial_median_rescaled +
                likert_rescaled + 
                multiple_choice_accuracy + 
                  digit_span_score + 
                (1 | participant) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_all_semantic_b)

```
significant effect of wm!

The 10-Point Scale responses were a significant predictor, b=0.29, SE=0.02, t(1398)=11.78, p<.001. Multiple choice accuracy was also a significant predictor, b=0.04,SE=0.01, t(1470)=4.65, p<.001. Digit span score significantly predicted BERT cosine similarity, b=0.005, SE=0.002, t(20.15)=2.53, p<.05.

## Individual fixed effects

#### Post-hoc Comprehension Ratings
```{r}
# post-hoc comprehension ratings
model_likert<- lmer(trial_median_rescaled ~ 
                likert_rescaled + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_likert)
```

A linear mixed model was fitted to predict the real-time comprehension scores from 10-point scale ratings, while accounting for random effects of working memory and hearing-in-noise scores. 

The model revealed a significant positive effect of 10-point scale ratings on real-time comprehension scores (β = 1.07, SE = 0.01, t(2736) = 109.88, p < .001). 

The intercept was also significant (β = -7.89, SE = 0.02, t(14.29) = -7.89, p < .001).

The random effects show minimal variability attributable to working memory (variance = 0.00, SD = 0.05) and hearing in noise (variance = 0.00, SD = 0.01), with the residual variance being 0.03 (SD = 0.18).

```{r}
# Calculate the average likert scale scores per speech_rate category

average_score_likert <- df.data %>%
  group_by(speech_rate) %>%
  summarise(average_likert = mean(likert_rescaled, na.rm = TRUE),
            sd = sd(likert_rescaled, na.rm = TRUE),
            n = n(),
            se = sd / sqrt(n)  # standard error
  )

# Plot
likert_plot <- ggplot(average_score_likert, aes(x = factor(speech_rate), y = average_likert)) +
  geom_bar(stat = "identity", width = 0.7, fill = "lightblue") +   
  labs(x = "Speech Speed", y = "Average 10-Point Scale Response") +
  geom_text(aes(label = round(average_likert, 2)), vjust = -0.7, size = 3.5) +
  scale_x_discrete(labels = function(x) paste0("x ", x)) +
  geom_errorbar(aes(ymin = average_likert - se, ymax = average_likert + se), 
                width = 0.25, position = position_dodge(width = 0.5)) +
  theme(axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14))

# Display the plot
print(likert_plot)
#change x axis labels to speech speed x
```


#### Multiple Choice Quesitons
```{r}
# multiple choice questions
model_multiple_choice<- lmer(trial_median_rescaled ~ 
                multiple_choice_accuracy + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_multiple_choice)
```
A linear mixed model was fitted to examine the effect of multiple choice accuracy on real-time comprehension, while accounting for random effects of working memory and hearing in noise. 

The model revealed a significant positive effect of multiple choice accuracy on real-time comprehension (β = 0.32, SE = 0.02, t(2741) = 19.94, p < .001).  The intercept was also significant (β = 0.27, SE = 0.04, t(2.30) = 7.24, p = .001).

The random effects show minimal variability attributable to working memory (variance = 0.00, SD = 0.07) and hearing in noise (variance = 0.00, SD = 0.06), with the residual variance being 0.15 (SD = 0.39).


```{r figure-size, fig.width=4, fig.height=4 }
# Calculate the average multiple_choice_accuracy scores per speech_rate category
average_scores <- df.data %>%
  group_by(speech_rate) %>%
  summarise(average_accuracy = mean(multiple_choice_accuracy, na.rm = TRUE),
            sd = sd(multiple_choice_accuracy, na.rm = TRUE),
            n = n(),
            se = sd / sqrt(n)  # standard error
  )

# Plot
question_plot <- ggplot(average_scores, aes(x = factor(speech_rate), y = average_accuracy)) +
  geom_bar(stat = "identity", width = 0.7, fill = "lightpink") +   
  labs(x = "Speech Speed", y = "Average Multiple Choice Accuracy") +
  geom_text(aes(label = round(average_accuracy, 2)), vjust = -1, size = 3.5) +
  scale_x_discrete(labels = function(x) paste0("x ", x)) +
  geom_errorbar(aes(ymin = average_accuracy - se, ymax = average_accuracy + se), 
                width = 0.25, position = position_dodge(width = 0.5)) +
  theme(axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14))

# Display the plot
print(question_plot)
#change x axis labels to speech speed x
```
Accuracy seems to lower as speech rate increases. Yet, even for the 5 times sped up condition, participants' performance is well above the chance level of 0.25.
One possibility is that as the segments are taken from the same audiobook, despite the randomization, participants might be gaining a sense of the story and be able to guess the correct answer regardless of their ability to understand the segments. If this is the case, their average accuracy should be better in latter trials compared to earlier ones.

```{r}
# Select trials with speech rate = 5
df_data_speech_rate_5 <- df.data %>%
  filter(speech_rate == 5)

df_data_speech_rate_5 <- df_data_speech_rate_5 %>%
  group_by(participant) %>%
  mutate(index_5 = row_number()) %>%
  ungroup()

# Display the updated data frame
print(df_data_speech_rate_5)

# Categorize trials into first , mid , last 
df_data_speech_rate_5 <- df_data_speech_rate_5 %>%
  mutate(trial_group = case_when(
    index_5 <= 8 ~ "First 8", 
    index_5 > 8 & index_5 <= 17 ~ "Mid 9",
    index_5 > 17 ~ "Last 8"
  ))

# Calculate the average multiple_choice_accuracy for each trial group
average_accuracy <- df_data_speech_rate_5 %>%
  group_by(trial_group) %>%
  summarise(average_accuracy = mean(multiple_choice_accuracy, na.rm = TRUE), 
            .groups = "drop") %>%
  mutate(trial_group = factor(trial_group, 
                              levels = c("First 8", "Mid 9", "Last 8"))) # order

# Plot
ggplot(average_accuracy, aes(x = trial_group, y = average_accuracy, fill = trial_group)) +
  geom_bar(stat = "identity") +
  labs(x = "Trial Group", y = "Average Accuracy", 
       title = "Average Accuracy per Trial Group (Speech Rate = 5)") +
  scale_fill_brewer(palette = "Pastel1") +
  geom_text(aes(label = round(average_accuracy, 3)), vjust = -0.5, size = 3) +
  theme(legend.position = "none")
print(average_accuracy)
```
There doesn't seem to be a clear pattern when you divide into 3 but there are only 25 trials for sr5 per participant

```{r}
# divide into 2
# Select trials with speech rate = 5
df_data_speech_rate_5 <- df.data %>%
  filter(speech_rate == 5)

df_data_speech_rate_5 <- df_data_speech_rate_5 %>%
  group_by(participant) %>%
  mutate(index_5 = row_number()) %>%
  ungroup()

# Display the updated data frame
print(df_data_speech_rate_5)

# Categorize trials into first , last 
df_data_speech_rate_5 <- df_data_speech_rate_5 %>%
  mutate(trial_group = case_when(
    index_5 <= 12 ~ "First 13", 
    index_5 > 12 ~ "Last 12"
  ))

# Calculate the average multiple_choice_accuracy for each trial group
average_accuracy <- df_data_speech_rate_5 %>%
  group_by(trial_group) %>%
  summarise(average_accuracy = mean(multiple_choice_accuracy, na.rm = TRUE), 
            .groups = "drop") %>%
  mutate(trial_group = factor(trial_group, 
                              levels = c("First 13", "Last 12"))) # order

# Plot
ggplot(average_accuracy, aes(x = trial_group, y = average_accuracy, fill = trial_group)) +
  geom_bar(stat = "identity") +
  labs(x = "Trial Group", y = "Average Accuracy", 
       title = "Average Accuracy per Trial Group (Speech Rate = 5)") +
  scale_fill_brewer(palette = "Pastel1") +
  geom_text(aes(label = round(average_accuracy, 3)), vjust = -0.5, size = 3) +
  theme(legend.position = "none")
print(average_accuracy)
```
huh there seems to be only a very small difference. but just in case:

Question accuracy seems to get higher as participants are exposed to more trials, so there might be an effect of context familiarity. Although, even for the first batch, the accuracy is way above the chance level of 0.25. 

Participant performance on multiple choice questions may be influenced by the trial number they are at. If the effect of trial number on multiple choice accuracy varies over time, incorporating this interaction into the model can enhance its predictive accuracy.

```{r}
# accounting for trial vs not- modal comparison
model_trial <- lmer(trial_median_rescaled ~  
                glove_sum_max_similarity_rescaled +
                likert_rescaled + 
                multiple_choice_accuracy * index +  # Interaction between trial number and multiple_choice_accuracy. 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)
summary(model_trial)
model_all_glove1 <- lmer(trial_median_rescaled ~  
                glove_sum_max_similarity_rescaled +
                likert_rescaled + 
                multiple_choice_accuracy +  # Interaction between trial number and multiple_choice_accuracy. 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)
# Compare the two models
comparison <- anova(model_all_glove1, model_trial)

print(comparison)
```
Model comparison is not significant, but close.

#### Summary

```{r}
# Summary- Glove (sum of maximum cosine values of summary words)
model_glove1<- lmer(trial_median_rescaled ~ 
                glove_sum_max_similarity_rescaled + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_glove1)
```
A linear mixed model was fitted to predict real-time comprehension scores from summary semantic similarity scores, while accounting for random effects of working memory and hearing in noise.

The model revealed a significant positive effect of summary scores on real-time comprehension (β = 1.36, SE = 0.04, t(1479) = 36.64, p < .001). The intercept was not significant.

The random effects show some variability attributable to working memory (variance = 0.02, SD = 0.13) and hearing in noise (variance = 0.01, SD = 0.09), with the residual variance being 0.09 (SD = 0.29).
```{r}
# visualize glove1 per speech rate
plot_glove_summary <- plot_glove_summary + 
  labs(title= NULL, x = "Speech Speed", y = "Average Normalized Semantic Similarity")  +
  theme(axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14))
plot_glove_summary
```

### Visualize single fixed effect regressions
```{r}
library(visreg)

# visualize the regressions

visreg_summary <- visreg(model_glove1, "glove_sum_max_similarity_rescaled", gg=TRUE,    
                         line=list(col="green3"), points=list(alpha=0.5),
                         xlab= "Normalized Semantic Similarity Score", 
                         ylab= "Predicted Real-Time Comprehension Score") +
  theme(axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14))


visreg_likert <- visreg(model_likert, "likert_rescaled", gg=TRUE, points=list(alpha=0.5),
                        xlab= "10-point Scale Response", 
                        ylab= "Predicted Real-Time Comprehension Score") +
  theme(axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14))


visreg_multiple <- visreg(model_multiple_choice, "multiple_choice_accuracy", gg=TRUE,
                          line=list(col="pink3"), points=list(alpha=0.5),
                          xlab= "Multiple Choice Accuracy", 
                          ylab= "Predicted Real-Time Comprehension Score") +
  theme(axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14))

visreg_summary
visreg_likert
visreg_multiple
```

```{r figure-size, fig.width=10, fig.height=12 }
# create a multi panel plot for a. bar and regressions for single fixed effects

# Create title grobs
title1 <- textGrob("Summary", gp=gpar(fontsize=16, fontface="bold"))
title2 <- textGrob("10-Point Scale", gp=gpar(fontsize=16, fontface="bold"))
title3 <- textGrob("Multiple Choice Question", gp=gpar(fontsize=16, fontface="bold"))

# Combine the plots in a 2x3 panel with increased row gap and column titles
combined_plot_effects <- grid.arrange(
  title1, title2, title3,
  visreg_summary, visreg_likert, visreg_multiple,
  plot_glove_summary, likert_plot, question_plot,
  ncol = 3,
  nrow = 4,
  heights = unit.c(unit(4, "null"), unit(8, "null"), unit(1.5, "null"), unit(8, "null")),  # Adjust heights
  layout_matrix = rbind(c(1, 2, 3), c(4, 5, 6), c(NA, NA, NA), c(7, 8, 9))
)

# Display the combined plot
print(combined_plot_effects)

ggsave(file="test.svg", plot=combined_plot_effects)

# bigger labels, further away, add A and B to panels
```

## All post-hoc measures as fixed effects

A Mixed Effects Linear Regression Analysis is conducted to model the median real-time comprehension scores. Comprehension scores of the post-hoc tests, which are the multiple-choice questions, 10-scale comprehension ratings, and summaries, are utilized as factors in the regression. Digit Span and Digit-In-Noise scores are assigned as random effects, accounting for individual subject variations. This approach allows us to explore to what degree the scores of our novel real-time comprehension measure can be explained by comprehension, above and beyond the contribution of individual differences in working memory and speech perception in noise capacities.

We conducted the same regression analysis with different semantic similarity calculations to see whether calculating semantic similarities with different embedding models creates a difference in how semantic similarity can predict the novel measure.

```{r}
model_all_glove1 <- lmer(trial_median_rescaled ~  
                glove_sum_max_similarity_rescaled +
                likert_rescaled + 
                multiple_choice_accuracy + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_all_glove1)
```

-Giris metni diger rapordan alinabilir! Kalan da snl abstractten alinabilir-

The model revealed a significant positive effect of summary scores on real-time comprehension (β = 0.14, SE = 0.03, t(295.32) = 4.47, p <.001). There was also a significant positive effect of 10-Point Scale responses on real-time comprehension (β = 1.03, SE = 0.02, t(403.09) = 50.14, p<.001). The effect of multiple choice accuracy on real-time comprehension was not significant. 
-> (β = 0.02, SE = 0.01, t(1468.56) = 1.49, p = .14). The intercept was significant (β = -0.16, SE = 0.02, t(18.32) = -7.93, p < .001).

The random effects show minimal variability attributable to working memory (variance = 0.00, SD = 0.07) and hearing in noise (variance = 0.00, SD = 0.01), with the residual variance being 0.03 (SD = 0.18).

### Visualize the multiple fixed effects regression
```{r}
# plot the regression (each predictor)
# Each call to visreg will create a plot showing the relationship between the specified predictor and the response variable, holding the other predictors constant at their mean values. So you are then visualizing the partial effects of the predictor of choice.
# y axis = fitted (predicted) values of real-time comprehension given X measure values 

visreg_summary <- visreg(model_all_glove1, "glove_sum_max_similarity_rescaled", gg=TRUE,
                          line=list(col="green3"), points=list(alpha=0.5),
                       xlab= "Normalized Semantic Similarity Score", 
                       ylab= NULL) 

visreg_likert <- visreg(model_all_glove1, "likert_rescaled", gg=TRUE,
                       xlab= "10-point Scale Response", points=list(alpha=0.5),
                       ylab= NULL)  

visreg_multiple <- visreg(model_all_glove1, "multiple_choice_accuracy", gg=TRUE,
                           line=list(col="pink3"), points=list(alpha=0.5),
                       xlab= "Multiple Choice Accuracy", 
                       ylab= NULL) 


# Arrange plots in a  grid
grid.arrange(
  arrangeGrob(
    visreg_summary, visreg_likert, visreg_multiple, 
    ncol = 3,
    left = textGrob("Predicted speech rate given the comprehension scores", rot = 90, vjust = 1)
  ) 
)

# For random slopes 
visreg_wm <- visreg(model_all_glove1, "digit_span_score", gg=TRUE,
                       xlab= "Working Memory Scores", 
                       ylab= NULL) 
  #scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))

visreg_hearing <- visreg(model_all_glove1, "digit_in_noise_score", gg=TRUE,
                       xlab= "Hearing In Noise", 
                       ylab= NULL)
  #scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))
  

# put them in a grid
grid.arrange(
  arrangeGrob(
   visreg_wm, visreg_hearing,
    ncol = 2,
    left = textGrob(" Predicted real-time comprehension rate given the post-hoc measures", 
                    rot = 90, vjust = 1)
  ) 
)
```

### Visualize the estimates
```{r}
# Get the fixed effects and confidence intervals
fixed_effects <- tidy(model_all_glove1, effects = "fixed", conf.int = TRUE)
# Plot the fixed effects
ggplot(fixed_effects, aes(x = term, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  theme_minimal() +
  labs(title = "Fixed Effects with Confidence Intervals",
       x = "Predictor",
       y = "Estimate")

# Extract random effects
random_effects <- ranef(model_all_glove1, condVar = TRUE)
# Convert the random effects to a data frame for plotting
random_effects_df <- do.call(rbind, lapply(random_effects, function(x) {
  x$grp <- rownames(x)
  x
}))

# Plot using sjPlot
# fixed effects (type = "est")
plot_model(model_all_glove1, type = "est", show.values = TRUE, show.p = TRUE)

```

## Predicting real time comprehension from speech rate 
```{r}
model_realtime_rate <- lmer( trial_median_rescaled~ 
                speech_rate +
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_realtime_rate)
```
We conducted a linear mixed model to predict real time comprehension scores based on speech rate (whether comprehension changes with speech rate), including random effects for digit span score and digit in noise score.Speech rate was a significant predictor of real time comprehension scores, b=−0.26, SE=0.003, t(2733)=−102.27, p<.001.
The random effects for digit span score had a variance of 0.01 (SD = 0.09), and the random effects for digit in noise score had a variance of 0.00 (SD = 0.04). The residual variance was 0.04 (SD = 0.19).

## Predicting speech rate from comprehension measures 
```{r}
model_rate <- lmer(speech_rate ~ 
                trial_median_rescaled +
                glove_sum_max_similarity_rescaled +
                likert_rescaled + 
                multiple_choice_accuracy + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_rate)
```

We investigated whether speech rate can be predicted by comprehension by conducting a Mixed Effects Linear Regression with all comprehension measures as fixed effects. 

The model revealed a significant negative effect of real-time comprehension on speech rate (β = -1.69, SE = 0.08, t(1476.16) = −20.01, p < .001). There was also a significant negative effect of semantic similarity score of summaries on speech rate (β = −0.78, SE = 0.11, t(1205.66)) = -7.23, p < .001), and a significant negative effect of 10-Point Scale responses significantly predicted speech rate as well, b=−1.30, SE=0.11, t(1413.94)=−11.68, p<.001. Multiple choice accuracy was also a significant predictor, b=−0.08, SE=0.04, t(1466.02)=−2.30, p<.05.

The random effects show some variability attributable to working memory (variance = 0.11, SD = 0.33) and hearing in noise (variance = 0.01, SD = 0.07), with the residual variance being variance = 0.33 (SD = 0.58).

## Visualize the regression predicting speech rateCHANGE THE COLOR CODES: summary-green, likert-blue, multiple- pink, realtime- purple
```{r, warning=FALSE}
# plot the multiple fixed effetcregression (each predictor)
# Each call to visreg will create a plot showing the relationship between the specified predictor and the response variable, holding the other predictors constant at their mean values. So you are then visualizing the partial effects of the predictor of choice.
# y axis = fitted (predicted) values of speech_rate given X measure values 

visreg_slider<- visreg(model_rate, "trial_median_rescaled", gg=TRUE,
                        line=list(col="purple3"), points=list(alpha=0.5),
                       xlab= "Real-Time Comprehension Score", 
                       ylab= NULL) + 
  scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))

visreg_summary <- visreg(model_rate, "glove_sum_max_similarity_rescaled", gg=TRUE,
                       line=list(col="green3"), points=list(alpha=0.5),
                       xlab= "Normalized Semantic Similarity Score", 
                       ylab= NULL) + 
  scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))

visreg_likert <- visreg(model_rate, "likert_rescaled", gg=TRUE,
                       xlab= "10-Point Scale Response", points=list(alpha=0.5),
                       ylab= NULL) + 
  scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))

visreg_multiple <- visreg(model_rate, "multiple_choice_accuracy", gg=TRUE,
                       line=list(col="pink3"), points=list(alpha=0.5),
                       xlab= "Multiple Choice Accuracy", 
                       ylab= NULL) + 
  scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))


# Arrange plots in a  grid
grid.arrange(
  arrangeGrob(
   visreg_slider, visreg_summary, visreg_likert, visreg_multiple, 
    ncol = 2,
    left = textGrob("     Predicted speech rate given the comprehension scores", rot = 90, vjust = 1)
  ) 
)

# For random slopes 
visreg_wm <- visreg(model_rate, "digit_span_score", gg=TRUE,
                       xlab= "Working Memory Scores", 
                       ylab= NULL) + 
  scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))

visreg_hearing <- visreg(model_rate, "digit_in_noise_score", gg=TRUE,
                       xlab= "Hearing In Noise", 
                       ylab= NULL) + 
  scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))
  

# put them in a grid
grid.arrange(
  arrangeGrob(
   visreg_wm, visreg_hearing,
    ncol = 2,
    left = textGrob("     Predicted speech rate given the control measures", rot = 90, vjust = 1)
  ) 
)
```

```{r}
# Predicted vs observed values of speech rate 

df.data_filtered <- df.data[!is.na(df.data$glove_sum_max_similarity_rescaled), ]

# Extract predicted values
predicted_values <- predict(model_rate)

# Get the observed values from the original data frame
observed_values <- df.data_filtered$speech_rate

# Create a new data frame with the predicted and observed values
data_plot <- data.frame(
  predicted = predicted_values,
  observed = observed_values
)
# Create the scatterplot with regression line
ggplot(data_plot, aes(x = observed, y = predicted)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  theme_minimal() +
  labs( x = "Speech Rate",
       y = "Predicted Speech Rate")
```
CHANGE LABELS & colors:
```{r}
# visualize the estimates of fixed effects (on speech_rate)

# Get the fixed effects and confidence intervals
fixed_effects <- tidy(model_rate, effects = "fixed", conf.int = TRUE)
# Plot the fixed effects
ggplot(fixed_effects, aes(x = term, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  theme_minimal() +
  labs(title = "Fixed Effects with Confidence Intervals",
       x = "Predictor",
       y = "Estimate")

# Extract random effects
random_effects <- ranef(model_rate, condVar = TRUE)
# Convert the random effects to a data frame for plotting
random_effects_df <- do.call(rbind, lapply(random_effects, function(x) {
  x$grp <- rownames(x)
  x
}))

# Plot the model with customized colors and labels
plot_model(model_rate, 
           type = "est", 
           show.values = TRUE, 
           show.p = TRUE,
           colors = "black",          # Change the color of estimates to black
           axis.labels = c("Multiple Choice Question Accuracy", "10-Point Scale", "Summary Semantic Similarity","Median Real-Time Comprehension"),
           title = "Estimates of Comprehension Measures Predicting Speech Rate" 
)

ggsave(file="estimate_sr.svg")

```
HERE!
Discussion: novel measure is sig correlated with post-hoc measures -> it correlates with the static way of measuring comprehension -> validated as a good measure of comprehension 
ASLINDA CORELASYONU SADECE MAIN VARIABLELAR ARASINDA YAPIP BU CUMLEYI TUTABILIRIZ
Point out the limitations of post-hoc:
- written summary- recency bias in some ways of assessing 
- multiple choice- worse than introspection, above chance level accuracy for x5 (even for trials they self-report that they understand nothing they perform better than chance- inferring the context - naturalistic stimuli)
Note that people use the slider differently-> it is indeed timer resolved not like a likert scale with the same score across time
Future steps
The slider can be used for any type of internal experience over time that does not have an overt expressive behavior (attention, affect etc.)

# Linear vs Categorical drop in comprehension with increasinf speech rate

```{r}
# linear vs poly regressions

# Calculate mean values
mean_values <- df.data %>%
  group_by(speech_rate) %>%
  summarise(mean_trial_median_rescaled = mean(trial_median_rescaled, na.rm = TRUE))
df.data <- df.data %>% #add the means
  left_join(mean_values, by = "speech_rate")

# LINEAR
# fit a linear regression
model_linear <- lmer(trial_median_rescaled ~ speech_rate + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)
summary(model_linear)
r2_linear<-r.squaredGLMM(model_linear)


# plot w linear regression line wo random slopes
df.data$predicted1 <- predict(model_linear, newdata = df.data, re.form = NA)
plot_linear <-ggplot(df.data, aes(x = speech_rate, y = trial_median_rescaled)) +
  geom_point() +                                 # Adds points
  geom_line(aes(y = predicted1), color = "blue") + 
  geom_point(data = mean_values, aes(y = mean_trial_median_rescaled), color = "blue3", size = 3, shape = 21, fill = "lightblue") +  # Mean points

      labs(x = "Speech Rate",
       y = "Trial Median Rescaled",
       title = "Scatter Plot with Linear Fit") 

# 2 FACTOR
# fit a polynomial regression
model_poly <- lmer(trial_median_rescaled ~ poly(speech_rate, 2) + 
                                   (1 | digit_span_score) + 
                                   (1 | digit_in_noise_score), data = df.data)
summary(model_poly)
r2_poly2<-r.squaredGLMM(model_poly)


# Predict values from the model for visualization
df.data$predicted2 <- predict(model_poly, newdata = df.data, re.form = NA)
plot_poly2 <-ggplot(df.data, aes(x = speech_rate, y = trial_median_rescaled)) +
  geom_point() +                                 # Adds the scatter plot
  geom_line(aes(y = predicted2), color = "blue") + # Adds the polynomial fit line
  geom_point(data = mean_values, aes(y = mean_trial_median_rescaled), color = "blue3", size = 3, shape = 21, fill = "lightblue") +  # Mean points
  labs(x = "Speech Rate",
       y = "Trial Median Rescaled",
       title = "Scatter Plot with Polynomial Fit- 2 factor")

# 3 FACTOR
# fit a polynomial regression
model_poly3 <- lmer(trial_median_rescaled ~ poly(speech_rate, 3) + 
                                   (1 | digit_span_score) + 
                                   (1 | digit_in_noise_score), data = df.data)
summary(model_poly3)
r2_poly3<-r.squaredGLMM(model_poly3)


# Predict values from the model for visualization
df.data$predicted3 <- predict(model_poly3, newdata = df.data, re.form = NA)
plot_poly3 <-ggplot(df.data, aes(x = speech_rate, y = trial_median_rescaled)) +
  geom_point() +                                 # Adds the scatter plot
  geom_line(aes(y = predicted3), color = "blue") + # Adds the polynomial fit line
  geom_point(data = mean_values, aes(y = mean_trial_median_rescaled), color = "blue3", size = 3, shape = 21, fill = "lightblue") +  # Mean points
  labs(x = "Speech Rate",
       y = "Trial Median Rescaled",
       title = "Scatter Plot with Polynomial Fit- 3 factor")

# 4 FACTOR
# fit a polynomial regression
model_poly4 <- lmer(trial_median_rescaled ~ poly(speech_rate, 4) + 
                                   (1 | digit_span_score) + 
                                   (1 | digit_in_noise_score), data = df.data)
summary(model_poly4)
r2_poly4<-r.squaredGLMM(model_poly4)


# Predict values from the model for visualization
df.data$predicted4 <- predict(model_poly4, newdata = df.data, re.form = NA)
plot_poly4 <-ggplot(df.data, aes(x = speech_rate, y = trial_median_rescaled)) +
  geom_point() +                                 # Adds the scatter plot
  geom_line(aes(y = predicted4), color = "blue") + # Adds the polynomial fit line
  geom_point(data = mean_values, aes(y = mean_trial_median_rescaled), color = "blue3", size = 3, shape = 21, fill = "lightblue") +  # Mean points
  labs(x = "Speech Rate",
       y = "Trial Median Rescaled",
       title = "Scatter Plot with Polynomial Fit- 4 fctor")
# put plots together
grid.arrange(plot_linear, plot_poly2, plot_poly3, plot_poly4,ncol = 4)

# compare
anova(model_linear, model_poly)
r2_linear
r2_poly2
r2_poly3
r2_poly4
```
hm the non-linear relationship is not significant for 2 factor poly but it is for 3 and 4


```{r}
# visualize the mean trial median rescaled by speech rate
ggplot(mean_values, aes(x = factor(speech_rate), y = mean_trial_median_rescaled, fill = factor(speech_rate))) +
  geom_bar(stat = "identity") +
  labs(x = "Speech Rate", 
       y = "Mean of the Median Real-Time Comprehension Scores", 
       title = "Mean of the Median Real-Time Comprehension Scores by Speech Rate")+
  theme(legend.position = "none") + 
  scale_fill_brewer(palette = "Set3") +
  geom_text(aes(label = round(mean_trial_median_rescaled, 2)), vjust = -0.5, size = 3.5) 

```

```{r}
# do a cross validation instead (train-test)- linear, 2, 3, 4.

# Set seed for reproducibility
set.seed(123)

# Split the data into training (80%) and test (20%) sets
trainIndex <- createDataPartition(df.data$trial_median_rescaled, p = .8, #also you shouldn't be sampling unevenly from sr's
                                  list = FALSE, 
                                  times = 1)
trainData <- df.data[ trainIndex,]
testData  <- df.data[-trainIndex,]

# Define a function to fit the model, predict, and calculate MSE
fit_and_evaluate <- function(trainData, testData, degree) {
  # Create the formula dynamically
  terms <- paste("I(speech_rate^", 1:degree, ")", collapse = " + ")
  formula <- as.formula(paste("trial_median_rescaled ~ ", terms, " + (1 | digit_span_score) + (1 | digit_in_noise_score)"))
  
  # Fit the model
  model <- lmer(formula, data = trainData)
  
  # Predict on test data
  testData$predicted <- predict(model, newdata = testData, re.form = NA)
  
  # Calculate MSE
  mse <- mean((testData$trial_median_rescaled - testData$predicted)^2)
  
  return(list(model = model, predictions = testData, mse = mse))
}

# Evaluate models with different polynomial degrees up to 10
degrees <- 1:4
results <- lapply(degrees, function(degree) fit_and_evaluate(trainData, testData, degree))

# Extract predictions and MSEs for plotting
predictions <- lapply(results, function(result) result$predictions)
mses <- sapply(results, function(result) result$mse)

# Print MSEs for each model
mse_results <- data.frame(Degree = degrees, MSE = mses)
print(mse_results)

# Create a combined data frame with predictions
combined_data <- bind_rows(predictions, .id = "degree")

# Calculate mean trial_median_rescaled per speech rate for the test data
mean_values <- testData %>%
  group_by(speech_rate) %>%
  summarise(mean_trial_median_rescaled = mean(trial_median_rescaled, na.rm = TRUE))

# Plot observed test data points and fitted lines from training data for each model
ggplot(combined_data, aes(x = speech_rate, y = trial_median_rescaled)) +
  geom_point(alpha = 0.5) +  # Observed test data
  geom_line(aes(y = predicted, color = factor(degree)), size = 1) +  # Fitted lines from training data
  facet_wrap(~ degree, ncol = 2, scales = "free_y") +  # Facet by degree
  geom_point(data = mean_values, aes(x = speech_rate, y = mean_trial_median_rescaled), color = "blue", size = 3, shape = 21, fill = "lightblue") +  # Mean points
  labs(x = "Speech Rate", 
       y = "Trial Median Rescaled", 
       title = "Model Fits by Polynomial Degree",
       color = "Polynomial Degree") 
```
```{r}
# k-fold cross validation-> folds created over the slider data

# Set seed for reproducibility
set.seed(123)

# Define the number of folds
k <- 5

# Define a function to fit the model, predict, and calculate MSE
fit_and_evaluate <- function(trainData, testData, degree) {
  # Create the formula dynamically
  terms <- paste("I(speech_rate^", 1:degree, ")", collapse = " + ")
  formula <- as.formula(paste("trial_median_rescaled ~ ", terms, " + (1 | digit_span_score) + (1 | digit_in_noise_score)"))
  
  # Fit the model
  model <- lmer(formula, data = trainData)
  
  # Predict on test data
  testData$predicted <- predict(model, newdata = testData, re.form = NA)
  
  # Calculate MSE
  mse <- mean((testData$trial_median_rescaled - testData$predicted)^2)
  
  return(list(model = model, predictions = testData, mse = mse))
}

# Perform k-fold cross-validation
folds <- createFolds(df.data$trial_median_rescaled, k = k, list = TRUE, returnTrain = TRUE)

# Evaluate models with different polynomial degrees up to 10
degrees <- 1:4
results <- lapply(degrees, function(degree) {
  fold_results <- lapply(folds, function(fold_indices) {
    trainData <- df.data[fold_indices, ]
    testData <- df.data[-fold_indices, ]
    fit_and_evaluate(trainData, testData, degree)
  })
  
  # Combine results from all folds
  combined_predictions <- bind_rows(lapply(fold_results, function(res) res$predictions), .id = "fold")
  mse <- mean(sapply(fold_results, function(res) res$mse))
  
  return(list(predictions = combined_predictions, mse = mse))
})

# Extract predictions and MSEs for plotting
predictions <- lapply(results, function(result) result$predictions)
mses <- sapply(results, function(result) result$mse)

# Print MSEs for each model
mse_results <- data.frame(Degree = degrees, MSE = mses)
print(mse_results)

# Calculate mean trial_median_rescaled per speech rate for the test data in each fold
mean_values_list <- lapply(folds, function(fold_indices) {
  testData <- df.data[-fold_indices, ]
  testData %>%
    group_by(speech_rate) %>%
    summarise(mean_trial_median_rescaled = mean(trial_median_rescaled, na.rm = TRUE))
})

# Combine mean values into a single data frame
mean_values <- bind_rows(mean_values_list, .id = "fold")

# Plot observed test data points and fitted lines from training data for each model
combined_data <- bind_rows(predictions, .id = "degree")
ggplot(combined_data, aes(x = speech_rate, y = trial_median_rescaled)) +
  geom_point(alpha = 0.5) +  # Observed test data
  geom_line(aes(y = predicted, color = factor(degree)), size = 1) +  # Fitted lines from training data
  facet_wrap(~ degree, ncol = 2, scales = "free_y") +  # Facet by degree
  geom_point(data = mean_values, aes(x = speech_rate, y = mean_trial_median_rescaled), color = "blue", size = 2, shape = 21, fill = "lightblue", alpha= 0.5) +  # Mean points
  labs(x = "Speech Rate", 
       y = "Trial Median Rescaled", 
       title = "Model Fits by Polynomial Degree",
       color = "Polynomial Degree") 
```
When creating folds for cross-validation, the default behavior in many implementations, such as createFolds from the caret package, is to ensure that each fold is representative of the overall distribution of the target variable (in this case, trial_median_rescaled). This means that the distribution of the target variable in each fold is similar to the distribution in the entire dataset. This is particularly useful for stratified cross-validation, where you want each fold to have a similar distribution of the target classes in classification problems.

However, if you want to ensure that each fold has a representative sample of a different variable (such as speech_rate), you might need to partition the data manually. This means ensuring that each fold contains a balanced representation of the levels of speech_rate.

Example: Suppose you have data with five different speech rates (1, 2, 3, 4, and 5). Creating folds over speech_rate would ensure that each fold contains a balanced representation of these speech rates. In contrast, creating folds over the target variable would ensure that the distribution of trial_median_rescaled is similar across folds but might not balance the speech rates.

# plot test performance over train performace (so the lines form both of them)

```{r}
# k-fold cross validation-> folds created over the speech rates

# Set seed for reproducibility
set.seed(123)

# Define the number of folds
k <- 5

# Define a function to fit the model, predict, and calculate MSE
fit_and_evaluate <- function(trainData, testData, degree) {
  # Create the formula dynamically
  terms <- paste("I(speech_rate^", 1:degree, ")", collapse = " + ")
  formula <- as.formula(paste("trial_median_rescaled ~ ", terms, " + (1 | digit_span_score) + (1 | digit_in_noise_score)"))
  
  # Fit the model
  model <- lmer(formula, data = trainData)
  
  # Predict on test data
  testData$predicted <- predict(model, newdata = testData, re.form = NA)
  
  # Calculate MSE
  mse <- mean((testData$trial_median_rescaled - testData$predicted)^2)
  
  return(list(model = model, predictions = testData, mse = mse))
}

# Perform k-fold cross-validation
folds <- createFolds(df.data$speech_rate, k = k, list = TRUE, returnTrain = TRUE)

# Evaluate models with different polynomial degrees up to 10
degrees <- 1:4
results <- lapply(degrees, function(degree) {
  fold_results <- lapply(folds, function(fold_indices) {
    trainData <- df.data[fold_indices, ]
    testData <- df.data[-fold_indices, ]
    fit_and_evaluate(trainData, testData, degree)
  })
  
  # Combine results from all folds
  combined_predictions <- bind_rows(lapply(fold_results, function(res) res$predictions), .id = "fold")
  mse <- mean(sapply(fold_results, function(res) res$mse))
  
  return(list(predictions = combined_predictions, mse = mse))
})

# Extract predictions and MSEs for plotting
predictions <- lapply(results, function(result) result$predictions)
mses <- sapply(results, function(result) result$mse)

# Print MSEs for each model
mse_results <- data.frame(Degree = degrees, MSE = mses)
print(mse_results)

# Calculate mean trial_median_rescaled per speech rate for the test data in each fold
mean_values_list <- lapply(folds, function(fold_indices) {
  testData <- df.data[-fold_indices, ]
  testData %>%
    group_by(speech_rate) %>%
    summarise(mean_trial_median_rescaled = mean(trial_median_rescaled, na.rm = TRUE))
})

# Combine mean values into a single data frame
mean_values <- bind_rows(mean_values_list, .id = "fold")

# Plot observed test data points and fitted lines from training data for each model
combined_data <- bind_rows(predictions, .id = "degree")
ggplot(combined_data, aes(x = speech_rate, y = trial_median_rescaled)) +
  geom_point(alpha = 0.5) +  # Observed test data
  geom_line(aes(y = predicted, color = factor(degree)), size = 1) +  # Fitted lines from training data
  facet_wrap(~ degree, ncol = 2, scales = "free_y") +  # Facet by degree
  geom_point(data = mean_values, aes(x = speech_rate, y = mean_trial_median_rescaled), color = "blue", size = 2, shape = 21, fill = "lightblue", alpha= 0.5) +  # Mean points
  labs(x = "Speech Rate", 
       y = "Trial Median Rescaled", 
       title = "Model Fits by Polynomial Degree",
       color = "Polynomial Degree") 
```
```{r}
# with both test and train regressions
# Set seed for reproducibility
set.seed(123)

# Define the number of folds
k <- 5

# Define a function to fit the model, predict, and calculate MSE
fit_and_evaluate <- function(trainData, testData, degree) {
  # Create the formula dynamically
  terms <- paste("I(speech_rate^", 1:degree, ")", collapse = " + ")
  formula <- as.formula(paste("trial_median_rescaled ~ ", terms, " + (1 | digit_span_score) + (1 | digit_in_noise_score)"))
  
  # Fit the model
  model <- lmer(formula, data = trainData)
  
  # Predict on train and test data
  trainData$predicted <- predict(model, newdata = trainData, re.form = NA)
  testData$predicted <- predict(model, newdata = testData, re.form = NA)
  
  # Calculate MSE
  mse <- mean((testData$trial_median_rescaled - testData$predicted)^2)
  
  return(list(model = model, train_predictions = trainData, test_predictions = testData, mse = mse))
}

# Perform k-fold cross-validation
folds <- createFolds(df.data$speech_rate, k = k, list = TRUE, returnTrain = TRUE)

# Evaluate models with different polynomial degrees up to 4
degrees <- 1:4
results <- lapply(degrees, function(degree) {
  fold_results <- lapply(folds, function(fold_indices) {
    trainData <- df.data[fold_indices, ]
    testData <- df.data[-fold_indices, ]
    fit_and_evaluate(trainData, testData, degree)
  })
  
  # Combine results from all folds
  combined_train_predictions <- bind_rows(lapply(fold_results, function(res) res$train_predictions), .id = "fold")
  combined_test_predictions <- bind_rows(lapply(fold_results, function(res) res$test_predictions), .id = "fold")
  mse <- mean(sapply(fold_results, function(res) res$mse))
  
  return(list(train_predictions = combined_train_predictions, test_predictions = combined_test_predictions, mse = mse))
})

# Extract predictions and MSEs for plotting
train_predictions <- lapply(results, function(result) result$train_predictions)
test_predictions <- lapply(results, function(result) result$test_predictions)
mses <- sapply(results, function(result) result$mse)

# Print MSEs for each model
mse_results <- data.frame(Degree = degrees, MSE = mses)
print(mse_results)

# Calculate mean trial_median_rescaled per speech rate for the test data in each fold
mean_values_list <- lapply(folds, function(fold_indices) {
  testData <- df.data[-fold_indices, ]
  testData %>%
    group_by(speech_rate) %>%
    summarise(mean_trial_median_rescaled = mean(trial_median_rescaled, na.rm = TRUE))
})

# Combine mean values into a single data frame
mean_values <- bind_rows(mean_values_list, .id = "fold")

# Plot observed test data points and fitted lines from training and test data for each model
combined_train_data <- bind_rows(train_predictions, .id = "degree")
combined_test_data <- bind_rows(test_predictions, .id = "degree")

ggplot() +
  geom_point(data = combined_test_data, aes(x = speech_rate, y = trial_median_rescaled), alpha = 0.5) +  # Observed test data
  geom_line(data = combined_train_data, aes(x = speech_rate, y = predicted, color = factor(degree)), size = 1, alpha = 0.3) +  # Fitted lines from training data with transparency
  geom_line(data = combined_test_data, aes(x = speech_rate, y = predicted, linetype = factor(degree), color = factor(degree)), size = 1, alpha = 0.7) +  # Fitted lines from test data
  facet_wrap(~ degree, ncol = 2, scales = "free_y") +  # Facet by degree
  geom_point(data = mean_values, aes(x = speech_rate, y = mean_trial_median_rescaled), color = "blue", size = 2, shape = 21, fill = "lightblue", alpha = 0.5) +  # Mean points
  labs(x = "Speech Rate", 
       y = "Trial Median Rescaled", 
       title = "Model Fits by Polynomial Degree",
       color = "Polynomial Degree",
       linetype = "Test Data Fit") 

```
hm perfect fit if i am coding correctly- double check
you should do the k-fold cross validation again- something seems wrong given the MSEs, speech both regression lines shouldn't match perfectly
```{r}
# try like in tobi's textbook

library(modelr)
library(tidyr)
library(purrr)
library(broom)
library(dplyr)
library(ggplot2)

# Define the dataset and the number of folds
df.cross <- df.data %>% 
  crossv_kfold(k = 10) %>% 
  mutate(
    model_linear = map(.x = train, .f = ~ lmer(trial_median_rescaled ~ 1 + speech_rate + (1 | digit_span_score) + (1 | digit_in_noise_score), data = as_tibble(.))),
    model_poly2 = map(.x = train, .f = ~ lmer(trial_median_rescaled ~ 1 + speech_rate + I(speech_rate^2) + (1 | digit_span_score) + (1 | digit_in_noise_score), data = as_tibble(.))),
    model_poly3 = map(.x = train, .f = ~ lmer(trial_median_rescaled ~ 1 + speech_rate + I(speech_rate^2) + I(speech_rate^3) + (1 | digit_span_score) + (1 | digit_in_noise_score), data = as_tibble(.))),
    model_poly4 = map(.x = train, .f = ~ lmer(trial_median_rescaled ~ 1 + speech_rate + I(speech_rate^2) + I(speech_rate^3) + I(speech_rate^4) + (1 | digit_span_score) + (1 | digit_in_noise_score), data = as_tibble(.)))
  ) %>% 
  pivot_longer(cols = contains("model"),
               names_to = "model",
               values_to = "fit") %>% 
  mutate(rsquare = map2_dbl(.x = fit,
                            .y = test,
                            .f = ~ rsquare(.x, .y)))


# Ensure the model column is of type factor for grouping
df.cross <- df.cross %>% mutate(model = as.factor(model))

# Summarize the results
df.cross_summary <- df.cross %>%
  group_by(model) %>%
  summarize(mean_mse = mean(mse, na.rm = TRUE), sd_mse = sd(mse, na.rm = TRUE))

print(df.cross_summary)

# Plot the MSE values
ggplot(df.cross, aes(x = model, y = mse)) +
  geom_boxplot() +
  labs(title = "MSE Values by Model Complexity",
       x = "Model Complexity",
       y = "MSE")
```

# Extra Analysis

## Correlation btw predictors

```{r, warning=FALSE}
# visualize the correlations btw all predictors.

# Filter out rows where glove_sum_max_similarity_rescaled is NA
df.filt<- df.data %>%
  filter(!is.na(glove_sum_max_similarity_rescaled))

df.corr <- df.filt %>%
  select(trial_median_rescaled, glove_sum_max_similarity_rescaled, likert_rescaled, 
         multiple_choice_accuracy, digit_span_score, digit_in_noise_score) %>%
  rename(
    `real-time comprehension` = trial_median_rescaled,
    summary = glove_sum_max_similarity_rescaled,
    `10-point scale` = likert_rescaled,
    `multiple choice accuracy` = multiple_choice_accuracy,
    `working memory` = digit_span_score,
    `hearing in noise` = digit_in_noise_score
  )


# Calculate the correlation matrix and p-values
corr_matrix <- rcorr(as.matrix(df.corr))
correlations <- corr_matrix$r
p_values <- corr_matrix$P

# Replace diagonal p-values with NA for clarity
diag(p_values) <- NA

# Print the correlation matrix and p-values
print(correlations)
print(p_values)

# Customize the color scale
color_scale <- colorRampPalette(c("magenta","snow2","mediumaquamarine"))(200)

# Visualize the correlation matrix with significance stars and customized color scale

corrplot(correlations, method = "circle", type = "upper", p.mat = p_values, 
         sig.level = c(0.001, 0.01, 0.05), insig = "label_sig", 
         pch.cex = 0.7, pch.col = "black", col = color_scale, 
         tl.col = "black", cl.pos = "r", addCoef.col = NULL, 
         tl.cex = 0.7, cl.lim = c(-1, 1), cl.ratio = 0.25, cl.cex = 0.6)

```

```{r}
# correlation matrix with ind data points
ggpairs(df.corr, progress = FALSE)
```

```{r}
# why is there a negative correlation btw hearing in noise and likert & wm
# mean & sd of 10 point scale -> order them by each persons’ hearing-in-noise score. 
# Maybe there is an outlier participant?
df_hearing <- df.raw %>%
  group_by(participant) %>%
  summarise(
    mean_likert_rescaled = mean(likert_rescaled, na.rm = TRUE),
    sd_likert_rescaled = sd(likert_rescaled, na.rm = TRUE),
    digit_in_noise_score = unique(digit_in_noise_score)
  ) %>%
  arrange(digit_in_noise_score)
df_hearing

ggplot(df_hearing, aes(x = digit_in_noise_score, y = mean_likert_rescaled)) +
  geom_point() +
  labs(
    title = "Scatter Plot",
    x = "Digit In Noise Score",
    y = "Mean Likert Rescaled"
  )
```

## Regression models with other semantic similarity calculations

### Semantic similarity as the only fixed efffect- real time comprehension measure
```{r}
# Fixed effect of Summary- Glove (average of maximum cosine values of segment text words) on real time comprehension
model_glove2 <- lmer(trial_median_rescaled ~  
                glove_text_average_max_similarity +
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_glove2)
```

```{r}
# Fixed effect of Summary- BERT (cosine values of summaries) on real time comprehension
model_bert <- lmer(trial_median_rescaled ~  
                bert_cosine_similarity +
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_bert)
```

### All post-hoc measures as fixed effects- real time comprehension measure
```{r}
# with glove text_average
model_all_glove2 <- lmer(trial_median_rescaled ~  
                glove_text_average_max_similarity +
                likert_rescaled + 
                multiple_choice_accuracy + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_all_glove2)
```

```{r}
# with BERT
model_all_bert <- lmer(trial_median_rescaled ~  
                bert_cosine_similarity +
                likert_rescaled + 
                multiple_choice_accuracy + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_all_bert)
```
