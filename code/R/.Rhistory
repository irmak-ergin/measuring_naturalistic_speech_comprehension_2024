df$summary_median <- median_value  # Add the median as a new column
return(df)
})
# Check the first dataframe in the list to confirm the summary_median column has been added
head(list_of_dfs_with_median[[8]])
# snaity check
# Directly create the unique identifier for each speech_rate-participant_id combination without converting to factors first
df.group <- df.raw %>%
mutate(group_id = interaction(speech_rate, participant))%>%
select(group_id, summary_duration)
# Now split the dataframe into a list of dataframes based on the group_id
list_of_dfs <- split(df.group, f = df.group$group_id)
# Iterate over each dataframe in the list to add the summary_median column
list_of_dfs_with_median <- lapply(list_of_dfs, function(df) {
median_value <- median(df$summary_duration, na.rm = TRUE)  # Calculate the median of summary_duration
df$summary_median <- median_value  # Add the median as a new column
return(df)
})
# Check the first dataframe in the list to confirm the summary_median column has been added
head(list_of_dfs_with_median[[9]])
# snaity check
# Directly create the unique identifier for each speech_rate-participant_id combination without converting to factors first
df.group <- df.raw %>%
mutate(group_id = interaction(speech_rate, participant))%>%
select(group_id, summary_duration)
# Now split the dataframe into a list of dataframes based on the group_id
list_of_dfs <- split(df.group, f = df.group$group_id)
# Iterate over each dataframe in the list to add the summary_median column
list_of_dfs_with_median <- lapply(list_of_dfs, function(df) {
median_value <- median(df$summary_duration, na.rm = TRUE)  # Calculate the median of summary_duration
df$summary_median <- median_value  # Add the median as a new column
return(df)
})
# Check the first dataframe in the list to confirm the summary_median column has been added
head(list_of_dfs_with_median[[10]])
# snaity check
# Directly create the unique identifier for each speech_rate-participant_id combination without converting to factors first
df.group <- df.raw %>%
mutate(group_id = interaction(speech_rate, participant))%>%
select(group_id, summary_duration)
# Now split the dataframe into a list of dataframes based on the group_id
list_of_dfs <- split(df.group, f = df.group$group_id)
# Iterate over each dataframe in the list to add the summary_median column
list_of_dfs_with_median <- lapply(list_of_dfs, function(df) {
median_value <- median(df$summary_duration, na.rm = TRUE)  # Calculate the median of summary_duration
df$summary_median <- median_value  # Add the median as a new column
return(df)
})
# Check the first dataframe in the list to confirm the summary_median column has been added
head(list_of_dfs_with_median[[10]])
# snaity check
# Directly create the unique identifier for each speech_rate-participant_id combination without converting to factors first
df.group <- df.raw %>%
mutate(group_id = interaction(speech_rate, participant))%>%
select(group_id, summary_duration)
# Now split the dataframe into a list of dataframes based on the group_id
list_of_dfs <- split(df.group, f = df.group$group_id)
# Iterate over each dataframe in the list to add the summary_median column
list_of_dfs_with_median <- lapply(list_of_dfs, function(df) {
median_value <- median(df$summary_duration, na.rm = TRUE)  # Calculate the median of summary_duration
df$summary_median <- median_value  # Add the median as a new column
return(df)
})
# Check the first dataframe in the list to confirm the summary_median column has been added
head(list_of_dfs_with_median[[11]])
# snaity check
# Directly create the unique identifier for each speech_rate-participant_id combination without converting to factors first
df.group <- df.raw %>%
mutate(group_id = interaction(speech_rate, participant))%>%
select(group_id, summary_duration)
# Now split the dataframe into a list of dataframes based on the group_id
list_of_dfs <- split(df.group, f = df.group$group_id)
# Iterate over each dataframe in the list to add the summary_median column
list_of_dfs_with_median <- lapply(list_of_dfs, function(df) {
median_value <- median(df$summary_duration, na.rm = TRUE)  # Calculate the median of summary_duration
df$summary_median <- median_value  # Add the median as a new column
return(df)
})
# Check the first dataframe in the list to confirm the summary_median column has been added
head(list_of_dfs_with_median[[12]])
# snaity check
# Directly create the unique identifier for each speech_rate-participant_id combination without converting to factors first
df.group <- df.raw %>%
mutate(group_id = interaction(speech_rate, participant))%>%
select(group_id, summary_duration)
# Now split the dataframe into a list of dataframes based on the group_id
list_of_dfs <- split(df.group, f = df.group$group_id)
# Iterate over each dataframe in the list to add the summary_median column
list_of_dfs_with_median <- lapply(list_of_dfs, function(df) {
median_value <- median(df$summary_duration, na.rm = TRUE)  # Calculate the median of summary_duration
df$summary_median <- median_value  # Add the median as a new column
return(df)
})
# Check the first dataframe in the list to confirm the summary_median column has been added
head(list_of_dfs_with_median[[13]])
# snaity check
# Directly create the unique identifier for each speech_rate-participant_id combination without converting to factors first
df.group <- df.raw %>%
mutate(group_id = interaction(speech_rate, participant))%>%
select(group_id, summary_duration)
# Now split the dataframe into a list of dataframes based on the group_id
list_of_dfs <- split(df.group, f = df.group$group_id)
# Iterate over each dataframe in the list to add the summary_median column
list_of_dfs_with_median <- lapply(list_of_dfs, function(df) {
median_value <- median(df$summary_duration, na.rm = TRUE)  # Calculate the median of summary_duration
df$summary_median <- median_value  # Add the median as a new column
return(df)
})
# Check the first dataframe in the list to confirm the summary_median column has been added
head(list_of_dfs_with_median[[14]])
# snaity check
# Directly create the unique identifier for each speech_rate-participant_id combination without converting to factors first
df.group <- df.raw %>%
mutate(group_id = interaction(speech_rate, participant))%>%
select(group_id, summary_duration)
# Now split the dataframe into a list of dataframes based on the group_id
list_of_dfs <- split(df.group, f = df.group$group_id)
# Iterate over each dataframe in the list to add the summary_median column
list_of_dfs_with_median <- lapply(list_of_dfs, function(df) {
median_value <- median(df$summary_duration, na.rm = TRUE)  # Calculate the median of summary_duration
df$summary_median <- median_value  # Add the median as a new column
return(df)
})
# Check the first dataframe in the list to confirm the summary_median column has been added
head(list_of_dfs_with_median[[15]])
# snaity check
# Directly create the unique identifier for each speech_rate-participant_id combination without converting to factors first
df.group <- df.raw %>%
mutate(group_id = interaction(speech_rate, participant))%>%
select(group_id, summary_duration)
# Now split the dataframe into a list of dataframes based on the group_id
list_of_dfs <- split(df.group, f = df.group$group_id)
# Iterate over each dataframe in the list to add the summary_median column
list_of_dfs_with_median <- lapply(list_of_dfs, function(df) {
median_value <- median(df$summary_duration, na.rm = TRUE)  # Calculate the median of summary_duration
df$summary_median <- median_value  # Add the median as a new column
return(df)
})
# Check the first dataframe in the list to confirm the summary_median column has been added
head(list_of_dfs_with_median[[16]])
# snaity check
# Directly create the unique identifier for each speech_rate-participant_id combination without converting to factors first
df.group <- df.raw %>%
mutate(group_id = interaction(speech_rate, participant))%>%
select(group_id, summary_duration)
# Now split the dataframe into a list of dataframes based on the group_id
list_of_dfs <- split(df.group, f = df.group$group_id)
# Iterate over each dataframe in the list to add the summary_median column
list_of_dfs_with_median <- lapply(list_of_dfs, function(df) {
median_value <- median(df$summary_duration, na.rm = TRUE)  # Calculate the median of summary_duration
df$summary_median <- median_value  # Add the median as a new column
return(df)
})
# Check the first dataframe in the list to confirm the summary_median column has been added
head(list_of_dfs_with_median[[17]])
# snaity check
# Directly create the unique identifier for each speech_rate-participant_id combination without converting to factors first
df.group <- df.raw %>%
mutate(group_id = interaction(speech_rate, participant))%>%
select(group_id, summary_duration)
# Now split the dataframe into a list of dataframes based on the group_id
list_of_dfs <- split(df.group, f = df.group$group_id)
# Iterate over each dataframe in the list to add the summary_median column
list_of_dfs_with_median <- lapply(list_of_dfs, function(df) {
median_value <- median(df$summary_duration, na.rm = TRUE)  # Calculate the median of summary_duration
df$summary_median <- median_value  # Add the median as a new column
return(df)
})
# Check the first dataframe in the list to confirm the summary_median column has been added
head(list_of_dfs_with_median[[18]])
View(median_duration_data)
View(median_duration_per_part)
View(median_summary_duration)
median_summary_duration_all <- df.raw %>%
group_by(speech_rate) %>%
summarize(median_duration = median(summary_duration, na.rm = TRUE), .groups = 'drop')
print(median_summary_duration_all)
median_summary_duration_all <- df.raw %>%
group_by(speech_rate)
View(median_duration_data)
View(median_duration_per_part)
View(median_summary_duration_all)
median_summary_duration_all <- df.raw %>%
group_by(speech_rate)
median_summary_duration_all <- df.raw %>%
group_by(speech_rate) %>%
summarize(median_duration = median(summary_duration, na.rm = TRUE), .groups = 'drop')
print(median_summary_duration_all)
#sanity check per speech rate
# Directly create the unique identifier for each speech_rate
df.group <- df.raw %>%
mutate(group_id = speech_rate)%>%
select(group_id, summary_duration)
# Now split the dataframe into a list of dataframes based on the group_id
list_of_dfs <- split(df.group, f = df.group$group_id)
# Iterate over each dataframe in the list to add the summary_median column
list_of_dfs_with_median <- lapply(list_of_dfs, function(df) {
median_value <- median(df$summary_duration, na.rm = TRUE)  # Calculate the median of summary_duration
df$summary_median <- median_value  # Add the median as a new column
return(df)
})
list_of_dfs
list_of_dfs_with_median
# Calculate the average summary_duration per speech_rate for each participant
avg_duration_per_speech_rate <- df.raw %>%
group_by(participant, speech_rate) %>%
summarise(average_summary_duration = mean(summary_duration, na.rm = TRUE)) %>%
ungroup()
# Ensure speech_rate and participant are treated as factors
avg_duration_per_speech_rate$speech_rate <- factor(avg_duration_per_speech_rate$speech_rate)
avg_duration_per_speech_rate$participant <- factor(avg_duration_per_speech_rate$participant)
# Plot
ggplot(avg_duration_per_speech_rate, aes(x = speech_rate, y = average_summary_duration, fill = speech_rate)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.75)) +
geom_text(aes(label = sprintf("%.2f", average_summary_duration)),
position = position_dodge(width = 0.75), vjust = -0.25, color = "black", size = 3.5) +
facet_wrap(~ participant, scales = "free_y", nrow = 1) +
theme_classic() +
labs(title = "Average Summary Duration by Speech Rate for Each Participant",
x = "Speech Rate",
y = "Average Summary Duration (seconds)") +
scale_fill_brewer(palette = "Pastel1", type = "qual") +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none")
# plot sem similarity per speech rate
average_similarity <- df.data %>%
group_by(speech_rate) %>%
summarise(average_similarity = mean(sum_max_similarity, na.rm = TRUE))
# Plot with bars in light green
ggplot(average_similarity, aes(x = factor(speech_rate), y = average_similarity, fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen1") +
labs(x = "Speech Rate", y = "Average Sum Max Similarity",
title = "Average Sum Max Similarity by Speech Rate") +
geom_text(aes(label = round(average_similarity, 2)),
vjust = -0.3, position = position_dodge(width = 0.9), size = 3) +
theme(legend.position = "none")
# plot sem similarity per speech rate per participant
average_similarity <- df.data %>%
group_by(participant, speech_rate) %>%
summarise(average_similarity = mean(sum_max_similarity, na.rm = TRUE), .groups = 'drop')
# Plot with bars in light green and facet by participant
average_similarity_participant<- ggplot(average_similarity, aes(x = factor(speech_rate), y = average_similarity, fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen1") +
facet_wrap(~participant, ncol = 3) +  # Faceted by participant
labs(x = "Speech Rate", y = "Average Sum Max Similarity",
title = "Average Sum Max Similarity by Participant and Speech Rate") +
geom_text(aes(label = round(average_similarity, 2)),
vjust = -0.3, position = position_dodge(width = 0.9), size = 3) +
theme(legend.position = "none")
print (average_similarity_participant)
#What about the number of words they reported regardless of the semantic similarity of the words?
# Calculate the number of words
df.data <- df.data %>%
mutate(word_count = str_count(textbox.text_corrected, "\\S+") + 1)
# Aggregate data by speech rate
aggregated_data <- df.data %>%
group_by(speech_rate) %>%
summarize(avg_word_count = mean(word_count, na.rm = TRUE),
.groups = 'drop')
# Plot
ggplot(aggregated_data, aes(x = as.factor(speech_rate), y = avg_word_count,
fill = as.factor(speech_rate))) +
geom_bar(stat = "identity" , position = "dodge", fill = "darkseagreen3") +
geom_text(aes(label = sprintf("%.2f", avg_word_count)), vjust = -0.5, color = "black") +
labs(title = "Average Number of Words Reported Across Speech Rates",
x = "Speech Rate",
y = "Average Word Count",
fill = "Speech Rate") +
theme(legend.position = "none")
# number of words for each speech rate per participant
# Calculate the number of words
df.data <- df.data %>%
mutate(word_count = str_count(textbox.text_corrected, "\\S+") + 1)
# Aggregate data by participant and speech rate
aggregated_data <- df.data %>%
group_by(participant, speech_rate) %>%
summarize(avg_word_count = mean(word_count, na.rm = TRUE),
.groups = 'drop')
# Plot with faceting by participant
plot_num_words_part <- ggplot(aggregated_data, aes(x = as.factor(speech_rate), y = avg_word_count,
fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen3") +
geom_text(aes(label = sprintf("%.2f", avg_word_count)),
vjust = -0.5, color = "black", position = position_dodge(width = 0.9)) +
labs(title = "Average Number of Words Reported Across Speech Rates",
x = "Speech Rate",
y = "Average Word Count",
fill = "Speech Rate") +
facet_wrap(~participant, ncol = 3, scales = "free_y") +
theme(legend.position = "none")
# Run the plot
print(plot_num_words_part)
View(df.data)
# load the sem similarity score data
# Load the csv file into R as a dataset named df.data
df.data <- read.csv("/Users/irmakergin/Desktop/df_raw_similarity.csv")
View(df.data)
# plot sem similarity for sentence embedding per speech rate
average_similarity_sent <- df.data %>%
group_by(speech_rate) %>%
summarise(average_similarity_sent = mean(cosine_similarity, na.rm = TRUE))
# Plot with bars in light green
ggplot(average_similarity_sent, aes(x = factor(speech_rate), y = average_similarity_sent, fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen1") +
labs(x = "Speech Rate", y = "Average Sum Max Similarity",
title = "Average Sum Max Similarity by Speech Rate") +
geom_text(aes(label = round(average_similarity_sent, 2)),
vjust = -0.3, position = position_dodge(width = 0.9), size = 3) +
theme(legend.position = "none")
#HERE
# plot sem similarity for sentence embedding per speech rate
average_similarity_sent <- df.data %>%
group_by(speech_rate) %>%
summarise(average_similarity_sent = median(cosine_similarity, na.rm = TRUE))
# Plot with bars in light green
ggplot(average_similarity_sent, aes(x = factor(speech_rate), y = average_similarity_sent, fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen1") +
labs(x = "Speech Rate", y = "Average Sum Max Similarity",
title = "Average Sum Max Similarity by Speech Rate") +
geom_text(aes(label = round(average_similarity_sent, 2)),
vjust = -0.3, position = position_dodge(width = 0.9), size = 3) +
theme(legend.position = "none")
#HERE
# plot sem similarity for sentence embedding per speech rate
average_similarity_sent <- df.data %>%
group_by(speech_rate) %>%
summarise(average_similarity_sent = mean(cosine_similarity, na.rm = TRUE))
# Plot with bars in light green
ggplot(average_similarity_sent, aes(x = factor(speech_rate), y = average_similarity_sent, fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen1") +
labs(x = "Speech Rate", y = "Average Sum Max Similarity",
title = "Average Sum Max Similarity by Speech Rate") +
geom_text(aes(label = round(average_similarity_sent, 2)),
vjust = -0.3, position = position_dodge(width = 0.9), size = 3) +
theme(legend.position = "none")
#HERE
# plot sem similarity for sentence embedding per speech rate
average_similarity_sent <- df.data %>%
group_by(speech_rate) %>%
summarise(average_similarity_sent = mean(cosine_similarity, na.rm = TRUE))
# Plot with bars in light green
ggplot(average_similarity_sent, aes(x = factor(speech_rate), y = average_similarity_sent, fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen1") +
labs(x = "Speech Rate", y = "Average Sentence Embedding Similarity",
title = "Average Sum Max Similarity by Speech Rate") +
geom_text(aes(label = round(average_similarity_sent, 2)),
vjust = -0.3, position = position_dodge(width = 0.9), size = 3) +
theme(legend.position = "none")
#HERE
# plot sem similarity for sentence embedding per speech rate
average_similarity_sent <- df.data %>%
group_by(speech_rate) %>%
summarise(average_similarity_sent = mean(cosine_similarity, na.rm = TRUE))
# Plot with bars in light green
ggplot(average_similarity_sent, aes(x = factor(speech_rate), y = average_similarity_sent, fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen1") +
labs(x = "Speech Rate", y = "Average Sentence Embedding Similarity",
title = "Average Cosine Similarity by Speech Rate") +
geom_text(aes(label = round(average_similarity_sent, 2)),
vjust = -0.3, position = position_dodge(width = 0.9), size = 3) +
theme(legend.position = "none")
# load the sem similarity score data
# Load the csv file into R as a dataset named df.data
df.data <- read.csv("/Users/irmakergin/Desktop/df_raw_similarity_mpnet.csv")
# load the sem similarity score data
# Load the csv file into R as a dataset named df.data
df.data <- read.csv("/Users/irmakergin/Desktop/df_raw_similarity_mpnet.csv")
#HERE
# plot sem similarity for sentence embedding per speech rate
average_similarity_sent <- df.data %>%
group_by(speech_rate) %>%
summarise(average_similarity_sent = mean(cosine_similarity, na.rm = TRUE))
# Plot with bars in light green
ggplot(average_similarity_sent, aes(x = factor(speech_rate), y = average_similarity_sent, fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen1") +
labs(x = "Speech Rate", y = "Average Sentence Embedding Similarity",
title = "Average Cosine Similarity by Speech Rate") +
geom_text(aes(label = round(average_similarity_sent, 2)),
vjust = -0.3, position = position_dodge(width = 0.9), size = 3) +
theme(legend.position = "none")
View(df.data)
# load the sem similarity score data
# Load the csv file into R as a dataset named df.data
df.data <- read.csv("/Users/irmakergin/Desktop/df_raw_similarity_mpnet.csv")
#HERE
# plot sem similarity for sentence embedding per speech rate
average_similarity_sent <- df.data %>%
group_by(speech_rate) %>%
summarise(average_similarity_sent = mean(cosine_similarity, na.rm = TRUE))
# Plot with bars in light green
ggplot(average_similarity_sent, aes(x = factor(speech_rate), y = average_similarity_sent, fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen1") +
labs(x = "Speech Rate", y = "Average Sentence Embedding Similarity",
title = "Average Cosine Similarity by Speech Rate") +
geom_text(aes(label = round(average_similarity_sent, 2)),
vjust = -0.3, position = position_dodge(width = 0.9), size = 3) +
theme(legend.position = "none")
# load the sem similarity score data
# Load the csv file into R as a dataset named df.data
df.data <- read.csv("/Users/irmakergin/Desktop/df_raw_similarity.csv")
#HERE
# plot sem similarity for sentence embedding per speech rate
average_similarity_sent <- df.data %>%
group_by(speech_rate) %>%
summarise(average_similarity_sent = mean(cosine_similarity, na.rm = TRUE))
# Plot with bars in light green
ggplot(average_similarity_sent, aes(x = factor(speech_rate), y = average_similarity_sent, fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen1") +
labs(x = "Speech Rate", y = "Average Sentence Embedding Similarity",
title = "Average Cosine Similarity by Speech Rate") +
geom_text(aes(label = round(average_similarity_sent, 2)),
vjust = -0.3, position = position_dodge(width = 0.9), size = 3) +
theme(legend.position = "none")
# per participant
# Assuming df.data is your data frame and it includes a 'participant' column
average_similarity_sent_participant <- df.data %>%
group_by(participant, speech_rate) %>%
summarise(average_similarity_sent = mean(cosine_similarity, na.rm = TRUE)) %>%
ungroup()
# Plot with bars, faceted per participant
ggplot(average_similarity_sent_participant, aes(x = factor(speech_rate), y = average_similarity_sent, fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen1") +
labs(x = "Speech Rate", y = "Average Sentence Embedding Similarity",
title = "Average Cosine Similarity by Speech Rate and Participant") +
geom_text(aes(label = round(average_similarity_sent, 2)), vjust = -0.3, position = position_dodge(width = 0.9), size = 3) +
theme(legend.position = "none") +
facet_wrap(~participant, ncol = 5) # Adjust the number of columns based on the number of participants
# I HOPE THIS DIDN'T WORK
# plot semantic similarity per speech rate excluding first 8 of each trial
# Step 1: Exclude the first 8 trials of each speech rate
df.filtered <- df.data %>%
group_by(speech_rate) %>%
arrange(speech_rate, .by_group = TRUE) %>% # Remove this line if your data is already in order
mutate(trial_rank = row_number()) %>%
filter(trial_rank > 8) %>%
ungroup()
# Step 2: Calculate average similarity, now excluding the first 8 trials
average_similarity <- df.filtered %>%
group_by(speech_rate) %>%
summarise(average_similarity = mean(sum_max_similarity, na.rm = TRUE))
# Step 3: Plot with bars in light green
ggplot(average_similarity, aes(x = factor(speech_rate), y = average_similarity, fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "seagreen") +
labs(x = "Speech Rate", y = "Average Sum Max Similarity",
title = "Average Sum Max Similarity by Speech Rate (Excluding First 8 Trials)") +
geom_text(aes(label = round(average_similarity, 2)),
vjust = -0.3, position = position_dodge(width = 0.9), size = 3) +
theme(legend.position = "none")
# number of words excluding first 8 trials
df.filtered <- df.data %>%
group_by(speech_rate) %>%
arrange(speech_rate, .by_group = TRUE) %>% # Remove this line if your data is already in order
mutate(trial_rank = row_number()) %>%
filter(trial_rank > 8) %>%
ungroup()
# Calculate the number of words
df.filtered <- df.filtered %>%
mutate(word_count = str_count(textbox.text_corrected, "\\S+") + 1)
# Aggregate data by speech rate
aggregated_data <- df.filtered %>%
group_by(speech_rate) %>%
summarize(avg_word_count = mean(word_count, na.rm = TRUE),
.groups = 'drop')
# Plot
ggplot(aggregated_data, aes(x = as.factor(speech_rate), y = avg_word_count,
fill = as.factor(speech_rate))) +
geom_bar(stat = "identity" , position = "dodge", fill = "darkseagreen3") +
geom_text(aes(label = sprintf("%.2f", avg_word_count)), vjust = -0.5, color = "black") +
labs(title = "Average Number of Words Reported Across Speech Rates",
x = "Speech Rate",
y = "Average Word Count",
fill = "Speech Rate") +
theme(legend.position = "none")
# number of words for first 8 trials
# Calculate the number of words
df.filtered <- df.filtered %>%
mutate(word_count = str_count(textbox.text_corrected, "\\S+") + 1)
# Aggregate data by speech rate
aggregated_data <- df.filtered %>%
group_by(speech_rate) %>%
summarize(avg_word_count = mean(word_count, na.rm = TRUE),
.groups = 'drop')
# Plot
ggplot(aggregated_data, aes(x = as.factor(speech_rate), y = avg_word_count,
fill = as.factor(speech_rate))) +
geom_bar(stat = "identity" , position = "dodge", fill = "violet") +
geom_text(aes(label = sprintf("%.2f", avg_word_count)), vjust = -0.5, color = "black") +
labs(title = "Average Number of Words Reported by Speech Rate (First 8 trials)",
x = "Speech Rate",
y = "Average Word Count",
fill = "Speech Rate") +
theme(legend.position = "none")
# the first 8 trials
# Step 1: Exclude the first 8 trials of each speech rate
df.filtered <- df.data %>%
group_by(speech_rate) %>%
arrange(speech_rate, .by_group = TRUE) %>% # Remove this line if your data is already in order
mutate(trial_rank = row_number()) %>%
filter(trial_rank < 9) %>%
ungroup()
# Step 2: Calculate average similarity, now excluding the first 8 trials
average_similarity <- df.filtered %>%
group_by(speech_rate) %>%
summarise(average_similarity = mean(sum_max_similarity, na.rm = TRUE))
# Step 3: Plot with bars in light green
ggplot(average_similarity, aes(x = factor(speech_rate), y = average_similarity, fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "purple") +
labs(x = "Speech Rate", y = "Average Sum Max Similarity",
title = "Average Sum Max Similarity by Speech Rate (First 8 Trials)") +
geom_text(aes(label = round(average_similarity, 2)),
vjust = -0.3, position = position_dodge(width = 0.9), size = 3) +
theme(legend.position = "none")
