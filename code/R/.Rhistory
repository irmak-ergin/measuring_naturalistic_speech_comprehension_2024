list("p_2", "Someday_2_condition-4x.wav", paste0(organized_data_path, "/", "p_2/slider_p_2/slider_p_2_001_Someday_2_condition-4x.wav.csv")),
list("p_3", "Someday_3_condition-5x.wav", paste0(organized_data_path, "/", "p_3/slider_p_3/slider_p_3_001_Someday_3_condition-5x.wav.csv")),
list("p_4", "Someday_4_condition-2x.wav", paste0(organized_data_path, "/", "p_4/slider_p_4/slider_p_4_001_Someday_4_condition-2x.wav.csv")),
list("p_5", "Someday_5_condition-3x.wav", paste0(organized_data_path, "/", "p_5/slider_p_5/slider_p_5_001_Someday_5_condition-3x.wav.csv")),
list("p_6", "Someday_6_condition-5x.wav", paste0(organized_data_path, "/",  "p_6/slider_p_6/slider_p_6_001_Someday_6_condition-5x.wav.csv")),
list("p_7", "Someday_7_condition-5x.wav", paste0(organized_data_path, "/",  "p_7/slider_p_7/slider_p_7_001_Someday_7_condition-5x.wav.csv")),
list("p_8", "Someday_8_condition-5x.wav", paste0(organized_data_path, "/",  "p_8/slider_p_8/slider_p_8_001_Someday_8_condition-5x.wav.csv")),
list("p_9", "Someday_9_condition-3x.wav", paste0(organized_data_path, "/",  "p_9/slider_p_9/slider_p_9_001_Someday_9_condition-3x.wav.csv"))
#list("p_10", "Someday_10_condition.wav", paste0(organized_data_path, "/",  #"p_10/slider_p_10/slider_p_10_001_Someday_10_condition.wav.csv")),
# list("p_11", "Someday_11_condition-2x.wav", paste0(organized_data_path, "/",  #"p_11/slider_p_11/slider_p_11_001_Someday_11_condition-2x.wav.csv")),
#list("p_12", "Someday_12_condition.wav", paste0(organized_data_path, "/",  #"p_12/slider_p_12/slider_p_12_001_Someday_12_condition.wav.csv"))
)
# Apply the function to each participant
for (p in participants_files) {
df.raw <- process_slider_file(p[[1]], p[[2]], p[[3]])
}
# Re-scale slider values to be between 0-1
# Convert slider_values from string to numeric lists
df.raw$slider_values <- strsplit(as.character(df.raw$slider_values),
",\\s*")
df.raw$slider_values <- lapply(df.raw$slider_values,
function(x) as.numeric(x))
rescale_values <- function(values) {
sapply(values, function(x) x / 255)
}
# Apply the rescaling function to each row's slider_values
df.raw$slider_values_rescaled <- lapply(df.raw$slider_values,
rescale_values)
# Sanity check: create a column showing number of slider values
df.raw <- df.raw %>%
mutate(slider_values_number = lengths(slider_values_rescaled))
# Depict slider movements
df.raw$slider_values_rescaled <- sapply(df.raw$slider_values_rescaled,
function(x) paste(x, collapse = ","))
df.slider <- df.raw%>%
mutate(slider_values_rescaled = strsplit(slider_values_rescaled, ",")) %>%
mutate(slider_rescale_float = map(slider_values_rescaled, ~as.numeric(.x)))
df_long <- df.slider %>%
mutate(id = row_number()) %>%
unnest(slider_rescale_float) %>%
rename(slider_value = slider_rescale_float) %>%
group_by(id, speech_rate) %>%
mutate(index = row_number()) %>%
ungroup() %>%
select(-slider_values_rescaled)
# Set the x-axis limits
p_slider_per_rate <- ggplot(df_long, aes(x = index,
y = slider_value,
group = interaction(id),
color = as.factor(id))) +
geom_line(alpha = 0.5, size = 0.5) +
scale_x_continuous(name = "Index of the Slider Value") +
scale_y_continuous(name = "Slider Value") +
facet_wrap(~ speech_rate,
scales = 'free_x',
ncol = 1) +
scale_color_manual(values = colorRampPalette(brewer.pal(12, "Paired"))
(length(unique(df_long$id)))) +
theme(legend.position = "none") +
labs(title = 'Slider Values by Speech Rate',
caption = "Each line represents the comprehension trajectory at different speech rates.
As the speech rate increases, comprehension scores seem to get lower."
)
# histograms
p_flipped_histogram_per_rate <- ggplot(df_long, aes(x = slider_value)) +
geom_histogram(bins = 10, fill = "skyblue", color = "navy") +
coord_flip() +
facet_wrap(~speech_rate, scales = 'free_x', ncol = 1) +
labs(y = "Slider Value Range", x = "Count", title = "Number of Slider Values by Speech Rate") +
theme(legend.position = "none") +
scale_y_continuous(labels = comma)
# combine
combined_plot <- p_slider_per_rate + p_flipped_histogram_per_rate +
plot_layout(ncol = 2, widths = c(2, 1)) +
theme(strip.background = element_rect(fill = "white", color = "black"))
print(combined_plot)
# Sanity check for slider values
rows_not_fitting_criteria_selected_columns <- df.raw %>%
filter(
(speech_rate == 1 & (slider_values_number < 6000 | slider_values_number > 8000)) |
(speech_rate == 2 & (slider_values_number < 3000 | slider_values_number > 4000)) |
(speech_rate == 3 & (slider_values_number < 2500 | slider_values_number > 3000)) |
(speech_rate == 4 & (slider_values_number < 1500 | slider_values_number > 2000)) |
(speech_rate == 5 & (slider_values_number < 1200 | slider_values_number > 1550))
) %>%
select(participant, wav_file, index, slider_values_number)
# To print the resulting selected columns for rows that do not fit the criteria
print(rows_not_fitting_criteria_selected_columns)
# manually inspect those to see if they are just sampled slightly longer/ shorter or if the match is wrong
# Only trials with the slider starting from point, with time one the x axis
# Modify df.slider to calculate a new column based on the condition
df.slider <- df.raw %>%
mutate(
slider_values_rescaled = strsplit(as.character(slider_values_rescaled), ","),
slider_time_rescaled = strsplit(as.character(slider_time), ",")) %>%
mutate(
slider_rescale_float = map(slider_values_rescaled, ~as.numeric(.x)),
slider_time_float = map(slider_time_rescaled, ~as.numeric(.x) / 1000)  # Convert from ms to seconds
) %>%
rowwise() %>%
mutate(include = ifelse(slider_values_rescaled[[1]][1] == 0, TRUE, FALSE)) %>%
ungroup()
# Create a long format data frame with both time and values
df_long <- df.slider %>%
filter(include) %>%
mutate(id = row_number()) %>%
unnest(c(slider_rescale_float, slider_time_float)) %>%
rename(slider_value = slider_rescale_float, slider_times = slider_time_float) %>%
group_by(id, speech_rate) %>%
mutate(index = row_number()) %>%
ungroup() %>%
select(-slider_values_rescaled, -slider_time_rescaled, -include)  # Remove unnecessary columns for plotting
# Set the x-axis limits and labels
p_slider_per_rate <- ggplot(df_long, aes(x = slider_times,
y = slider_value,
group = interaction(id),
color = as.factor(id))) +
geom_line(alpha = 0.5, size = 0.5) +
scale_x_continuous(name = "Time (seconds)") +
scale_y_continuous(name = "Slider Value") +
facet_wrap(~ speech_rate,
scales = 'free_x',
ncol = 1) +
scale_color_manual(values = colorRampPalette(brewer.pal(12, "Paired"))
(length(unique(df_long$id)))) +
theme(legend.position = "none") +
labs(title = 'Slider Values by Speech Rate',
caption = "Each line represents the comprehension trajectory at different speech rates.
As the speech rate increases, comprehension scores seem to get lower.")
# Histograms
p_flipped_histogram_per_rate <- ggplot(df_long, aes(x = slider_value)) +
geom_histogram(bins = 10, fill = "skyblue", color = "navy") +
coord_flip() +
facet_wrap(~speech_rate, scales = 'free_x', ncol = 1) +
labs(y = "Slider Value Range", x = "Count", title = "Number of Slider Values by Speech Rate") +
theme(legend.position = "none") +
scale_y_continuous(labels = scales::comma)
# combine
combined_plot <- p_slider_per_rate + p_flipped_histogram_per_rate +
plot_layout(ncol = 2, widths = c(2, 1)) +
theme(strip.background = element_rect(fill = "white", color = "black"))
print(combined_plot)
# Find trials that do not have a number right before .wav in the wav_file column
# (slowest condition)
attention_check_trials <- df.raw %>%
filter(!grepl("x\\.wav$", wav_file))
# Calculate the accuracy
accuracy_results <- attention_check_trials %>%
group_by(participant) %>%
summarise(
accuracy = mean(multiple_choice_accuracy == 1, na.rm = TRUE),
n = n()
) %>%
mutate(passed = if_else(accuracy >= 0.75, TRUE, FALSE))
# Print out participants who passed or failed
passed_participants <- accuracy_results %>%
filter(passed) %>%
pull(participant)
failed_participants <- accuracy_results %>%
filter(!passed) %>%
pull(participant)
if(length(passed_participants) > 0) {
cat(paste(passed_participants, collapse = ", "),
"passed the question accuracy criterion\n")
}
if(length(failed_participants) > 0) {
cat(paste(failed_participants, collapse = ", "),
"failed the question accuracy criterion\n")
}
# t-test
participants <- unique(df.raw$participant)
# Create a vector to store results
rating_criteria_results <- character(length(participants))
# Loop through each participant
for (i in seq_along(participants)) {
part_id <- participants[i]
# Subset data for the current participant for slowest and fastest speech rates
subset_data <- df.raw %>%
filter(participant == part_id & (speech_rate == 1 | speech_rate == 4)) %>%
select(likert_response, speech_rate)
# Perform a t-test comparing likert_response for speech_rate 1 vs 4
test_result <- t.test(likert_response ~ speech_rate, data = subset_data)
# Check if the difference is significant (p-value < 0.05)
if (test_result$p.value < 0.05) {
rating_criteria_results[i] <- paste(part_id, "passed the rating criterion")
} else {
rating_criteria_results[i] <- paste(part_id, "failed the rating criterion")
}
}
# Print the results
cat(rating_criteria_results, sep = "\n")
# If all rows under textbox.tex are empty
# Check if all 'textbox.text' entries are empty for each participant
summary_criterion <- df.raw %>%
group_by(participant) %>%
summarize(all_empty = all(textbox.text == "" | is.na(textbox.text))) %>%
ungroup()
# Print the results for participants where all textbox.text are empty
summary_criterion %>%
filter(all_empty) %>%
pull(participant) %>%
walk(~ cat(.x, "failed summary criterion\n"))
# Print the results for participants where not all textbox.text are empty
summary_criterion %>%
filter(!all_empty) %>%
pull(participant) %>%
walk(~ cat(.x, " passed summary criterion\n"))
# Calculate movement score with magnitude for each trial
df.raw <- df.raw %>%
rowwise() %>%
mutate(trial_movement_score_magnitude =
ifelse(slider_values[[1]][1] == 0, sum(abs(diff(unlist(slider_values)))), NA))
# Aggregate these scores for each participant
participant_movement_scores <- df.raw %>%
group_by(participant) %>%
summarise(movement_score_all = sum(trial_movement_score_magnitude, na.rm = TRUE))
# Add the movement scores back to the df
df.raw <- left_join(df.raw, participant_movement_scores, by = "participant")
# Calculate mean and standard deviation for movement scores, identify outliers
mean_movement_score <- mean(participant_movement_scores$movement_score_all, na.rm = TRUE)
sd_movement_score <- sd(participant_movement_scores$movement_score_all, na.rm = TRUE)
cutoff_upper <- mean_movement_score + 3.5 * sd_movement_score
cutoff_lower <- mean_movement_score - 3.5 * sd_movement_score
outliers <- participant_movement_scores %>%
filter(movement_score_all < cutoff_lower | movement_score_all > cutoff_upper) %>%
pull(participant)
# Print participant IDs for those outside the ±3.5 SD range
cat("Participants outside the ±3.5 SD range:", paste(outliers, collapse = ", "), "\n")
# Remove 'p_2' and 'p_6" from dataset
df.raw <- df.raw %>%
filter(participant != "p_2")
df.raw <- df.raw %>%
filter(participant != "p_6")
# check
count(df.raw, participant)
# Convert list to string for slider values
df.raw$slider_values <- sapply(df.raw$slider_values,
function(x) paste(x, collapse = ","))
output_file_path <- file.path(organized_data_path, "organized_data_9p.csv")
write.csv(df.raw, output_file_path, row.names = FALSE)
# remove p_2 and p_6 from df slider (df_slider has been created before to depics slider movements)
df.slider <- df.slider %>%
filter(participant != "p_2")
df.slider <- df.slider %>%
filter(participant != "p_6")
df_long <- df.slider %>%
mutate(id = row_number()) %>%
unnest(slider_rescale_float) %>%
rename(slider_value = slider_rescale_float) %>%
group_by(id, speech_rate) %>%
mutate(index = row_number()) %>%
ungroup() %>%
select(-slider_values_rescaled)
# Set the x-axis limits
p_slider_per_rate <- ggplot(df_long, aes(x = index,
y = slider_value,
group = interaction(id),
color = as.factor(id))) +
geom_line(alpha = 0.5, size = 0.5) +
scale_x_continuous(name = "Index of the Slider Value") +
scale_y_continuous(name = "Slider Value") +
facet_wrap(~ speech_rate,
scales = 'free_x',
ncol = 1) +
scale_color_manual(values = colorRampPalette(brewer.pal(12, "Paired"))
(length(unique(df_long$id)))) +
theme(legend.position = "none") +
labs(title = 'Slider Values by Speech Rate',
caption = "Each line represents the comprehension trajectory at different speech rates.
As the speech rate increases, comprehension scores seem to get lower."
)
# histograms
p_flipped_histogram_per_rate <- ggplot(df_long, aes(x = slider_value)) +
geom_histogram(bins = 10, fill = "skyblue", color = "navy") +
coord_flip() +
facet_wrap(~speech_rate, scales = 'free_x', ncol = 1) +
labs(y = "Slider Value Range", x = "Count", title = "Number of Slider Values by Speech Rate") +
theme(legend.position = "none") +
scale_y_continuous(labels = comma)
# combine
combined_plot <- p_slider_per_rate + p_flipped_histogram_per_rate +
plot_layout(ncol = 2, widths = c(2, 1)) +
theme(strip.background = element_rect(fill = "white", color = "black"))
print(combined_plot)
# Load libraries
library(stringr)
library(fs)
library(dplyr)
library(readxl)
library(readr)
library(simr)
library(lmerTest)
library(patchwork)
library(RColorBrewer)
library(cowplot)
library(ggplot2)
library(tidyr)
library(purrr)
library(scales)
library(gridExtra)
# set the default ggplot theme
theme_set(theme_classic())
# Set path and participant ids
# Change the folder path to final-project-irmak-ergin/data relative to your wd
folder_path <- '/Users/irmakergin/Desktop/2023_speech_rate/data_experiment'
# Define the list of participant IDs
participant_ids <- c("p_1","p_2","p_3","p_4","p_5", "p_6", "p_7", "p_8", "p_9")
organized_data_path<-"/Users/irmakergin/Desktop/2023_speech_rate/organized_data_experiment"
# upload data after semantic similarity calculations
df.data <- read.csv(file.path(organized_data_path, "df_semantic.csv"))
# Compute mean & median slider values for each trial with rescaled values
df.data <- df.data %>%
mutate(
slider_values_numeric = str_split(slider_values_rescaled, ",") %>%
map(~ as.numeric(.x))
)
# Mean and median slider values for each trial
df.data <- df.data %>%
rowwise() %>%
mutate(trial_mean_rescaled = mean(unlist(slider_values_numeric), na.rm = TRUE),
trial_median_rescaled = median(unlist(slider_values_numeric), na.rm = TRUE)) %>%
ungroup()
# post-hoc comprehension ratings
model_likert<- lmer(trial_median_rescaled ~
likert_response +
(1 | digit_span_score) +
(1 | digit_in_noise_score), data = df.data)
summary(model_likert)
# multiple choice questions
model_multiple_choice<- lmer(trial_median_rescaled ~
multiple_choice_accuracy +
(1 | digit_span_score) +
(1 | digit_in_noise_score), data = df.data)
summary(model_multiple_choice)
# Calculate the average multiple_choice_accuracy scores per speech_rate category
average_scores <- df.data %>%
group_by(speech_rate) %>%
summarise(average_accuracy = mean(multiple_choice_accuracy, na.rm = TRUE),
sd = sd(multiple_choice_accuracy, na.rm = TRUE),
n = n(),
se = sd / sqrt(n)  # standard error
)
# plot
ggplot(average_scores, aes(x = factor(speech_rate), y = average_accuracy)) +
geom_bar(stat = "identity", fill = "lightpink") +
labs(x = "Speech Rate",
y = "Average Multiple Choice Accuracy",
title = "Average Multiple Choice Accuracy by Speech Rate") +
geom_text(aes(label = round(average_accuracy, 2)), vjust = -1.6, size = 3.5)+
geom_errorbar(aes(ymin = average_accuracy - se, ymax = average_accuracy + se),
width = 0.25, position = position_dodge(width = 0.9))
# Summary- Glove (sum of maximum cosine values of summary words)
model_glove1<- lmer(trial_median_rescaled ~
glove_sum_max_similarity +
(1 | digit_span_score) +
(1 | digit_in_noise_score), data = df.data)
summary(model_glove1)
# Summary- Glove (average of maximum cosine values of segment text words)
model_glove2 <- lmer(trial_median_rescaled ~
glove_text_average_max_similarity +
(1 | digit_span_score) +
(1 | digit_in_noise_score), data = df.data)
summary(model_glove2)
# Summary- BERT (cosine values of summaries)
model_bert <- lmer(trial_median_rescaled ~
bert_cosine_similarity +
(1 | digit_span_score) +
(1 | digit_in_noise_score), data = df.data)
summary(model_bert)
# Visualize summary scores per speech rate
# glove- summaries
average_similarity <- df.data %>%
group_by(speech_rate) %>%
summarise(
average_similarity = mean( glove_sum_max_similarity, na.rm = TRUE),
sd = sd(glove_sum_max_similarity, na.rm = TRUE),
n = n(),
se = sd / sqrt(n)  # standard error
)
print(average_similarity[, c("speech_rate", "sd", "n", "se")])
# Plot
plot_glove_summary <-  ggplot(average_similarity, aes(x = factor(speech_rate),
y = average_similarity,
fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen1") +
labs(x = "Speech Rate", y = "Average Similarity",
title = "GloVe- Average of Sum of Maximum Cosine Values for Summary Words By Speech Rate") +
geom_text(aes(label = round(average_similarity, 2)),
vjust = -1.1, position = position_dodge(width = 0.9), size = 3) +
geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se),
width = 0.25, position = position_dodge(width = 0.9)) +
theme(plot.title = element_text(size = 13,hjust = 0.5)  # arange title
)
# Glove- over text (not over summary)
average_similarity <- df.data %>%
group_by(speech_rate) %>%
summarise(
average_similarity = mean( glove_text_average_max_similarity, na.rm = TRUE),
sd = sd( glove_text_average_max_similarity, na.rm = TRUE),
n = n(),
se = sd / sqrt(n)  # standard error
)
print(average_similarity[, c("speech_rate", "sd", "n", "se")])
# Plot
plot_glove_text <- ggplot(average_similarity, aes(x = factor(speech_rate),
y = average_similarity,
fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen2") +
labs(x = "Speech Rate", y = "Average Similarity",
title = "GloVe- Average of Maximum Cosine Values For Segment Text Words By Speech Rate") +
geom_text(aes(label = round(average_similarity, 2)),
vjust = -0.7, position = position_dodge(width = 0.9), size = 3) +
geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se),
width = 0.25, position = position_dodge(width = 0.9)) +
theme(plot.title = element_text(size = 13,hjust = 0.5)  # margins
)
# BERT with whole summary-whole text
average_similarity <- df.data %>%
group_by(speech_rate) %>%
summarise(
average_similarity = mean(bert_cosine_similarity, na.rm = TRUE),
sd = sd(bert_cosine_similarity, na.rm = TRUE),
n = n(),
se = sd / sqrt(n)  # standard error
)
print(average_similarity[, c("speech_rate", "sd", "n", "se")])
# Plot
plot_bert <- ggplot(average_similarity, aes(x = factor(speech_rate),
y = average_similarity,
fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge", fill = "darkseagreen3") +
labs(x = "Speech Rate", y = "Average Similarity",
title = "BERT Semantic Similarity by Speech Rate") +
geom_text(aes(label = round(average_similarity, 2)),
vjust = -1, position = position_dodge(width = 0.9), size = 3) +
geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se),
width = 0.25, position = position_dodge(width = 0.9)) +
theme(plot.title = element_text(size = 13,hjust = 0.5)  # margins
)
# Arrange plots in a single column grid
grid.arrange(plot_glove_summary, plot_glove_text, plot_bert, ncol = 1)
model_all_glove1 <- lmer(trial_median_rescaled ~
glove_sum_max_similarity +
likert_response +
multiple_choice_accuracy +
(1 | digit_span_score) +
(1 | digit_in_noise_score), data = df.data)
summary(model_all_glove1)
# Depict the likert scale and slider values for each speech rate. Their patterns should look quite similar based on the regression analysis
# Calculate the average  likert_response scores per speech_rate category
average_likert_scores <- df.data %>%
group_by(speech_rate) %>%
summarise(average_likert = mean(likert_response, na.rm = TRUE))
# Plotting the average likert scale across speech rates
p1 <- ggplot(average_likert_scores, aes(x = factor(speech_rate), y = average_likert)) +
geom_bar(stat = "identity", fill = "cornflowerblue") +
labs(x = "Speech Rate", y = "Post-hoc Rating Scores", title = "Average Post-hoc Ratings
per Speech Rate") +
geom_text(aes(label = round(average_likert, 2)), vjust = -0.5, size = 3.5)
# Calculate the average trial_median_rescaled for each speech_rate category
average_trial_median <- df.data %>%
group_by(speech_rate) %>%
summarise(average_trial_median = mean(trial_median_rescaled, na.rm = TRUE))
# Plot
p2 <- ggplot(average_trial_median, aes(x = factor(speech_rate), y = average_trial_median,
fill = as.factor(speech_rate))) +
geom_bar(stat = "identity", position = "dodge") +
scale_fill_brewer(palette = "Pastel1", name = "Speech Rate") +
labs(x = "Speech Rate", y = "Real-Time Comprehension Scores",
title = "  Average of the Median
Real-Time Comprehension Scores
per Speech Rate") +
geom_text(aes(label = round(average_trial_median, 2)), vjust = -0.3,
position = position_dodge(width = 0.9), size = 3) +
theme(legend.position = "none")
# Arrange the plots side by side
combined_plot <- p1 + p2 + plot_layout(ncol = 2)
print(combined_plot)
model_all_glove2 <- lmer(trial_median_rescaled ~
glove_text_average_max_similarity +
likert_response +
multiple_choice_accuracy +
(1 | digit_span_score) +
(1 | digit_in_noise_score), data = df.data)
summary(model_all_glove2)
model_all_bert <- lmer(trial_median_rescaled ~
bert_cosine_similarity +
likert_response +
multiple_choice_accuracy +
(1 | digit_span_score) +
(1 | digit_in_noise_score), data = df.data)
summary(model_all_bert)
model_rate <- lmer(speech_rate ~
trial_median_rescaled +
glove_sum_max_similarity +
likert_response +
multiple_choice_accuracy +
(1 | digit_span_score) +
(1 | digit_in_noise_score), data = df.data)
summary(model_rate)
plot<- df.data %>%
ggplot(aes(speech_rate, trial_median_rescaled)) + geom_point() + theme_bw() +
geom_smooth(method="lm", alpha=0.5, color='violet', size=1)
plot
plot<- df.data %>%
ggplot(aes(trial_median_rescaled,likert_response )) + geom_point() + theme_bw() +
geom_smooth(method="lm", alpha=0.5, color='violet', size=1)
plot
plot<- df.data %>%
ggplot(aes(likert_response,trial_median_rescaled )) + geom_point() + theme_bw() +
geom_smooth(method="lm", alpha=0.5, color='violet', size=1)
plot
