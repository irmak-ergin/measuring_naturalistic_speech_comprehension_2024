# Data Wrangling

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
# Load libraries
library(stringr)
library(fs)
library(dplyr)
library(readxl)
library(readr)
library(simr)
library(lmerTest)
library(patchwork)
library(RColorBrewer)
library(cowplot)
library(ggplot2)
library(tidyr) 
library(purrr)
library(scales)
library(gridExtra)
library(PerformanceAnalytics)
library(corrplot)
library(Hmisc)
library(GGally)
library(visreg)
library(broom.mixed)
library(sjPlot)
library(gridExtra)
library(grid) 
library(rempsyc)
library(flextable)
library(officer)

# set the default ggplot theme 
theme_set(theme_classic())
```

```{r}
# Set path and participant ids

# Change the folder path to final-project-irmak-ergin/data relative to your wd
folder_path <- '/Users/irmakergin/Desktop/data_all/data_experiment'

# Define the list of participant IDs
participant_ids <- c("p_1","p_2","p_3","p_4","p_5", "p_6", "p_7", "p_8", "p_9","p_10", "p_11", "p_12", "p_13", "p_14")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Organize the data csv files

# Get a list of all files in the folder
files <- dir(folder_path, full.names = TRUE)

# Set a path to save the organized files
organized_data_path<-"/Users/irmakergin/Desktop/data_all/organized_data_experiment"


# Find files by participant ID 
for(part_id in participant_ids) {

  # Match the slider files that involve the participant ID
  slider_pattern <- paste0("slider_", part_id, ".*\\.csv$")
  
  # Match data (csv) files that start with the participant ID
  additional_csv_pattern <- paste0("^", part_id, "_.*\\.csv$")
  
  # Find slider files that match the pattern
  slider_matched_files <- files[str_detect(basename(files), slider_pattern)]
  
  # Find data CSV files that match the pattern
  additional_csv_matched_files <- files[str_detect(basename(files), 
                                                   additional_csv_pattern)]
   
  # Create a directory for the current participant on the organized data path
  participant_folder <- file.path(organized_data_path, part_id)
  if(!dir.exists(participant_folder)) {
    dir.create(participant_folder, recursive = TRUE, showWarnings = TRUE)
  }
  
  # Create a subdirectory for slider files within the participant folder
  slider_folder <- file.path(participant_folder, paste0("slider_", part_id))
  if(!dir.exists(slider_folder)) {
    dir.create(slider_folder, recursive = TRUE, showWarnings = TRUE)
  }
  
  # Move slider matched files to the slider subfolder
  if(length(slider_matched_files) > 0) {
    for(file in slider_matched_files) {
      file_copy(file, file.path(slider_folder, basename(file)), overwrite = TRUE)
    }
  }
  
  # Move additional CSV matched files (experiment data) to the main participant folder 
  #and rename the participant folders
  if(length(additional_csv_matched_files) > 0) {
  # loop to rename the file during the copy process
  for(file in additional_csv_matched_files) {
    # New filename with '_raw_data'
    new_name <- paste0(part_id, "_raw_data.csv")
    # Copy and rename the file to the new name in the participant folder
    file_copy(file, file.path(participant_folder, new_name), overwrite = TRUE)
  }
}
  # check
  if(length(slider_matched_files) > 0 || length(additional_csv_matched_files) > 0) {
    cat("Files for participant ID '", part_id, 
        "' have been organized successfully. Slider files in: ", slider_folder, 
        ", other CSVs in: ", participant_folder, "\n")
  } else {
    cat("No files found for participant ID '", part_id, "'.\n")
  }
}

# # Put excel files (of digit in noise and digit span) in the organized data folder 
# 
# source_folder<-"/Users/irmakergin/Desktop/2023_speech_rate/data_experiment"
# destination_folder<-"//Users/irmakergin/Desktop/2023_speech_rate/organized_data_experiment"
# 
# # List all Excel files in the source folder
# excel_files <- dir_ls(path = source_folder, glob = "*.xlsx")
# 
# # Copy each Excel file to the destination folder- commented out to re-run
# for (file_path in excel_files) {
# dest_file_path <- file.path(destination_folder)
# file_copy(file_path, dest_file_path)
# # Check
# cat("Copied:", file_path, "to", dest_file_path, "\n")
# }

# Loop through each participant ID
for (part_id in participant_ids) {
  # Define the path to the participant's raw data CSV
  raw_data_path <- file.path(organized_data_path, 
                             part_id, 
                             paste0(part_id, "_raw_data.csv"))
# Read the CSV for each participant (if it exists)
if (file.exists(raw_data_path)) {
  df.raw <- read_csv(raw_data_path) %>%
    filter(!wav_file %in% c("Someday_trial_condition.wav", 
                            "Someday_trial_condition-5x.wav")) %>%
    mutate(index = row_number()) %>%
    select(index, participant, wav_file, old_name, segment_text, question, 
           C1, C2, C3, C4, C_correct, duration,
           A.numClicks, B.numClicks, C.numClicks, D.numClicks, slider.response, 
           summary, `textbox.text`) %>%
    group_by(wav_file) %>%
    summarise(
      index = first(index), 
      participant = first(participant), 
      old_name = first(old_name),
      segment_text = first(segment_text),
      question = first(question),
      C1 = first(C1),
      C2 = first(C2),
      C3 = first(C3),
      C4 = first(C4),
      C_correct = first(C_correct),
      duration = first(duration),
      summary = first(summary),
      textbox.text = first(textbox.text),
      A.numClicks = max(A.numClicks, na.rm = TRUE),
      B.numClicks = max(B.numClicks, na.rm = TRUE),
      C.numClicks = max(C.numClicks, na.rm = TRUE),
      D.numClicks = max(D.numClicks, na.rm = TRUE),
      likert_response = max(slider.response, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    arrange(index) %>%
    mutate(index = row_number()) 
  
  # Fill NA values in participant_id column with part_id for participant p_3
    if (part_id == "p_3") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
    if (part_id == "p_1") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
    if (part_id == "p_2") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
    if (part_id == "p_14") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
    if (part_id == "p_13") {
      df.raw <- df.raw %>%
        mutate(participant = replace_na(participant, part_id))
    }
   # Remove the extra last row from df.raw
    df.raw <- head(df.raw, n = -1)
   
  #Assign processed data of each participant to a new df
  assign(paste0("df.raw.merged_", part_id), df.raw, envir = .GlobalEnv)
  }
}
# Sanity check
# how many rows do we have- should be 125
nrow(df.raw.merged_p_2)
```
```{r}
# p_14 raw 83 is extra ##DOUBLE CHECK
df.raw.merged_p_14 <- df.raw.merged_p_14[-83, ]
df.raw.merged_p_14$index <- 1:nrow(df.raw.merged_p_14)
```

```{r}
# Merge all participant dfs into one df

# Create an empty data frame to store merged data
df.raw <- data.frame()

# Loop through each participant ID and merge their df
for (part_id in participant_ids) {
  # Construct the name of the data frame variable
  df_name <- paste0("df.raw.merged_", part_id)
  
  # Check if the data frame exists in the global environment
  if (exists(df_name, envir = .GlobalEnv)) {
    # Get the data frame from its name
    df_participant <- get(df_name, envir = .GlobalEnv)
    
    # Merge the data frame with the main merged data frame
    df.raw <- rbind(df.raw, df_participant)
  }
}
```

```{r}
# Add accuracy columns for multiple choice questions

# Calculate the multiple choice accuracy

df.raw<- df.raw %>%
  mutate(
    multiple_choice_accuracy = apply(., 1, function(row) {
      # Extract the first character from C_correct - it is the correct letter to click on
      correct_answer <- substr(row[["C_correct"]], 1, 1)
      
      # Determine which of the click columns matches the correct answer
      clicked_column <- ifelse(row[["A.numClicks"]] == 1, "A",
                               ifelse(row[["B.numClicks"]] == 1, "B",
                                      ifelse(row[["C.numClicks"]] == 1, "C",
                                             ifelse(row[["D.numClicks"]] == 1, "D", NA))))
      
      # Return 1 if the clicked column matches the correct answer, 0 otherwise
      if (!is.na(clicked_column) && clicked_column == correct_answer) {
        return(1)
      } else {
        return(0)
      }
    })
  )

# Change the column the order
df.raw<- df.raw[c("index", "participant", "wav_file", "old_name", 
                                 "duration","segment_text", 
                                 "question", "C1", "C2", "C3", "C4", "C_correct", 
                                 "A.numClicks", "B.numClicks", "C.numClicks",
                                 "D.numClicks", "multiple_choice_accuracy", 
                                 "likert_response", "summary","textbox.text")]
```

```{r}
# Extract the speech rate information from the trial name and create a speech_rate column
df.raw<- df.raw %>%
  mutate(speech_rate = case_when(
    str_detect(wav_file, "2x") ~ 2,
    str_detect(wav_file, "3x") ~ 3,
    str_detect(wav_file, "4x") ~ 4,
    str_detect(wav_file, "5x") ~ 5,
    TRUE ~ 1  # not sped up if there are no numbers + x in the name 
  ))
```

```{r}
# Add the Digit Span (working memory) scores of each participant

# Create an empty digit_span_score column
df.raw$digit_span_score <- NA

for (part_id in unique(df.raw$participant)) {
  # Construct the filename based on the participant ID
  filename <- paste0(organized_data_path, "/digit_span_", part_id, ".xlsx")
  
  # Check if the file exists
  if (file.exists(filename)) {
    # Correctly read the file using read_excel
    digit_span_data <- read_excel(filename)
    
    # The score is located in the first row of the 'score' column
    score <- digit_span_data$score[1]
      
    df.raw<- df.raw%>%
      mutate(digit_span_score = ifelse(participant == part_id, score, digit_span_score))
  } else {
    cat("File not found for participant", part_id, "\n")
  }
}
```

```{r}
# Add the hearWHO (digit-in-noise task for hearing in noise) scores of each participants

hearwho_pilot_data <- read_excel(paste0(organized_data_path,
                                        "/hearwho_experiment.xlsx"))

# Excel file has columns named 'participant' and 'score'
# Merge the scores into df.raw based on participant ID
df.raw <- merge(df.raw, hearwho_pilot_data[, c("participant", "score")], 
                       by.x = "participant", by.y = "participant", all.x = TRUE)

# Rename the 'score' column to 'digit_in_noise_score'
names(df.raw)[names(df.raw) == "score"] <- "digit_in_noise_score"
```

```{r}
# check
head(df.raw)
```
```{r}
# Add slider values and time points

# Check if the 'slider_values' column exists, if not, create it
if (!"slider_values" %in% names(df.raw)) {
  df.raw$slider_values <- NA
}

# Check if the 'slider_time' column exists, if not, create it
if (!"slider_time" %in% names(df.raw)) {
  df.raw$slider_time <- NA
}

# Define a function to extract and return slider values and time as strings
get_slider_values_and_time <- function(wav_file_name, part_id) {
  # Extract the trial number from wav_file name
  matches <- regmatches(wav_file_name, regexec("Someday_([0-9]+)", wav_file_name))
  if (length(matches[[1]]) < 2) { # If no match or match does not have a capture group
    return(list(values = NA, time = NA))
  }
  trial_number <- matches[[1]][2]

  # Construct the path to the slider file
  slider_file_path <- sprintf("%s/%s/slider_%s/slider_%s_001_Someday_%s_condition*.csv", 
                              organized_data_path, 
                              part_id, part_id, part_id, 
                              trial_number)

  # Find slider files matching the pattern
  slider_files <- Sys.glob(slider_file_path)
  
  if (length(slider_files) == 0) {
    return(list(values = NA, time = NA)) # No matching file found
  }
  
  slider_data <- read.csv(slider_files[1], skip = 1)

  # Prepare output for values
  slider_values_str <- if ("value" %in% colnames(slider_data)) {
    paste(slider_data$value, collapse = ",")
  } else {
    NA
  }

  # Prepare output for time, rounding to two decimals
  slider_time_str <- if ("time" %in% colnames(slider_data)) {
    rounded_times <- round(slider_data$time, 2)  # Round the time values to two decimal places
    paste(rounded_times, collapse = ",")
  } else {
    NA
  }

  return(list(values = slider_values_str, time = slider_time_str))
}

# Apply the function to each row of df.raw and extract results into separate columns
slider_info <- mapply(get_slider_values_and_time, 
                      df.raw$wav_file, 
                      df.raw$participant, 
                      SIMPLIFY = FALSE)  # Ensure output is a list to handle multiple return values

# Assign values and time from the list output to respective columns
df.raw$slider_values <- sapply(slider_info, '[[', "values")
df.raw$slider_time <- sapply(slider_info, '[[', "time")

```

```{r}
# Re-scale slider values to be between 0-1

# Convert slider_values from string to numeric lists 
df.raw$slider_values <- strsplit(as.character(df.raw$slider_values), 
                                        ",\\s*")
df.raw$slider_values <- lapply(df.raw$slider_values, 
                                      function(x) as.numeric(x))

rescale_values <- function(values) {
  sapply(values, function(x) x / 255)
}

# Apply the rescaling function to each row's slider_values
df.raw$slider_values_rescaled <- lapply(df.raw$slider_values, 
                                               rescale_values)
```

```{r}
# Sanity check: create a column showing number of slider values
df.raw <- df.raw %>%
  mutate(slider_values_number = lengths(slider_values_rescaled))
```

```{r}
# Depict slider movements

df.raw$slider_values_rescaled <- sapply(df.raw$slider_values_rescaled,
                                               function(x) paste(x, collapse = ","))

df.slider <- df.raw%>%
  mutate(slider_values_rescaled = strsplit(slider_values_rescaled, ",")) %>%
  mutate(slider_rescale_float = map(slider_values_rescaled, ~as.numeric(.x)))

df_long <- df.slider %>%
  mutate(id = row_number()) %>%
  unnest(slider_rescale_float) %>%
  rename(slider_value = slider_rescale_float) %>%
  group_by(id, speech_rate) %>%
  mutate(index = row_number()) %>%
  ungroup() %>%
  select(-slider_values_rescaled)

# Set the x-axis limits
p_slider_per_rate <- ggplot(df_long, aes(x = index,
                                         y = slider_value,
                                         group = interaction(id),
                                         color = as.factor(id))) +
  geom_line(alpha = 0.5, size = 0.5) +
  scale_x_continuous(name = "Index of the Slider Value") +
  scale_y_continuous(name = "Slider Value") +
  facet_wrap(~ speech_rate,
             scales = 'free_x',
             ncol = 1) +
  scale_color_manual(values = colorRampPalette(brewer.pal(12, "Paired"))
                     (length(unique(df_long$id)))) +
  theme(legend.position = "none") +
  labs(title = 'Slider Values by Speech Rate',
      caption = "Each line represents the comprehension trajectory at different speech rates.
      As the speech rate increases, comprehension scores seem to get lower."
)

# histograms

p_flipped_histogram_per_rate <- ggplot(df_long, aes(x = slider_value)) +
  geom_histogram(bins = 10, fill = "skyblue", color = "navy") +
  coord_flip() +
  facet_wrap(~speech_rate, scales = 'free_x', ncol = 1) +
  labs(y = "Slider Value Range", x = "Count", title = "Number of Slider Values by Speech Rate") +
  theme(legend.position = "none") +
  scale_y_continuous(labels = comma)

# combine
combined_plot <- p_slider_per_rate + p_flipped_histogram_per_rate +
  plot_layout(ncol = 2, widths = c(2, 1)) +
  theme(strip.background = element_rect(fill = "white", color = "black"))



print(combined_plot)

```

```{r}
# Sanity check for slider values 

rows_not_fitting_criteria_selected_columns <- df.raw %>%
  filter(
    (speech_rate == 1 & (slider_values_number < 6000 | slider_values_number > 8000)) |
    (speech_rate == 2 & (slider_values_number < 3000 | slider_values_number > 4000)) |
    (speech_rate == 3 & (slider_values_number < 2500 | slider_values_number > 3000)) |
    (speech_rate == 4 & (slider_values_number < 1500 | slider_values_number > 2000)) |
    (speech_rate == 5 & (slider_values_number < 1200 | slider_values_number > 1550)) 
  ) %>%
  select(participant, wav_file, index, slider_values_number)

# To print the resulting selected columns for rows that do not fit the criteria
print(rows_not_fitting_criteria_selected_columns)

# manually inspect those to see if they are just sampled slightly longer/ shorter or if the match is wrong
```
check slider files!!!

```{r}
# Only trials with the slider starting from point, with time one the x axis

# Modify df.slider to calculate a new column based on the condition
df.slider <- df.raw %>%
  mutate(
    slider_values_rescaled = strsplit(as.character(slider_values_rescaled), ","),
    slider_time_rescaled = strsplit(as.character(slider_time), ",")) %>%
  mutate(
    slider_rescale_float = map(slider_values_rescaled, ~as.numeric(.x)),
    slider_time_float = map(slider_time_rescaled, ~as.numeric(.x) / 1000)  # Convert from ms to seconds
  ) %>%
  rowwise() %>%
  mutate(include = ifelse(slider_values_rescaled[[1]][1] == 0, TRUE, FALSE)) %>%
  ungroup()

# Create a long format data frame with both time and values
df_long <- df.slider %>%
  filter(include) %>%
  mutate(id = row_number()) %>%
  unnest(c(slider_rescale_float, slider_time_float)) %>%
  rename(slider_value = slider_rescale_float, slider_times = slider_time_float) %>%
  group_by(id, speech_rate) %>%
  mutate(index = row_number()) %>%
  ungroup() %>%
  select(-slider_values_rescaled, -slider_time_rescaled, -include)  # Remove unnecessary columns for plotting

# Set the x-axis limits and labels
p_slider_per_rate <- ggplot(df_long, aes(x = slider_times,
                                         y = slider_value,
                                         group = interaction(id),
                                         color = as.factor(id))) +
  geom_line(alpha = 0.5, size = 0.5) +
  scale_x_continuous(name = "Time (seconds)") +
  scale_y_continuous(name = "Slider Value") +
  facet_wrap(~ speech_rate,
             scales = 'free_x',
             ncol = 1) +
  scale_color_manual(values = colorRampPalette(brewer.pal(12, "Paired"))
                     (length(unique(df_long$id)))) +
  theme(legend.position = "none") +
  labs(title = 'Slider Values by Speech Rate',
       caption = "Each line represents the comprehension trajectory at different speech rates.
       As the speech rate increases, comprehension scores seem to get lower.")

# Histograms
p_flipped_histogram_per_rate <- ggplot(df_long, aes(x = slider_value)) +
  geom_histogram(bins = 10, fill = "skyblue", color = "navy") +
  coord_flip() +
  facet_wrap(~speech_rate, scales = 'free_x', ncol = 1) +
  labs(y = "Slider Value Range", x = "Count", title = "Number of Slider Values by Speech Rate") +
  theme(legend.position = "none") +
  scale_y_continuous(labels = scales::comma)

# combine
combined_plot <- p_slider_per_rate + p_flipped_histogram_per_rate +
  plot_layout(ncol = 2, widths = c(2, 1)) +
  theme(strip.background = element_rect(fill = "white", color = "black"))
print(combined_plot)

```

## Calculations for the data exclusion criteria
For the post-hoc comprehension measures, to ensure that we are validating the novel measure against actual comprehension, following exclusion criteria will be applied: 

1. Absolute threshold for comprehension: 75% correctness on the multiple choice questions for the slowest (easiest to comprehend) speech rate.

```{r}
# Find trials that do not have a number right before .wav in the wav_file column 
# (slowest condition)
attention_check_trials <- df.raw %>%
  filter(!grepl("x\\.wav$", wav_file))

# Calculate the accuracy
accuracy_results <- attention_check_trials %>%
  group_by(participant) %>%
  summarise(
    accuracy = mean(multiple_choice_accuracy == 1, na.rm = TRUE),
    n = n()
  ) %>%
  mutate(passed = if_else(accuracy >= 0.75, TRUE, FALSE))

# Print out participants who passed or failed
passed_participants <- accuracy_results %>%
  filter(passed) %>%
  pull(participant)

failed_participants <- accuracy_results %>%
  filter(!passed) %>%
  pull(participant)

if(length(passed_participants) > 0) {
  cat(paste(passed_participants, collapse = ", "), 
      "passed the question accuracy criterion\n")
}

if(length(failed_participants) > 0) {
  cat(paste(failed_participants, collapse = ", "), 
      "failed the question accuracy criterion\n")
}
```

2. Ratings on the 10-point (post-hoc likert) scale should be significantly different for the slowest and fastest speech rates.

```{r}
# t-test 
participants <- unique(df.raw$participant)

# Create a vector to store results
rating_criteria_results <- character(length(participants))

# Loop through each participant
for (i in seq_along(participants)) {
  part_id <- participants[i]
  
  # Subset data for the current participant for slowest and fastest speech rates
  subset_data <- df.raw %>%
    filter(participant == part_id & (speech_rate == 1 | speech_rate == 4)) %>%
    select(likert_response, speech_rate)
  
  # Perform a t-test comparing likert_response for speech_rate 1 vs 4
  test_result <- t.test(likert_response ~ speech_rate, data = subset_data)
  
  # Check if the difference is significant (p-value < 0.05)
  if (test_result$p.value < 0.05) {
    rating_criteria_results[i] <- paste(part_id, "passed the rating criterion")
  } else {
    rating_criteria_results[i] <- paste(part_id, "failed the rating criterion")
  }
}

# Print the results
cat(rating_criteria_results, sep = "\n")
```
3. Participants shouldn't skip all the summaries

```{r}
# If all rows under textbox.tex are empty

# Check if all 'textbox.text' entries are empty for each participant
summary_criterion <- df.raw %>%
  group_by(participant) %>%
  summarize(all_empty = all(textbox.text == "" | is.na(textbox.text))) %>%
  ungroup()

# Print the results for participants where all textbox.text are empty
summary_criterion %>%
  filter(all_empty) %>%
  pull(participant) %>%
  walk(~ cat(.x, "failed summary criterion\n"))

# Print the results for participants where not all textbox.text are empty
summary_criterion %>%
  filter(!all_empty) %>%
  pull(participant) %>%
  walk(~ cat(.x, " passed summary criterion\n"))
```
For the physical slider to ensure that participants actually used it actively: Distribution of the amount of slider movements across trials and participants- If a participant's slider movements exceeds ±3.5 standard deviations from the mean, we will remove those participants' data.

```{r}
# Calculate movement score with magnitude for each trial
df.raw <- df.raw %>%
  rowwise() %>%
  mutate(trial_movement_score_magnitude = 
           ifelse(slider_values[[1]][1] == 0, sum(abs(diff(unlist(slider_values)))), NA))

# Aggregate these scores for each participant
participant_movement_scores <- df.raw %>%
  group_by(participant) %>%
  summarise(movement_score_all = sum(trial_movement_score_magnitude, na.rm = TRUE))

# Add the movement scores back to the df
df.raw <- left_join(df.raw, participant_movement_scores, by = "participant")

# Calculate mean and standard deviation for movement scores, identify outliers
mean_movement_score <- mean(participant_movement_scores$movement_score_all, na.rm = TRUE)
sd_movement_score <- sd(participant_movement_scores$movement_score_all, na.rm = TRUE)

cutoff_upper <- mean_movement_score + 3.5 * sd_movement_score
cutoff_lower <- mean_movement_score - 3.5 * sd_movement_score

outliers <- participant_movement_scores %>%
  filter(movement_score_all < cutoff_lower | movement_score_all > cutoff_upper) %>%
  pull(participant)

# Print participant IDs for those outside the ±3.5 SD range
cat("Participants outside the ±3.5 SD range:", paste(outliers, collapse = ", "), "\n")
```
Remove the participants that failed the criteria. 
p_12, p_2, p_6 failed the question accuracy criterion.

```{r}
# Remove 'p_2' and 'p_6" from dataset
df.raw <- df.raw %>%
  filter(participant != "p_2")
df.raw <- df.raw %>%
  filter(participant != "p_6")
df.raw <- df.raw %>%
  filter(participant != "p_2")
df.raw <- df.raw %>%
  filter(participant != "p_12")
# check
count(df.raw, participant)
```

```{r}
# Save df
# Convert list to string for slider values
df.raw$slider_values <- sapply(df.raw$slider_values, 
                                      function(x) paste(x, collapse = ","))

output_file_path <- file.path(organized_data_path, "organized_data_14p.csv")
write.csv(df.raw, output_file_path, row.names = FALSE)

```

# Analysis

```{r}
# upload data after semantic similarity calculations

df.data <- read.csv(file.path(organized_data_path, "df_semantic.csv"))
```

```{r}
# Calculate median trial movements for each trial

df.data <- df.data %>%
  mutate(
    slider_values_numeric = str_split(slider_values_rescaled, ",") %>%
      map(~ as.numeric(.x))
  )

# Mean and median slider values for each trial
df.data <- df.data %>%
  rowwise() %>%
  mutate(trial_mean_rescaled = mean(unlist(slider_values_numeric), na.rm = TRUE),
         trial_median_rescaled = median(unlist(slider_values_numeric), na.rm = TRUE)) %>%
  ungroup()
```


## Semantic Similarity
Which semantic similarity analysis explains comprehension better?

### Visualize summary scores per speech rate

```{r figure-size, fig.width=9, fig.height=5}
#figure-size, fig.width=10, fig.height=15
# Visualize summary scores per speech rate

# GloVe - summaries
average_similarity_summary <- df.data %>%
  group_by(speech_rate) %>%
  summarise(
    average_similarity = mean(glove_sum_max_similarity, na.rm = TRUE),
    sd = sd(glove_sum_max_similarity, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n)  # standard error
  )

print(average_similarity_summary[, c("speech_rate", "sd", "n", "se")])

# Plot GloVe summaries
plot_glove_summary <- ggplot(average_similarity_summary, aes(x = factor(speech_rate), 
                               y = average_similarity, 
                               fill = as.factor(speech_rate))) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7, fill = "darkseagreen1") +
  labs(x = "Speech Rate", y = "Average Semantic Similarity", 
       title = "GloVe- Heard Segment") +
  geom_text(aes(label = round(average_similarity, 2)), 
            vjust = -1.7, position = position_dodge(width = 0.7), size = 3) +
  geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se), 
                width = 0.25, position = position_dodge(width = 0.7)) + 
  theme(plot.title = element_text(size = 13, hjust = 0.5))  # arrange title

# GloVe - over text
average_similarity_text <- df.data %>%
  group_by(speech_rate) %>%
  summarise(
    average_similarity = mean(glove_text_average_max_similarity, na.rm = TRUE),
    sd = sd(glove_text_average_max_similarity, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n)  # standard error
  )

print(average_similarity_text[, c("speech_rate", "sd", "n", "se")])

# Plot GloVe over text
plot_glove_text <- ggplot(average_similarity_text, aes(x = factor(speech_rate), 
                               y = average_similarity, 
                               fill = as.factor(speech_rate))) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7, fill = "darkseagreen2") +
  labs(x = "Speech Rate", y = "Average Semantic Similarity", 
       title = "GloVe- Written Summary") +
  geom_text(aes(label = round(average_similarity, 2)), 
            vjust = -1.2, position = position_dodge(width = 0.7), size = 3) +
  geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se), 
                width = 0.25, position = position_dodge(width = 0.7)) +
  theme(plot.title = element_text(size = 13, hjust = 0.5))  # margins

# BERT with whole summary-whole text
average_similarity_bert <- df.data %>%
  group_by(speech_rate) %>%
  summarise(
    average_similarity = mean(bert_cosine_similarity, na.rm = TRUE),
    sd = sd(bert_cosine_similarity, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n)  # standard error
  )

print(average_similarity_bert[, c("speech_rate", "sd", "n", "se")])

# Plot BERT
plot_bert <- ggplot(average_similarity_bert, aes(x = factor(speech_rate), 
                               y = average_similarity, 
                               fill = as.factor(speech_rate))) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7, fill = "darkseagreen3") +
  labs(x = "Speech Rate", y = "Average Semantic Similarity", 
       title = "BERT") +
  geom_text(aes(label = round(average_similarity, 2)), 
            vjust = -1.5, position = position_dodge(width = 0.7), size = 3) +
  geom_errorbar(aes(ymin = average_similarity - se, ymax = average_similarity + se), 
                width = 0.25, position = position_dodge(width = 0.7)) +
  theme(plot.title = element_text(size = 13, hjust = 0.5))  # margins

# Arrange plots in a single row grid
grid.arrange(plot_glove_summary, plot_glove_text, plot_bert, ncol = 3)

```
glove- written summary: Average of Maximum Cosine Values For Segment Text Words By Speech Rate
glove- heard segment: Average of Maximum Cosine Values For Segment Text Words By Speech Rate

### Correlation
Do 3 different semantic similarity measures correlate with each other?
```{r}
# Subset the relevant columns
semantic_columns <- df.data[, c("bert_cosine_similarity", "glove_text_average_max_similarity", "glove_sum_max_similarity")]

# Calculate the correlation matrix
correlation_matrix <- cor(semantic_columns, use = "complete.obs")

# Print 
print(correlation_matrix)


```

```{r}
# test if any of these correlations are statistically significant

# Correlation test between bert_cosine_similarity and glove_text_average_max_similarity
test1 <- cor.test(df.data$bert_cosine_similarity, df.data$glove_text_average_max_similarity)

# Correlation test between bert_cosine_similarity and glove_sum_max_similarity
test2 <- cor.test(df.data$bert_cosine_similarity, df.data$glove_sum_max_similarity)

# Correlation test between glove_text_average_max_similarity and glove_sum_max_similarity
test3 <- cor.test(df.data$glove_text_average_max_similarity, df.data$glove_sum_max_similarity)

print(test1)
print(test2)
print(test3)

```
A Pearson correlation analysis was conducted to examine the relationship between three measures: BERT cosine similarity, GloVe text average max similarity, and GloVe sum max similarity. The correlation between BERT cosine similarity and GloVe text average max similarity was positive and moderate (r= 0.516, t(742)=16.404, p<.001.). The correlation between BERT cosine similarity and GloVe sum max similarity was positive and moderate (r=0.585,t(742)=19.647, p<.001). This moderate positive correlation indicates that higher BERT cosine similarity scores are associated with higher GloVe sum max similarity scores.F inally, the strongest correlation observed between the two GloVe-based measures (r=0.777, t(742)=33.577, p<.001). 

### Model comparison
Semantic similarity as a fixed effect to predict liker scale responses
```{r}
# Glove 1- likert response
model_g1<- lmer(likert_response ~ 
                glove_sum_max_similarity + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_g1)
```
```{r}
# glove2 -likert response
model_g2<- lmer(likert_response ~ 
                glove_text_average_max_similarity + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_g2)
```
```{r}
# bert- likert
model_b<- lmer(likert_response ~ 
                 bert_cosine_similarity + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_b)
```
Now compare models and see which semantic similarity analysis explains comprehension better
```{r}
model_compare <- anova(model_g1, model_g2, model_b)
model_compare
```
```{r}
# make a table close to apa style 
table_sem <- nice_table(model_compare)

#save as docx
output_path <- "/Users/irmakergin/Desktop/data_all/figures/model_comparison_table.docx"

read_docx() %>%
  body_add_flextable(value = table_sem) %>%
  print(target = output_path)
```


By running a Linear Mixed Effects Regression analysis we saw how different semantic similarity calculations predict comprehension (post-hoc rating responses). ANOVA- Semantic similarity score obtained by using GLoVe to calcaulte summed  maximum cosine value of each summary word had the lowest AIC and BIC values and therefore used as the semantic similarity scores in the rest of the analysis. 

The results suggest that Model G1, which uses GloVe sum max similarity, provides the best fit to the data based on both AIC and BIC criteria. Thus, GloVe sum max similarity is the preferred predictor for the likert response in this context.(The lowest AIC (3309.5) and BIC (3332.6) values). 
Therefore, I will use/ report the GLoVe- summary words' max cosine values summed for the rest of the analyses.

## Individual fixed effects

#### Post-hoc Comprehension Ratings
```{r}
# post-hoc comprehension ratings
model_likert<- lmer(trial_median_rescaled ~ 
                likert_response + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_likert)
```

A linear mixed model was fitted to predict the real-time comprehension scores from 10-point scale ratings, while accounting for random effects of working memory and hearing-in-noise scores. 

The model revealed a significant positive effect of post-hoc ratings on real-time comprehension scores (β = 0.10, SE = 0.00, t(1372) = 75.32, p < .001). The intercept was also significant (β = -0.12, SE = 0.02, t(6.24) = -7.19, p < .001).

The random effects show minimal variability attributable to working memory (variance = 0.00, SD = 0.02) and hearing in noise (variance = 0.00, SD = 0.02), with the residual variance being 0.03 (SD = 0.18).

#### Multiple Choice Quesitons
```{r}
# multiple choice questions
model_multiple_choice<- lmer(trial_median_rescaled ~ 
                multiple_choice_accuracy + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_multiple_choice)
```
A linear mixed model was fitted to examine the effect of multiple choice accuracy on real-time comprehension, while accounting for random effects of working memory and hearing in noise. 

The model revealed a significant positive effect of multiple choice accuracy on real-time comprehension (β = 0.34, SE = 0.02, t(1370) = 15.20, p < .001).  The intercept was also significant (β = 0.29, SE = 0.04, t(4.26) = 8.30, p = .001).

The random effects show minimal variability attributable to working memory (variance = 0.00, SD = 0.04) and hearing in noise (variance = 0.00, SD = 0.05), with the residual variance being 0.15 (SD = 0.38).


```{r figure-size, fig.width=4, fig.height=4 }
# Calculate the average multiple_choice_accuracy scores per speech_rate category
average_scores <- df.data %>%
  group_by(speech_rate) %>%
  summarise(average_accuracy = mean(multiple_choice_accuracy, na.rm = TRUE),
    sd = sd(multiple_choice_accuracy, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n)  # standard error
  )

# plot
ggplot(average_scores, aes(x = factor(speech_rate), y = average_accuracy)) +
  geom_bar(stat = "identity", width = 0.7, fill = "lightpink") +   

  labs(x = "Speech Rate", 
       y = "Average Multiple Choice Accuracy", 
       #title = "Average Multiple Choice Accuracy by Speech Rate"
       ) +
  geom_text(aes(label = round(average_accuracy, 2)), vjust = -1.3, size = 3.5)+
  geom_errorbar(aes(ymin = average_accuracy - se, ymax = average_accuracy + se), 
                width = 0.25, position = position_dodge(width = 0.5)) 
```

#### Summary

```{r}
# Summary- Glove (sum of maximum cosine values of summary words)
model_glove1<- lmer(trial_median_rescaled ~ 
                glove_sum_max_similarity + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_glove1)
```

A linear mixed model was fitted to predict real-time comprehension scores from summary semantic similarity scores, while accounting for random effects of working memory and hearing in noise.

The model revealed a significant positive effect ofsummary scores on real-time comprehension (β = 0.03, SE = 0.001, t(740.85) = 26.91, p < .001). The intercept was not significant (β = 0.02, SE = 0.09, t(5.30) = 0.19, p = .856).

The random effects show some variability attributable to working memory (variance = 0.02, SD = 0.15) and hearing in noise (variance = 0.01, SD = 0.11), with the residual variance being 0.08 (SD = 0.29).


## All post-hoc measures as fixed effects
A Mixed Effects Linear Regression Analysis is conducted to model the median real-time comprehension scores. Comprehension scores of the post-hoc tests, which are the multiple-choice questions, 10-scale comprehension ratings, and summaries, are utilized as factors in the regression. Digit Span and Digit-In-Noise scores are assigned as random effects, accounting for individual subject variations. This approach allows us to explore to what degree the scores of our novel real-time comprehension measure can be explained by comprehension, above and beyond the contribution of individual differences in working memory and speech perception in noise capacities.

We conducted the same regression analysis with different semantic similarity calculations to see whether calculating semantic similarities with different embedding models creates a diffece in how semantic similarity can predict the novel measure.

```{r}
model_all_glove1 <- lmer(trial_median_rescaled ~  
                glove_sum_max_similarity +
                likert_response + 
                multiple_choice_accuracy + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_all_glove1)
```

Giris metni diger rapordan alinabilir! Kalan da snl abstractten alinabilir

The model revealed a significant positive effect of summary scores on real-time comprehension (β = 0.00, SE = 0.00, t(381.42) = 2.31, p = .021). There was also a significant positive effect of post-hoc rating on real-time comprehension (β = 0.10, SE = 0.00, t(367.57) = 33.80, p < .001). The effect of multiple choice accuracy on real-time comprehension was not significant (β = 0.03, SE = 0.02, t(737.07) = 1.86, p = .064). The intercept was significant (β = -0.16, SE = 0.02, t(18.32) = -7.93, p < .001).

The random effects show minimal variability attributable to working memory (variance = 0.00, SD = 0.03) and hearing in noise (variance = 0.00, SD = 0.00), with the residual variance being 0.03 (SD = 0.18).

## Correlation btw predictors

```{r, warning=FALSE}
# visualize the correlations btw all predictors.

# Filter out rows where glove_sum_max_similarity is NA
df.filt<- df.data %>%
  filter(!is.na(glove_sum_max_similarity))

df.corr <- df.filt %>%
  select(trial_median_rescaled, glove_sum_max_similarity, likert_response, 
         multiple_choice_accuracy, digit_span_score, digit_in_noise_score) %>%
  rename(
    `real-time comprehension` = trial_median_rescaled,
    summary = glove_sum_max_similarity,
    `10-point scale` = likert_response,
    `multiple choice accuracy` = multiple_choice_accuracy,
    `working memory` = digit_span_score,
    `hearing in noise` = digit_in_noise_score
  )


# Calculate the correlation matrix and p-values
corr_matrix <- rcorr(as.matrix(df.corr))
correlations <- corr_matrix$r
p_values <- corr_matrix$P

# Replace diagonal p-values with NA for clarity
diag(p_values) <- NA

# Print the correlation matrix and p-values
print(correlations)
print(p_values)

# Customize the color scale
color_scale <- colorRampPalette(c("magenta","snow2","mediumaquamarine"))(200)

# Visualize the correlation matrix with significance stars and customized color scale

corrplot(correlations, method = "circle", type = "upper", p.mat = p_values, 
         sig.level = c(0.001, 0.01, 0.05), insig = "label_sig", 
         pch.cex = 0.7, pch.col = "black", col = color_scale, 
         tl.col = "black", cl.pos = "r", addCoef.col = NULL, 
         tl.cex = 0.7, cl.lim = c(-1, 1), cl.ratio = 0.25, cl.cex = 0.6)

```
yildizlar icin subnote lazim!!

Median real-time comprehension scores correlate with post-hoc comprehension score: They are moderately positively correlated with summary scores (r = 0.65, p < .001), strongly positively correlated with likert scale ratings (r = 0.89, p < .001), moderately positively correlated with summary scores (r = 0.65, p < .001), and a weekly positively correlated with multiple choice accuracy (r = 0.34, p < .001). 

Importantly, real-time comprehension scores do not correlate with working memory capacity of participants. On the other hand, all post-hoc comprehension measures' scores positively correlate with working memory (summary score: r=0.24, p < .001  ; likert scale rating: r = 0.09, p < .01; multiple choice accuracy: r=0.08, p < .01).

Furthermore, all post-hoc measures correlate with each other (summary scores- likert scale: r = 0.71, p < .001; summary score- multiple choice accuracy: r = 0.28, p < .001; likert scale- multiple choice accuracy: r = 0.35, p < .001).

There was also a significant, but weaker, negative correlation between real-time comprehension and hearing in noise (r = -0.09, p < .05).

```{r}
# correlation matrix with ind data points
ggpairs(df.corr, progress = FALSE)
```


## Predicting speech rate from comprehension measures 

```{r}
model_rate <- lmer(speech_rate ~ 
                trial_median_rescaled +
                glove_sum_max_similarity +
                likert_response + 
                multiple_choice_accuracy + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_rate)
```
We investigated whether comprehension changes based on speech rate by conducting a Mixed Effects Linear Regression with all comprehension measures as fixed effects. 

The model revealed a significant negative effect of real-time comprehension on speech rate (β = -1.69, SE = 0.11, t(734.57) = -15.14, p < .001). There was also a significant negative effect of GloVe sum max similarity on speech rate (β = -0.02, SE = 0.00, t(401.38) = -4.46, p < .001), and a significant negative effect of post-hoc rating on speech rate (β = -0.14, SE = 0.01, t(554.17) = -9.44, p < .001). The effect of multiple choice accuracy on speech rate was not significant (β = -0.09, SE = 0.05, t(733.46) = -1.90, p = .058). The intercept was significant (β = 5.00, SE = 0.12, t(9.44) = 42.17, p < .001).

The random effects show some variability attributable to working memory (variance = 0.07, SD = 0.27) and hearing in noise (variance = 0.00, SD = 0.05), with the residual variance being 0.30 (SD = 0.55).


```{r, warning=FALSE}
# plot the regression (each predictor)
# Each call to visreg will create a plot showing the relationship between the specified predictor and the response variable, holding the other predictors constant at their mean values. So you are then visualizing the partial effects of the predictor of choice.
# y axis = fitted (predicted) values of speech_rate given X measure values 

visreg_slider<- visreg(model_rate, "trial_median_rescaled", gg=TRUE,
                       xlab= "Real-Time Comprehension Scores", 
                       ylab= NULL) + 
  scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))

visreg_summary <- visreg(model_rate, "glove_sum_max_similarity", gg=TRUE,
                       xlab= "Summary Scores", 
                       ylab= NULL) + 
  scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))

visreg_likert <- visreg(model_rate, "likert_response", gg=TRUE,
                       xlab= "Post-hoc Rating Scores", 
                       ylab= NULL) + 
  scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))

visreg_multiple <- visreg(model_rate, "multiple_choice_accuracy", gg=TRUE,
                       xlab= "Multiple Choice Accuracy", 
                       ylab= NULL) + 
  scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))


# Arrange plots in a  grid
grid.arrange(
  arrangeGrob(
   visreg_slider, visreg_summary, visreg_likert, visreg_multiple, 
    ncol = 2,
    left = textGrob("     Predicted speech rate given the comprehension scores", rot = 90, vjust = 1)
  ) 
)

# For random slopes 
visreg_wm <- visreg(model_rate, "digit_span_score", gg=TRUE,
                       xlab= "Working Memory Scores", 
                       ylab= NULL) + 
  scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))

visreg_hearing <- visreg(model_rate, "digit_in_noise_score", gg=TRUE,
                       xlab= "Hearing In Noise", 
                       ylab= NULL) + 
  scale_y_continuous(breaks = c(1, 2, 3, 4, 5), limits = c(1, 5))
  

# put them in a grid
grid.arrange(
  arrangeGrob(
   visreg_wm, visreg_hearing,
    ncol = 2,
    left = textGrob("     Predicted speech rate given the control measures", rot = 90, vjust = 1)
  ) 
)
```
```{r}
# Predicted vs observed values of speech rate 

df.data_filtered <- df.data[!is.na(df.data$glove_sum_max_similarity), ]

# Extract predicted values
predicted_values <- predict(model_rate)

# Get the observed values from the original data frame
observed_values <- df.data_filtered$speech_rate

# Create a new data frame with the predicted and observed values
data_plot <- data.frame(
  predicted = predicted_values,
  observed = observed_values
)
# Create the scatterplot with regression line
ggplot(data_plot, aes(x = observed, y = predicted)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  theme_minimal() +
  labs( x = "Speech Rate",
       y = "Predicted Speech Rate")
```
```{r}
# visualize the estimates of fixed effects (on speech_rate)

# Get the fixed effects and confidence intervals
fixed_effects <- tidy(model_rate, effects = "fixed", conf.int = TRUE)
# Plot the fixed effects
ggplot(fixed_effects, aes(x = term, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  theme_minimal() +
  labs(title = "Fixed Effects with Confidence Intervals",
       x = "Predictor",
       y = "Estimate")

# Extract random effects
random_effects <- ranef(model_rate, condVar = TRUE)
# Convert the random effects to a data frame for plotting
random_effects_df <- do.call(rbind, lapply(random_effects, function(x) {
  x$grp <- rownames(x)
  x
}))

# Plot using sjPlot
# fixed effects (type = "est")
plot_model(model_rate, type = "est", show.values = TRUE, show.p = TRUE)

```



```{r}
# out of 3 similarity metric is the best one but there is a bias towards the last bin (recency bias- memory compnent) to highlight the limitation
# Su an bertle yaptik hesaplamayi- glove1 ile yapman lazim!
```

Discussion: novel measure is sig correlated with post-hoc measures -> it correlates with the static way of measuring comprehension -> validated as a good measure of comprehension 
Point out the limitations of post-hoc:
- written summary- recency bias
- multiple choice- worse than introspection, above chance level accuracy for x5 (even for trials they self-report that they understand nothing they perform better than chance- inferring the context - naturalistic stimuli)
Note that people use the sldier differently-> it is indeed timer resolved not like a likert scale with tge same score across time
Future steps
The slider can be used for any type of internal experience over time that does not have an overt expressive behavior (attention, affect etc.)

# Extra Analysis- regression models with other semantic similarity calculations

## Semantiuc similarity as the only fixed efffect- real time comprehension measure
```{r}
# Fixed effect of Summary- Glove (average of maximum cosine values of segment text words) on real time comprehension
model_glove2 <- lmer(trial_median_rescaled ~  
                glove_text_average_max_similarity +
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_glove2)
```

```{r}
# Fixed effect of Summary- BERT (cosine values of summaries) on real time comprehension
model_bert <- lmer(trial_median_rescaled ~  
                bert_cosine_similarity +
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_bert)
```

## All post-hoc measures as fixed effects- real time comprehension measure
```{r}
# with glove text_average
model_all_glove2 <- lmer(trial_median_rescaled ~  
                glove_text_average_max_similarity +
                likert_response + 
                multiple_choice_accuracy + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_all_glove2)
```

```{r}
# with BERT
model_all_bert <- lmer(trial_median_rescaled ~  
                bert_cosine_similarity +
                likert_response + 
                multiple_choice_accuracy + 
                (1 | digit_span_score) + 
                (1 | digit_in_noise_score), data = df.data)

summary(model_all_bert)
```
